{
  "8e1325f7-959a-4441-904b-86af3092ef9c": {
    "pk": "8e1325f7-959a-4441-904b-86af3092ef9c",
    "title": "Optimal Federated Learning for Nonparametric Regression with Heterogeneous Distributed Differential Privacy Constraints",
    "abstract": "This paper studies federated learning for nonparametric regression in the context of distributed samples across different servers, each adhering to distinct differential privacy constraints. The setting we consider is heterogeneous, encompassing both varying sample sizes and differential privacy constraints across servers. Within this framework, both global and pointwise estimation are considered, and optimal rates of convergence over the Besov spaces are established.   Distributed privacy-preserving estimators are proposed and their risk properties are investigated. Matching minimax lower bounds, up to a logarithmic factor, are established for both global and pointwise estimation. Together, these findings shed light on the tradeoff between statistical accuracy and privacy preservation. In particular, we characterize the compromise not only in terms of the privacy budget but also concerning the loss incurred by distributing data within the privacy framework as a whole. This insight captures the folklore wisdom that it is easier to retain privacy in larger samples, and explores the differences between pointwise and global estimation under distributed privacy constraints.",
    "authors": [
      "T. Tony Cai",
      "Abhinav Chakraborty",
      "Lasse Vuursteen"
    ],
    "url": "http://arxiv.org/abs/2406.06755v1",
    "timestamp": 1718048047,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.ST",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "253a9240-d266-41b2-8068-06a98077e730": {
    "pk": "253a9240-d266-41b2-8068-06a98077e730",
    "title": "Decentralized Personalized Federated Learning",
    "abstract": "This work tackles the challenges of data heterogeneity and communication limitations in decentralized federated learning. We focus on creating a collaboration graph that guides each client in selecting suitable collaborators for training personalized models that leverage their local data effectively. Our approach addresses these issues through a novel, communication-efficient strategy that enhances resource efficiency. Unlike traditional methods, our formulation identifies collaborators at a granular level by considering combinatorial relations of clients, enhancing personalization while minimizing communication overhead. We achieve this through a bi-level optimization framework that employs a constrained greedy algorithm, resulting in a resource-efficient collaboration graph for personalized learning. Extensive evaluation against various baselines across diverse datasets demonstrates the superiority of our method, named DPFL. DPFL consistently outperforms other approaches, showcasing its effectiveness in handling real-world data heterogeneity, minimizing communication overhead, enhancing resource efficiency, and building personalized models in decentralized federated learning scenarios.",
    "authors": [
      "Salma Kharrat",
      "Marco Canini",
      "Samuel Horvath"
    ],
    "url": "http://arxiv.org/abs/2406.06520v1",
    "timestamp": 1718042328,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "24490a71-2724-4855-a72f-f7c982581d07": {
    "pk": "24490a71-2724-4855-a72f-f7c982581d07",
    "title": "Optimisation of federated learning settings under statistical heterogeneity variations",
    "abstract": "Federated Learning (FL) enables local devices to collaboratively learn a shared predictive model by only periodically sharing model parameters with a central aggregator. However, FL can be disadvantaged by statistical heterogeneity produced by the diversity in each local devices data distribution, which creates different levels of Independent and Identically Distributed (IID) data. Furthermore, this can be more complex when optimising different combinations of FL parameters and choosing optimal aggregation. In this paper, we present an empirical analysis of different FL training parameters and aggregators over various levels of statistical heterogeneity on three datasets. We propose a systematic data partition strategy to simulate different levels of statistical heterogeneity and a metric to measure the level of IID. Additionally, we empirically identify the best FL model and key parameters for datasets of different characteristics. On the basis of these, we present recommended guidelines for FL parameters and aggregators to optimise model performance under different levels of IID and with different datasets",
    "authors": [
      "Basem Suleiman",
      "Muhammad Johan Alibasa",
      "Rizka Widyarini Purwanto",
      "Lewis Jeffries",
      "Ali Anaissi",
      "Jacky Song"
    ],
    "url": "http://arxiv.org/abs/2406.06340v1",
    "timestamp": 1718031663,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0d6fdd22-147e-48e3-a6f2-426f8a24de32": {
    "pk": "0d6fdd22-147e-48e3-a6f2-426f8a24de32",
    "title": "Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning",
    "abstract": "Federated Learning (FL) is a collaborative machine learning technique where multiple clients work together with a central server to train a global model without sharing their private data. However, the distribution shift across non-IID datasets of clients poses a challenge to this one-model-fits-all method hindering the ability of the global model to effectively adapt to each client's unique local data. To echo this challenge, personalized FL (PFL) is designed to allow each client to create personalized local models tailored to their private data. While extensive research has scrutinized backdoor risks in FL, it has remained underexplored in PFL applications. In this study, we delve deep into the vulnerabilities of PFL to backdoor attacks. Our analysis showcases a tale of two cities. On the one hand, the personalization process in PFL can dilute the backdoor poisoning effects injected into the personalized local models. Furthermore, PFL systems can also deploy both server-end and client-end defense mechanisms to strengthen the barrier against backdoor attacks. On the other hand, our study shows that PFL fortified with these defense methods may offer a false sense of security. We propose \\textit{PFedBA}, a stealthy and effective backdoor attack strategy applicable to PFL systems. \\textit{PFedBA} ingeniously aligns the backdoor learning task with the main learning task of PFL by optimizing the trigger generation process. Our comprehensive experiments demonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding triggers into personalized local models. \\textit{PFedBA} yields outstanding attack performance across 10 state-of-the-art PFL algorithms, defeating the existing 6 defense mechanisms. Our study sheds light on the subtle yet potent backdoor threats to PFL systems, urging the community to bolster defenses against emerging backdoor challenges.",
    "authors": [
      "Xiaoting Lyu",
      "Yufei Han",
      "Wei Wang",
      "Jingkai Liu",
      "Yongsheng Zhu",
      "Guangquan Xu",
      "Jiqiang Liu",
      "Xiangliang Zhang"
    ],
    "url": "http://arxiv.org/abs/2406.06207v1",
    "timestamp": 1718021645,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7c168e26-4dc3-46bb-adf5-33255952975f": {
    "pk": "7c168e26-4dc3-46bb-adf5-33255952975f",
    "title": "Federated learning in food research",
    "abstract": "Research in the food domain is at times limited due to data sharing obstacles, such as data ownership, privacy requirements, and regulations. While important, these obstacles can restrict data-driven methods such as machine learning. Federated learning, the approach of training models on locally kept data and only sharing the learned parameters, is a potential technique to alleviate data sharing obstacles. This systematic review investigates the use of federated learning within the food domain, structures included papers in a federated learning framework, highlights knowledge gaps, and discusses potential applications. A total of 41 papers were included in the review. The current applications include solutions to water and milk quality assessment, cybersecurity of water processing, pesticide residue risk analysis, weed detection, and fraud detection, focusing on centralized horizontal federated learning. One of the gaps found was the lack of vertical or transfer federated learning and decentralized architectures.",
    "authors": [
      "Zuzanna Fendor",
      "Bas H. M. van der Velden",
      "Xinxin Wang",
      "Andrea Jr. Carnoli",
      "Osman Mutlu",
      "Ali H\u00fcrriyeto\u011flu"
    ],
    "url": "http://arxiv.org/abs/2406.06202v1",
    "timestamp": 1718020691,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b4179176-65bb-433d-8967-6de50406e607": {
    "pk": "b4179176-65bb-433d-8967-6de50406e607",
    "title": "Fed-Sophia: A Communication-Efficient Second-Order Federated Learning Algorithm",
    "abstract": "Federated learning is a machine learning approach where multiple devices collaboratively learn with the help of a parameter server by sharing only their local updates. While gradient-based optimization techniques are widely adopted in this domain, the curvature information that second-order methods exhibit is crucial to guide and speed up the convergence. This paper introduces a scalable second-order method, allowing the adoption of curvature information in federated large models. Our method, coined Fed-Sophia, combines a weighted moving average of the gradient with a clipping operation to find the descent direction. In addition to that, a lightweight estimation of the Hessian's diagonal is used to incorporate the curvature information. Numerical evaluation shows the superiority, robustness, and scalability of the proposed Fed-Sophia scheme compared to first and second-order baselines.",
    "authors": [
      "Ahmed Elbakary",
      "Chaouki Ben Issaid",
      "Mohammad Shehab",
      "Karim Seddik",
      "Tamer ElBatt",
      "Mehdi Bennis"
    ],
    "url": "http://arxiv.org/abs/2406.06655v1",
    "timestamp": 1718013450,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e532aec1-19ef-4190-82d3-4b9c2ee5a818": {
    "pk": "e532aec1-19ef-4190-82d3-4b9c2ee5a818",
    "title": "Comments on \"Federated Learning with Differential Privacy: Algorithms and Performance Analysis\"",
    "abstract": "In the paper by Wei et al. (\"Federated Learning with Differential Privacy: Algorithms and Performance Analysis\"), the convergence performance of the proposed differential privacy algorithm in federated learning (FL), known as Noising before Model Aggregation FL (NbAFL), was studied. However, the presented convergence upper bound of NbAFL (Theorem 2) is incorrect. This comment aims to present the correct form of the convergence upper bound for NbAFL.",
    "authors": [
      "Mahtab Talaei",
      "Iman Izadi"
    ],
    "url": "http://arxiv.org/abs/2406.05858v1",
    "timestamp": 1717952636,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8b39c446-e36a-4cd4-9bca-9a3f87109ab9": {
    "pk": "8b39c446-e36a-4cd4-9bca-9a3f87109ab9",
    "title": "Blockchain Integrated Federated Learning in Edge-Fog-Cloud Systems for IoT based Healthcare Applications A Survey",
    "abstract": "Modern Internet of Things (IoT) applications generate enormous amounts of data, making data-driven machine learning essential for developing precise and reliable statistical models. However, data is often stored in silos, and strict user-privacy legislation complicates data utilization, limiting machine learning's potential in traditional centralized paradigms due to diverse data probability distributions and lack of personalization. Federated learning, a new distributed paradigm, supports collaborative learning while preserving privacy, making it ideal for IoT applications. By employing cryptographic techniques, IoT systems can securely store and transmit data, ensuring consistency. The integration of federated learning and blockchain is particularly advantageous for handling sensitive data, such as in healthcare. Despite the potential of these technologies, a comprehensive examination of their integration in edge-fog-cloud-based IoT computing systems and healthcare applications is needed. This survey article explores the architecture, structure, functions, and characteristics of federated learning and blockchain, their applications in various computing paradigms, and evaluates their implementations in healthcare.",
    "authors": [
      "Shinu M. Rajagopal",
      "Supriya M.",
      "Rajkumar Buyya"
    ],
    "url": "http://arxiv.org/abs/2406.05517v1",
    "timestamp": 1717864608,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d8d33614-1c57-4ad8-b69c-3fc62c5f8946": {
    "pk": "d8d33614-1c57-4ad8-b69c-3fc62c5f8946",
    "title": "FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models",
    "abstract": "Federated learning has enabled multiple parties to collaboratively train large language models without directly sharing their data (FedLLM). Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy. However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works all rely on artificially constructed datasets, failing to capture properties in real-world scenarios. Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community. FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747. Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios. Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration). We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons. Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench.",
    "authors": [
      "Rui Ye",
      "Rui Ge",
      "Xinyu Zhu",
      "Jingyi Chai",
      "Yaxin Du",
      "Yang Liu",
      "Yanfeng Wang",
      "Siheng Chen"
    ],
    "url": "http://arxiv.org/abs/2406.04845v1",
    "timestamp": 1717759170,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5e783b99-381a-4b22-8024-b75d058a948a": {
    "pk": "5e783b99-381a-4b22-8024-b75d058a948a",
    "title": "Federated Representation Learning in the Under-Parameterized Regime",
    "abstract": "Federated representation learning (FRL) is a popular personalized federated learning (FL) framework where clients work together to train a common representation while retaining their personalized heads. Existing studies, however, largely focus on the over-parameterized regime. In this paper, we make the initial efforts to investigate FRL in the under-parameterized regime, where the FL model is insufficient to express the variations in all ground-truth models. We propose a novel FRL algorithm FLUTE, and theoretically characterize its sample complexity and convergence rate for linear models in the under-parameterized regime. To the best of our knowledge, this is the first FRL algorithm with provable performance guarantees in this regime. FLUTE features a data-independent random initialization and a carefully designed objective function that aids the distillation of subspace spanned by the global optimal representation from the misaligned local representations. On the technical side, we bridge low-rank matrix approximation techniques with the FL analysis, which may be of broad interest. We also extend FLUTE beyond linear representations. Experimental results demonstrate that FLUTE outperforms state-of-the-art FRL solutions in both synthetic and real-world tasks.",
    "authors": [
      "Renpu Liu",
      "Cong Shen",
      "Jing Yang"
    ],
    "url": "http://arxiv.org/abs/2406.04596v2",
    "timestamp": 1717729207,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e8f56986-73cf-47aa-84e4-5b65c5538d98": {
    "pk": "e8f56986-73cf-47aa-84e4-5b65c5538d98",
    "title": "1-D CNN-Based Online Signature Verification with Federated Learning",
    "abstract": "Online signature verification plays a pivotal role in security infrastructures. However, conventional online signature verification models pose significant risks to data privacy, especially during training processes. To mitigate these concerns, we propose a novel federated learning framework that leverages 1-D Convolutional Neural Networks (CNN) for online signature verification. Furthermore, our experiments demonstrate the effectiveness of our framework regarding 1-D CNN and federated learning. Particularly, the experiment results highlight that our framework 1) minimizes local computational resources; 2) enhances transfer effects with substantial initialization data; 3) presents remarkable scalability. The centralized 1-D CNN model achieves an Equal Error Rate (EER) of 3.33% and an accuracy of 96.25%. Meanwhile, configurations with 2, 5, and 10 agents yield EERs of 5.42%, 5.83%, and 5.63%, along with accuracies of 95.21%, 94.17%, and 94.06%, respectively.",
    "authors": [
      "Lingfeng Zhang",
      "Yuheng Guo",
      "Yepeng Ding",
      "Hiroyuki Sato"
    ],
    "url": "http://arxiv.org/abs/2406.06597v1",
    "timestamp": 1717662448,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "25496223-1d34-4566-bfa6-ecee43447092": {
    "pk": "25496223-1d34-4566-bfa6-ecee43447092",
    "title": "FedPylot: Navigating Federated Learning for Real-Time Object Detection in Internet of Vehicles",
    "abstract": "The Internet of Vehicles (IoV) emerges as a pivotal component for autonomous driving and intelligent transportation systems (ITS), by enabling low-latency big data processing in a dense interconnected network that comprises vehicles, infrastructures, pedestrians and the cloud. Autonomous vehicles are heavily reliant on machine learning (ML) and can strongly benefit from the wealth of sensory data generated at the edge, which calls for measures to reconcile model training with preserving the privacy of sensitive user data. Federated learning (FL) stands out as a promising solution to train sophisticated ML models in vehicular networks while protecting the privacy of road users and mitigating communication overhead. This paper examines the federated optimization of the cutting-edge YOLOv7 model to tackle real-time object detection amid data heterogeneity, encompassing unbalancedness, concept drift, and label distribution skews. To this end, we introduce FedPylot, a lightweight MPI-based prototype to simulate federated object detection experiments on high-performance computing (HPC) systems, where we safeguard server-client communications using hybrid encryption. Our study factors in accuracy, communication cost, and inference speed, thereby presenting a balanced approach to the challenges faced by autonomous vehicles. We demonstrate promising results for the applicability of FL in IoV and hope that FedPylot will provide a basis for future research into federated real-time object detection. The source code is available at https://github.com/cyprienquemeneur/fedpylot.",
    "authors": [
      "Cyprien Qu\u00e9m\u00e9neur",
      "Soumaya Cherkaoui"
    ],
    "url": "http://arxiv.org/abs/2406.03611v1",
    "timestamp": 1717618019,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "567ead87-c486-4ff0-877d-90e61616600d": {
    "pk": "567ead87-c486-4ff0-877d-90e61616600d",
    "title": "Fantastyc: Blockchain-based Federated Learning Made Secure and Practical",
    "abstract": "Federated Learning is a decentralized framework that enables multiple clients to collaboratively train a machine learning model under the orchestration of a central server without sharing their local data. The centrality of this framework represents a point of failure which is addressed in literature by blockchain-based federated learning approaches. While ensuring a fully-decentralized solution with traceability, such approaches still face several challenges about integrity, confidentiality and scalability to be practically deployed. In this paper, we propose Fantastyc, a solution designed to address these challenges that have been never met together in the state of the art.",
    "authors": [
      "William Boitier",
      "Antonella Del Pozzo",
      "\u00c1lvaro Garc\u00eda-P\u00e9rez",
      "Stephane Gazut",
      "Pierre Jobic",
      "Alexis Lemaire",
      "Erwan Mahe",
      "Aurelien Mayoue",
      "Maxence Perion",
      "Deepika Singh",
      "Tuanir Franca Rezende",
      "Sara Tucci-Piergiovanni"
    ],
    "url": "http://arxiv.org/abs/2406.03608v1",
    "timestamp": 1717617709,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "efc64717-9fbd-4d81-94eb-0bcc96b4bbd8": {
    "pk": "efc64717-9fbd-4d81-94eb-0bcc96b4bbd8",
    "title": "Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning",
    "abstract": "High utility and rigorous data privacy are of the main goals of a federated learning (FL) system, which learns a model from the data distributed among some clients. The latter has been tried to achieve by using differential privacy in FL (DPFL). There is often heterogeneity in clients privacy requirements, and existing DPFL works either assume uniform privacy requirements for clients or are not applicable when server is not fully trusted (our setting). Furthermore, there is often heterogeneity in batch and/or dataset size of clients, which as shown, results in extra variation in the DP noise level across clients model updates. With these sources of heterogeneity, straightforward aggregation strategies, e.g., assigning clients aggregation weights proportional to their privacy parameters will lead to lower utility. We propose Robust-HDP, which efficiently estimates the true noise level in clients model updates and reduces the noise-level in the aggregated model updates considerably. Robust-HDP improves utility and convergence speed, while being safe to the clients that may maliciously send falsified privacy parameter to server. Extensive experimental results on multiple datasets and our theoretical analysis confirm the effectiveness of Robust-HDP. Our code can be found here.",
    "authors": [
      "Saber Malekmohammadi",
      "Yaoliang Yu",
      "Yang Cao"
    ],
    "url": "http://arxiv.org/abs/2406.03519v1",
    "timestamp": 1717609302,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b972d553-094b-468e-b5b5-c46e180874cd": {
    "pk": "b972d553-094b-468e-b5b5-c46e180874cd",
    "title": "Buffered Asynchronous Secure Aggregation for Cross-Device Federated Learning",
    "abstract": "Asynchronous federated learning (AFL) is an effective method to address the challenge of device heterogeneity in cross-device federated learning. However, AFL is usually incompatible with existing secure aggregation protocols used to protect user privacy in federated learning because most existing secure aggregation protocols are based on synchronous aggregation. To address this problem, we propose a novel secure aggregation protocol named buffered asynchronous secure aggregation (BASA) in this paper. Compared with existing protocols, BASA is fully compatible with AFL and provides secure aggregation under the condition that each user only needs one round of communication with the server without relying on any synchronous interaction among users. Based on BASA, we propose the first AFL method which achieves secure aggregation without extra requirements on hardware. We empirically demonstrate that BASA outperforms existing secure aggregation protocols for cross-device federated learning in terms of training efficiency and scalability.",
    "authors": [
      "Kun Wang",
      "Yi-Rui Yang",
      "Wu-Jun Li"
    ],
    "url": "http://arxiv.org/abs/2406.03516v1",
    "timestamp": 1717605572,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8c09befa-cb6f-4130-be96-159ebb2ba883": {
    "pk": "8c09befa-cb6f-4130-be96-159ebb2ba883",
    "title": "FedStaleWeight: Buffered Asynchronous Federated Learning with Fair Aggregation via Staleness Reweighting",
    "abstract": "Federated Learning (FL) endeavors to harness decentralized data while preserving privacy, facing challenges of performance, scalability, and collaboration. Asynchronous Federated Learning (AFL) methods have emerged as promising alternatives to their synchronous counterparts bounded by the slowest agent, yet they add additional challenges in convergence guarantees, fairness with respect to compute heterogeneity, and incorporation of staleness in aggregated updates. Specifically, AFL biases model training heavily towards agents who can produce updates faster, leaving slower agents behind, who often also have differently distributed data which is not learned by the global model. Naively upweighting introduces incentive issues, where true fast updating agents may falsely report updates at a slower speed to increase their contribution to model training. We introduce FedStaleWeight, an algorithm addressing fairness in aggregating asynchronous client updates by employing average staleness to compute fair re-weightings. FedStaleWeight reframes asynchronous federated learning aggregation as a mechanism design problem, devising a weighting strategy that incentivizes truthful compute speed reporting without favoring faster update-producing agents by upweighting agent updates based on staleness. Leveraging only observed agent update staleness, FedStaleWeight results in more equitable aggregation on a per-agent basis. We both provide theoretical convergence guarantees in the smooth, non-convex setting and empirically compare FedStaleWeight against the commonly used asynchronous FedBuff with gradient averaging, demonstrating how it achieves stronger fairness, expediting convergence to a higher global model accuracy. Finally, we provide an open-source test bench to facilitate exploration of buffered AFL aggregation strategies, fostering further research in asynchronous federated learning paradigms.",
    "authors": [
      "Jeffrey Ma",
      "Alan Tu",
      "Yiling Chen",
      "Vijay Janapa Reddi"
    ],
    "url": "http://arxiv.org/abs/2406.02877v1",
    "timestamp": 1717555942,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "64ad466e-2dbf-4a4d-9f82-0742c21f3ffb": {
    "pk": "64ad466e-2dbf-4a4d-9f82-0742c21f3ffb",
    "title": "Reducing Bias in Federated Class-Incremental Learning with Hierarchical Generative Prototypes",
    "abstract": "Federated Learning (FL) aims at unburdening the training of deep models by distributing computation across multiple devices (clients) while safeguarding data privacy. On top of that, Federated Continual Learning (FCL) also accounts for data distribution evolving over time, mirroring the dynamic nature of real-world environments. In this work, we shed light on the Incremental and Federated biases that naturally emerge in FCL. While the former is a known problem in Continual Learning, stemming from the prioritization of recently introduced classes, the latter (i.e., the bias towards local distributions) remains relatively unexplored. Our proposal constrains both biases in the last layer by efficiently fine-tuning a pre-trained backbone using learnable prompts, resulting in clients that produce less biased representations and more biased classifiers. Therefore, instead of solely relying on parameter aggregation, we also leverage generative prototypes to effectively balance the predictions of the global model. Our method improves on the current State Of The Art, providing an average increase of +7.9% in accuracy.",
    "authors": [
      "Riccardo Salami",
      "Pietro Buzzega",
      "Matteo Mosconi",
      "Mattia Verasani",
      "Simone Calderara"
    ],
    "url": "http://arxiv.org/abs/2406.02447v1",
    "timestamp": 1717517547,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "eb49dd59-f50a-4ae8-883d-1262d40f612a": {
    "pk": "eb49dd59-f50a-4ae8-883d-1262d40f612a",
    "title": "FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning",
    "abstract": "Federated Learning (FL) has emerged as a pivotal framework for the development of effective global models (global FL) or personalized models (personalized FL) across clients with heterogeneous, non-iid data distribution. A key challenge in FL is client drift, where data heterogeneity impedes the aggregation of scattered knowledge. Recent studies have tackled the client drift issue by identifying significant divergence in the last classifier layer. To mitigate this divergence, strategies such as freezing the classifier weights and aligning the feature extractor accordingly have proven effective. Although the local alignment between classifier and feature extractor has been studied as a crucial factor in FL, we observe that it may lead the model to overemphasize the observed classes within each client. Thus, our objectives are twofold: (1) enhancing local alignment while (2) preserving the representation of unseen class samples. This approach aims to effectively integrate knowledge from individual clients, thereby improving performance for both global and personalized FL. To achieve this, we introduce a novel algorithm named FedDr+, which empowers local model alignment using dot-regression loss. FedDr+ freezes the classifier as a simplex ETF to align the features and improves aggregated global models by employing a feature distillation mechanism to retain information about unseen/missing classes. Consequently, we provide empirical evidence demonstrating that our algorithm surpasses existing methods that use a frozen classifier to boost alignment across the diverse distribution.",
    "authors": [
      "Seongyoon Kim",
      "Minchan Jeong",
      "Sungnyun Kim",
      "Sungwoo Cho",
      "Sumyeong Ahn",
      "Se-Young Yun"
    ],
    "url": "http://arxiv.org/abs/2406.02355v1",
    "timestamp": 1717511653,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "716e42d8-64e1-4c4b-a525-05a8dbbf5df0": {
    "pk": "716e42d8-64e1-4c4b-a525-05a8dbbf5df0",
    "title": "One-Shot Federated Learning with Bayesian Pseudocoresets",
    "abstract": "Optimization-based techniques for federated learning (FL) often come with prohibitive communication cost, as high dimensional model parameters need to be communicated repeatedly between server and clients. In this paper, we follow a Bayesian approach allowing to perform FL with one-shot communication, by solving the global inference problem as a product of local client posteriors. For models with multi-modal likelihoods, such as neural networks, a naive application of this scheme is hampered, since clients will capture different posterior modes, causing a destructive collapse of the posterior on the server side. Consequently, we explore approximate inference in the function-space representation of client posteriors, hence suffering less or not at all from multi-modality. We show that distributed function-space inference is tightly related to learning Bayesian pseudocoresets and develop a tractable Bayesian FL algorithm on this insight. We show that this approach achieves prediction performance competitive to state-of-the-art while showing a striking reduction in communication cost of up to two orders of magnitude. Moreover, due to its Bayesian nature, our method also delivers well-calibrated uncertainty estimates.",
    "authors": [
      "Tim d'Hondt",
      "Mykola Pechenizkiy",
      "Robert Peharz"
    ],
    "url": "http://arxiv.org/abs/2406.02177v1",
    "timestamp": 1717496079,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c5d1b04f-ae90-4f1a-8e6d-7bd789a1f1c1": {
    "pk": "c5d1b04f-ae90-4f1a-8e6d-7bd789a1f1c1",
    "title": "Mixed-Precision Over-The-Air Federated Learning via Approximated Computing",
    "abstract": "Over-the-Air Federated Learning (OTA-FL) has been extensively investigated as a privacy-preserving distributed learning mechanism. Realistic systems will see FL clients with diverse size, weight, and power configurations. A critical research gap in existing OTA-FL research is the assumption of homogeneous client computational bit precision. Indeed, many clients may exploit approximate computing (AxC) where bit precisions are adjusted for energy and computational efficiency. The dynamic distribution of bit precision updates amongst FL clients poses an open challenge for OTA-FL, as is is incompatible in the wireless modulation superposition space.   Here, we propose an AxC-based OTA-FL framework of clients with multiple precisions, demonstrating the following innovations: (i) optimize the quantization-performance trade-off for both server and clients within the constraints of varying edge computing capabilities and learning accuracy requirements, and (ii) develop heterogeneous gradient resolution OTA-FL modulation schemes to ensure compatibility with physical layer OTA aggregation. Our findings indicate that we can design modulation schemes that enable AxC based OTA-FL, which can achieve 50\\% faster and smoother server convergence and a performance enhancement for the lowest precision clients compared to a homogeneous precision approach. This demonstrates the great potential of our AxC-based OTA-FL approach in heterogeneous edge computing environments.",
    "authors": [
      "Jinsheng Yuan",
      "Zhuangkun Wei",
      "Weisi Guo"
    ],
    "url": "http://arxiv.org/abs/2406.03402v1",
    "timestamp": 1717492065,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c1024cf9-2307-442e-96de-45d974744bbb": {
    "pk": "c1024cf9-2307-442e-96de-45d974744bbb",
    "title": "Parameterizing Federated Continual Learning for Reproducible Research",
    "abstract": "Federated Learning (FL) systems evolve in heterogeneous and ever-evolving environments that challenge their performance. Under real deployments, the learning tasks of clients can also evolve with time, which calls for the integration of methodologies such as Continual Learning. To enable research reproducibility, we propose a set of experimental best practices that precisely capture and emulate complex learning scenarios. Our framework, Freddie, is the first entirely configurable framework for Federated Continual Learning (FCL), and it can be seamlessly deployed on a large number of machines thanks to the use of Kubernetes and containerization. We demonstrate the effectiveness of Freddie on two use cases, (i) large-scale FL on CIFAR100 and (ii) heterogeneous task sequence on FCL, which highlight unaddressed performance challenges in FCL scenarios.",
    "authors": [
      "Bart Cox",
      "Jeroen Galjaard",
      "Aditya Shankar",
      "J\u00e9r\u00e9mie Decouchant",
      "Lydia Y. Chen"
    ],
    "url": "http://arxiv.org/abs/2406.02015v1",
    "timestamp": 1717484093,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fea6fd21-5660-4efb-a3ee-f21174d307e8": {
    "pk": "fea6fd21-5660-4efb-a3ee-f21174d307e8",
    "title": "Efficient Data Distribution Estimation for Accelerated Federated Learning",
    "abstract": "Federated Learning(FL) is a privacy-preserving machine learning paradigm where a global model is trained in-situ across a large number of distributed edge devices. These systems are often comprised of millions of user devices and only a subset of available devices can be used for training in each epoch. Designing a device selection strategy is challenging, given that devices are highly heterogeneous in both their system resources and training data. This heterogeneity makes device selection very crucial for timely model convergence and sufficient model accuracy. To tackle the FL client heterogeneity problem, various client selection algorithms have been developed, showing promising performance improvement in terms of model coverage and accuracy. In this work, we study the overhead of client selection algorithms in a large scale FL environment. Then we propose an efficient data distribution summary calculation algorithm to reduce the overhead in a real-world large scale FL environment. The evaluation shows that our proposed solution could achieve up to 30x reduction in data summary time, and up to 360x reduction in clustering time.",
    "authors": [
      "Yuanli Wang",
      "Lei Huang"
    ],
    "url": "http://arxiv.org/abs/2406.01774v1",
    "timestamp": 1717446797,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "47acac76-7e14-4e36-b84e-321ac55d7dd1": {
    "pk": "47acac76-7e14-4e36-b84e-321ac55d7dd1",
    "title": "Federated Learning-based Collaborative Wideband Spectrum Sensing and Scheduling for UAVs in UTM Systems",
    "abstract": "In this paper, we propose a data-driven framework for collaborative wideband spectrum sensing and scheduling for networked unmanned aerial vehicles (UAVs), which act as the secondary users (SUs) to opportunistically utilize detected \"spectrum holes\". Our overall framework consists of three main stages. Firstly, in the model training stage, we explore dataset generation in a multi-cell environment and training a machine learning (ML) model using the federated learning (FL) architecture. Unlike the existing studies on FL for wireless that presume datasets are readily available for training, we propose a novel architecture that directly integrates wireless dataset generation, which involves capturing I/Q samples from over-the-air signals in a multi-cell environment, into the FL training process. Secondly, in the collaborative spectrum inference stage, we propose a collaborative spectrum fusion strategy that is compatible with the unmanned aircraft system traffic management (UTM) ecosystem. Finally, in the spectrum scheduling stage, we leverage reinforcement learning (RL) solutions to dynamically allocate the detected spectrum holes to the secondary users. To evaluate the proposed methods, we establish a comprehensive simulation framework that generates a near-realistic synthetic dataset using MATLAB LTE toolbox by incorporating base-station~(BS) locations in a chosen area of interest, performing ray-tracing, and emulating the primary users channel usage in terms of I/Q samples. This evaluation methodology provides a flexible framework to generate large spectrum datasets that could be used for developing ML/AI-based spectrum management solutions for aerial devices.",
    "authors": [
      "Sravan Reddy Chintareddy",
      "Keenan Roach",
      "Kenny Cheung",
      "Morteza Hashemi"
    ],
    "url": "http://arxiv.org/abs/2406.01727v1",
    "timestamp": 1717439967,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a43f40cf-f22b-4664-ad2b-cb28d968db22": {
    "pk": "a43f40cf-f22b-4664-ad2b-cb28d968db22",
    "title": "Asynchronous Multi-Server Federated Learning for Geo-Distributed Clients",
    "abstract": "Federated learning (FL) systems enable multiple clients to train a machine learning model iteratively through synchronously exchanging the intermediate model weights with a single server. The scalability of such FL systems can be limited by two factors: server idle time due to synchronous communication and the risk of a single server becoming the bottleneck. In this paper, we propose a new FL architecture, to our knowledge, the first multi-server FL system that is entirely asynchronous, and therefore addresses these two limitations simultaneously. Our solution keeps both servers and clients continuously active. As in previous multi-server methods, clients interact solely with their nearest server, ensuring efficient update integration into the model. Differently, however, servers also periodically update each other asynchronously, and never postpone interactions with clients. We compare our solution to three representative baselines - FedAvg, FedAsync and HierFAVG - on the MNIST and CIFAR-10 image classification datasets and on the WikiText-2 language modeling dataset. Our solution converges to similar or higher accuracy levels than previous baselines and requires 61% less time to do so in geo-distributed settings.",
    "authors": [
      "Yuncong Zuo",
      "Bart Cox",
      "J\u00e9r\u00e9mie Decouchant",
      "Lydia Y. Chen"
    ],
    "url": "http://arxiv.org/abs/2406.01439v1",
    "timestamp": 1717428586,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "34abce71-1646-4d61-b361-dd57517653e0": {
    "pk": "34abce71-1646-4d61-b361-dd57517653e0",
    "title": "Asynchronous Byzantine Federated Learning",
    "abstract": "Federated learning (FL) enables a set of geographically distributed clients to collectively train a model through a server. Classically, the training process is synchronous, but can be made asynchronous to maintain its speed in presence of slow clients and in heterogeneous networks. The vast majority of Byzantine fault-tolerant FL systems however rely on a synchronous training process. Our solution is one of the first Byzantine-resilient and asynchronous FL algorithms that does not require an auxiliary server dataset and is not delayed by stragglers, which are shortcomings of previous works. Intuitively, the server in our solution waits to receive a minimum number of updates from clients on its latest model to safely update it, and is later able to safely leverage the updates that late clients might send. We compare the performance of our solution with state-of-the-art algorithms on both image and text datasets under gradient inversion, perturbation, and backdoor attacks. Our results indicate that our solution trains a model faster than previous synchronous FL solution, and maintains a higher accuracy, up to 1.54x and up to 1.75x for perturbation and gradient inversion attacks respectively, in the presence of Byzantine clients than previous asynchronous FL solutions.",
    "authors": [
      "Bart Cox",
      "Abele M\u0103lan",
      "J\u00e9r\u00e9mie Decouchant",
      "Lydia Y. Chen"
    ],
    "url": "http://arxiv.org/abs/2406.01438v1",
    "timestamp": 1717428578,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fb113392-0b7a-4ce5-b1e8-b2055aa978ec": {
    "pk": "fb113392-0b7a-4ce5-b1e8-b2055aa978ec",
    "title": "Accelerating Heterogeneous Federated Learning with Closed-form Classifiers",
    "abstract": "Federated Learning (FL) methods often struggle in highly statistically heterogeneous settings. Indeed, non-IID data distributions cause client drift and biased local solutions, particularly pronounced in the final classification layer, negatively impacting convergence speed and accuracy. To address this issue, we introduce Federated Recursive Ridge Regression (Fed3R). Our method fits a Ridge Regression classifier computed in closed form leveraging pre-trained features. Fed3R is immune to statistical heterogeneity and is invariant to the sampling order of the clients. Therefore, it proves particularly effective in cross-device scenarios. Furthermore, it is fast and efficient in terms of communication and computation costs, requiring up to two orders of magnitude fewer resources than the competitors. Finally, we propose to leverage the Fed3R parameters as an initialization for a softmax classifier and subsequently fine-tune the model using any FL algorithm (Fed3R with Fine-Tuning, Fed3R+FT). Our findings also indicate that maintaining a fixed classifier aids in stabilizing the training and learning more discriminative features in cross-device settings. Official website: https://fed-3r.github.io/.",
    "authors": [
      "Eros Fan\u00ec",
      "Raffaello Camoriano",
      "Barbara Caputo",
      "Marco Ciccone"
    ],
    "url": "http://arxiv.org/abs/2406.01116v1",
    "timestamp": 1717404726,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "daaa9092-2415-418d-aad2-bd4f3e90ae1f": {
    "pk": "daaa9092-2415-418d-aad2-bd4f3e90ae1f",
    "title": "Cohort Squeeze: Beyond a Single Communication Round per Cohort in Cross-Device Federated Learning",
    "abstract": "Virtually all federated learning (FL) methods, including FedAvg, operate in the following manner: i) an orchestrating server sends the current model parameters to a cohort of clients selected via certain rule, ii) these clients then independently perform a local training procedure (e.g., via SGD or Adam) using their own training data, and iii) the resulting models are shipped to the server for aggregation. This process is repeated until a model of suitable quality is found. A notable feature of these methods is that each cohort is involved in a single communication round with the server only. In this work we challenge this algorithmic design primitive and investigate whether it is possible to ``squeeze more juice\" out of each cohort than what is possible in a single communication round. Surprisingly, we find that this is indeed the case, and our approach leads to up to 74% reduction in the total communication cost needed to train a FL model in the cross-device setting. Our method is based on a novel variant of the stochastic proximal point method (SPPM-AS) which supports a large collection of client sampling procedures some of which lead to further gains when compared to classical client selection approaches.",
    "authors": [
      "Kai Yi",
      "Timur Kharisov",
      "Igor Sokolov",
      "Peter Richt\u00e1rik"
    ],
    "url": "http://arxiv.org/abs/2406.01115v1",
    "timestamp": 1717404529,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "99ea47f8-48f1-480e-9d62-d19783f4ea20": {
    "pk": "99ea47f8-48f1-480e-9d62-d19783f4ea20",
    "title": "FedAdOb: Privacy-Preserving Federated Deep Learning with Adaptive Obfuscation",
    "abstract": "Federated learning (FL) has emerged as a collaborative approach that allows multiple clients to jointly learn a machine learning model without sharing their private data. The concern about privacy leakage, albeit demonstrated under specific conditions, has triggered numerous follow-up research in designing powerful attacking methods and effective defending mechanisms aiming to thwart these attacking methods. Nevertheless, privacy-preserving mechanisms employed in these defending methods invariably lead to compromised model performances due to a fixed obfuscation applied to private data or gradients. In this article, we, therefore, propose a novel adaptive obfuscation mechanism, coined FedAdOb, to protect private data without yielding original model performances. Technically, FedAdOb utilizes passport-based adaptive obfuscation to ensure data privacy in both horizontal and vertical federated learning settings. The privacy-preserving capabilities of FedAdOb, specifically with regard to private features and labels, are theoretically proven through Theorems 1 and 2. Furthermore, extensive experimental evaluations conducted on various datasets and network architectures demonstrate the effectiveness of FedAdOb by manifesting its superior trade-off between privacy preservation and model performance, surpassing existing methods.",
    "authors": [
      "Hanlin Gu",
      "Jiahuan Luo",
      "Yan Kang",
      "Yuan Yao",
      "Gongxi Zhu",
      "Bowen Li",
      "Lixin Fan",
      "Qiang Yang"
    ],
    "url": "http://arxiv.org/abs/2406.01085v1",
    "timestamp": 1717402329,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8bf958d6-0b4f-49b4-87af-b95bc20ffb2f": {
    "pk": "8bf958d6-0b4f-49b4-87af-b95bc20ffb2f",
    "title": "No Vandalism: Privacy-Preserving and Byzantine-Robust Federated Learning",
    "abstract": "Federated learning allows several clients to train one machine learning model jointly without sharing private data, providing privacy protection. However, traditional federated learning is vulnerable to poisoning attacks, which can not only decrease the model performance, but also implant malicious backdoors. In addition, direct submission of local model parameters can also lead to the privacy leakage of the training dataset. In this paper, we aim to build a privacy-preserving and Byzantine-robust federated learning scheme to provide an environment with no vandalism (NoV) against attacks from malicious participants. Specifically, we construct a model filter for poisoned local models, protecting the global model from data and model poisoning attacks. This model filter combines zero-knowledge proofs to provide further privacy protection. Then, we adopt secret sharing to provide verifiable secure aggregation, removing malicious clients that disrupting the aggregation process. Our formal analysis proves that NoV can protect data privacy and weed out Byzantine attackers. Our experiments illustrate that NoV can effectively address data and model poisoning attacks, including PGD, and outperforms other related schemes.",
    "authors": [
      "Zhibo Xing",
      "Zijian Zhang",
      "Zi'ang Zhang",
      "Jiamou Liu",
      "Liehuang Zhu",
      "Giovanni Russello"
    ],
    "url": "http://arxiv.org/abs/2406.01080v1",
    "timestamp": 1717401550,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "508ca456-0c80-4003-b76b-f6bef3001b71": {
    "pk": "508ca456-0c80-4003-b76b-f6bef3001b71",
    "title": "Blockchain-aided wireless federated learning: Resource allocation and client scheduling",
    "abstract": "Federated learning (FL) based on the centralized design faces both challenges regarding the trust issue and a single point of failure. To alleviate these issues, blockchain-aided decentralized FL (BDFL) introduces the decentralized network architecture into the FL training process, which can effectively overcome the defects of centralized architecture. However, deploying BDFL in wireless networks usually encounters challenges such as limited bandwidth, computing power, and energy consumption. Driven by these considerations, a dynamic stochastic optimization problem is formulated to minimize the average training delay by jointly optimizing the resource allocation and client selection under the constraints of limited energy budget and client participation. We solve the long-term mixed integer non-linear programming problem by employing the tool of Lyapunov optimization and thereby propose the dynamic resource allocation and client scheduling BDFL (DRC-BDFL) algorithm. Furthermore, we analyze the learning performance of DRC-BDFL and derive an upper bound for convergence regarding the global loss function. Extensive experiments conducted on SVHN and CIFAR-10 datasets demonstrate that DRC-BDFL achieves comparable accuracy to baseline algorithms while significantly reducing the training delay by 9.24% and 12.47%, respectively.",
    "authors": [
      "Jun Li",
      "Weiwei Zhang",
      "Kang Wei",
      "Guangji Chen",
      "Feng Shu",
      "Wen Chen",
      "Shi Jin"
    ],
    "url": "http://arxiv.org/abs/2406.00752v1",
    "timestamp": 1717337429,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0a20911d-ba7e-49d3-8bff-a19f4b97bb0c": {
    "pk": "0a20911d-ba7e-49d3-8bff-a19f4b97bb0c",
    "title": "A Novel Defense Against Poisoning Attacks on Federated Learning: LayerCAM Augmented with Autoencoder",
    "abstract": "Recent attacks on federated learning (FL) can introduce malicious model updates that circumvent widely adopted Euclidean distance-based detection methods. This paper proposes a novel defense strategy, referred to as LayerCAM-AE, designed to counteract model poisoning in federated learning. The LayerCAM-AE puts forth a new Layer Class Activation Mapping (LayerCAM) integrated with an autoencoder (AE), significantly enhancing detection capabilities. Specifically, LayerCAM-AE generates a heat map for each local model update, which is then transformed into a more compact visual format. The autoencoder is designed to process the LayerCAM heat maps from the local model updates, improving their distinctiveness and thereby increasing the accuracy in spotting anomalous maps and malicious local models. To address the risk of misclassifications with LayerCAM-AE, a voting algorithm is developed, where a local model update is flagged as malicious if its heat maps are consistently suspicious over several rounds of communication. Extensive tests of LayerCAM-AE on the SVHN and CIFAR-100 datasets are performed under both Independent and Identically Distributed (IID) and non-IID settings in comparison with existing ResNet-50 and REGNETY-800MF defense models. Experimental results show that LayerCAM-AE increases detection rates (Recall: 1.0, Precision: 1.0, FPR: 0.0, Accuracy: 1.0, F1 score: 1.0, AUC: 1.0) and test accuracy in FL, surpassing the performance of both the ResNet-50 and REGNETY-800MF. Our code is available at: https://github.com/jjzgeeks/LayerCAM-AE",
    "authors": [
      "Jingjing Zheng",
      "Xin Yuan",
      "Kai Li",
      "Wei Ni",
      "Eduardo Tovar",
      "Jon Crowcroft"
    ],
    "url": "http://arxiv.org/abs/2406.02605v1",
    "timestamp": 1717331832,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "038d0f61-55c2-4dc3-8bf7-2ad089cb6747": {
    "pk": "038d0f61-55c2-4dc3-8bf7-2ad089cb6747",
    "title": "Redefining Contributions: Shapley-Driven Federated Learning",
    "abstract": "Federated learning (FL) has emerged as a pivotal approach in machine learning, enabling multiple participants to collaboratively train a global model without sharing raw data. While FL finds applications in various domains such as healthcare and finance, it is challenging to ensure global model convergence when participants do not contribute equally and/or honestly. To overcome this challenge, principled mechanisms are required to evaluate the contributions made by individual participants in the FL setting. Existing solutions for contribution assessment rely on general accuracy evaluation, often failing to capture nuanced dynamics and class-specific influences. This paper proposes a novel contribution assessment method called ShapFed for fine-grained evaluation of participant contributions in FL. Our approach uses Shapley values from cooperative game theory to provide a granular understanding of class-specific influences. Based on ShapFed, we introduce a weighted aggregation method called ShapFed-WA, which outperforms conventional federated averaging, especially in class-imbalanced scenarios. Personalizing participant updates based on their contributions further enhances collaborative fairness by delivering differentiated models commensurate with the participant contributions. Experiments on CIFAR-10, Chest X-Ray, and Fed-ISIC2019 datasets demonstrate the effectiveness of our approach in improving utility, efficiency, and fairness in FL systems. The code can be found at https://github.com/tnurbek/shapfed.",
    "authors": [
      "Nurbek Tastan",
      "Samar Fares",
      "Toluwani Aremu",
      "Samuel Horvath",
      "Karthik Nandakumar"
    ],
    "url": "http://arxiv.org/abs/2406.00569v1",
    "timestamp": 1717281631,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "209a9e25-750d-42dd-8afc-efc697febd12": {
    "pk": "209a9e25-750d-42dd-8afc-efc697febd12",
    "title": "Federated Model Heterogeneous Matryoshka Representation Learning",
    "abstract": "Model heterogeneous federated learning (MHeteroFL) enables FL clients to collaboratively train models with heterogeneous structures in a distributed fashion. However, existing MHeteroFL methods rely on training loss to transfer knowledge between the client model and the server model, resulting in limited knowledge exchange. To address this limitation, we propose the Federated model heterogeneous Matryoshka Representation Learning (FedMRL) approach for supervised learning tasks. It adds an auxiliary small homogeneous model shared by clients with heterogeneous local models. (1) The generalized and personalized representations extracted by the two models' feature extractors are fused by a personalized lightweight representation projector. This step enables representation fusion to adapt to local data distribution. (2) The fused representation is then used to construct Matryoshka representations with multi-dimensional and multi-granular embedded representations learned by the global homogeneous model header and the local heterogeneous model header. This step facilitates multi-perspective representation learning and improves model learning capability. Theoretical analysis shows that FedMRL achieves a $O(1/T)$ non-convex convergence rate. Extensive experiments on benchmark datasets demonstrate its superior model accuracy with low communication and computational costs compared to seven state-of-the-art baselines. It achieves up to 8.48% and 24.94% accuracy improvement compared with the state-of-the-art and the best same-category baseline, respectively.",
    "authors": [
      "Liping Yi",
      "Han Yu",
      "Chao Ren",
      "Gang Wang",
      "Xiaoguang Liu",
      "Xiaoxiao Li"
    ],
    "url": "http://arxiv.org/abs/2406.00488v1",
    "timestamp": 1717259828,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6fa7e8d3-a7bc-4fdb-969b-e6bbaa13a011": {
    "pk": "6fa7e8d3-a7bc-4fdb-969b-e6bbaa13a011",
    "title": "SpaFL: Communication-Efficient Federated Learning with Sparse Models and Low computational Overhead",
    "abstract": "The large communication and computation overhead of federated learning (FL) is one of the main challenges facing its practical deployment over resource-constrained clients and systems. In this work, SpaFL: a communication-efficient FL framework is proposed to optimize sparse model structures with low computational overhead. In SpaFL, a trainable threshold is defined for each filter/neuron to prune its all connected parameters, thereby leading to structured sparsity. To optimize the pruning process itself, only thresholds are communicated between a server and clients instead of parameters, thereby learning how to prune. Further, global thresholds are used to update model parameters by extracting aggregated parameter importance. The generalization bound of SpaFL is also derived, thereby proving key insights on the relation between sparsity and performance. Experimental results show that SpaFL improves accuracy while requiring much less communication and computing resources compared to sparse baselines.",
    "authors": [
      "Minsu Kim",
      "Walid Saad",
      "Merouane Debbah",
      "Choong Seon Hong"
    ],
    "url": "http://arxiv.org/abs/2406.00431v1",
    "timestamp": 1717247435,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e65d619a-bdf8-4a8f-93b0-1ef797d2c502": {
    "pk": "e65d619a-bdf8-4a8f-93b0-1ef797d2c502",
    "title": "Non-Federated Multi-Task Split Learning for Heterogeneous Sources",
    "abstract": "With the development of edge networks and mobile computing, the need to serve heterogeneous data sources at the network edge requires the design of new distributed machine learning mechanisms. As a prevalent approach, Federated Learning (FL) employs parameter-sharing and gradient-averaging between clients and a server. Despite its many favorable qualities, such as convergence and data-privacy guarantees, it is well-known that classic FL fails to address the challenge of data heterogeneity and computation heterogeneity across clients. Most existing works that aim to accommodate such sources of heterogeneity stay within the FL operation paradigm, with modifications to overcome the negative effect of heterogeneous data. In this work, as an alternative paradigm, we propose a Multi-Task Split Learning (MTSL) framework, which combines the advantages of Split Learning (SL) with the flexibility of distributed network architectures. In contrast to the FL counterpart, in this paradigm, heterogeneity is not an obstacle to overcome, but a useful property to take advantage of. As such, this work aims to introduce a new architecture and methodology to perform multi-task learning for heterogeneous data sources efficiently, with the hope of encouraging the community to further explore the potential advantages we reveal. To support this promise, we first show through theoretical analysis that MTSL can achieve fast convergence by tuning the learning rate of the server and clients. Then, we compare the performance of MTSL with existing multi-task FL methods numerically on several image classification datasets to show that MTSL has advantages over FL in training speed, communication cost, and robustness to heterogeneous data.",
    "authors": [
      "Yilin Zheng",
      "Atilla Eryilmaz"
    ],
    "url": "http://arxiv.org/abs/2406.00150v1",
    "timestamp": 1717183623,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f08c24e9-8189-4d4b-871f-3c44135c5c85": {
    "pk": "f08c24e9-8189-4d4b-871f-3c44135c5c85",
    "title": "Communication-Efficient Distributed Deep Learning via Federated Dynamic Averaging",
    "abstract": "Driven by the ever-growing volume and decentralized nature of data, coupled with the need to harness this data and generate knowledge from it, has led to the extensive use of distributed deep learning (DDL) techniques for training. These techniques rely on local training that is performed at the distributed nodes based on locally collected data, followed by a periodic synchronization process that combines these models to create a global model. However, frequent synchronization of DL models, encompassing millions to many billions of parameters, creates a communication bottleneck, severely hindering scalability. Worse yet, DDL algorithms typically waste valuable bandwidth, and make themselves less practical in bandwidth-constrained federated settings, by relying on overly simplistic, periodic, and rigid synchronization schedules. These drawbacks also have a direct impact on the time required for the training process, necessitating excessive time for data communication. To address these shortcomings, we propose Federated Dynamic Averaging (FDA), a communication-efficient DDL strategy that dynamically triggers synchronization based on the value of the model variance. In essence, the costly synchronization step is triggered only if the local models, which are initialized from a common global model after each synchronization, have significantly diverged. This decision is facilitated by the communication of a small local state from each distributed node/worker. Through extensive experiments across a wide range of learning tasks we demonstrate that FDA reduces communication cost by orders of magnitude, compared to both traditional and cutting-edge communication-efficient algorithms. Additionally, we show that FDA maintains robust performance across diverse data heterogeneity settings.",
    "authors": [
      "Michail Theologitis",
      "Georgios Frangias",
      "Georgios Anestis",
      "Vasilis Samoladas",
      "Antonios Deligiannakis"
    ],
    "url": "http://arxiv.org/abs/2405.20988v2",
    "timestamp": 1717173251,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "71fef179-e1bc-4a5d-a3d3-61773d204027": {
    "pk": "71fef179-e1bc-4a5d-a3d3-61773d204027",
    "title": "ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning",
    "abstract": "In Federated Learning (FL), a set of clients collaboratively train a machine learning model (called global model) without sharing their local training data. The local training data of clients is typically non-i.i.d. and heterogeneous, resulting in varying contributions from individual clients to the final performance of the global model. In response, many contribution evaluation methods were proposed, where the server could evaluate the contribution made by each client and incentivize the high-contributing clients to sustain their long-term participation in FL. Existing studies mainly focus on developing new metrics or algorithms to better measure the contribution of each client. However, the security of contribution evaluation methods of FL operating in adversarial environments is largely unexplored. In this paper, we propose the first model poisoning attack on contribution evaluation methods in FL, termed ACE. Specifically, we show that any malicious client utilizing ACE could manipulate the parameters of its local model such that it is evaluated to have a high contribution by the server, even when its local training data is indeed of low quality. We perform both theoretical analysis and empirical evaluations of ACE. Theoretically, we show our design of ACE can effectively boost the malicious client's perceived contribution when the server employs the widely-used cosine distance metric to measure contribution. Empirically, our results show ACE effectively and efficiently deceive five state-of-the-art contribution evaluation methods. In addition, ACE preserves the accuracy of the final global models on testing inputs. We also explore six countermeasures to defend ACE. Our results show they are inadequate to thwart ACE, highlighting the urgent need for new defenses to safeguard the contribution evaluation methods in FL.",
    "authors": [
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Luyao Niu",
      "Jinyuan Jia",
      "Bo Li",
      "Radha Poovendran"
    ],
    "url": "http://arxiv.org/abs/2405.20975v2",
    "timestamp": 1717172515,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ecebe31a-8a6c-4603-855e-8302c720aba3": {
    "pk": "ecebe31a-8a6c-4603-855e-8302c720aba3",
    "title": "Sheaf HyperNetworks for Personalized Federated Learning",
    "abstract": "Graph hypernetworks (GHNs), constructed by combining graph neural networks (GNNs) with hypernetworks (HNs), leverage relational data across various domains such as neural architecture search, molecular property prediction and federated learning. Despite GNNs and HNs being individually successful, we show that GHNs present problems compromising their performance, such as over-smoothing and heterophily. Moreover, we cannot apply GHNs directly to personalized federated learning (PFL) scenarios, where a priori client relation graph may be absent, private, or inaccessible. To mitigate these limitations in the context of PFL, we propose a novel class of HNs, sheaf hypernetworks (SHNs), which combine cellular sheaf theory with HNs to improve parameter sharing for PFL. We thoroughly evaluate SHNs across diverse PFL tasks, including multi-class classification, traffic and weather forecasting. Additionally, we provide a methodology for constructing client relation graphs in scenarios where such graphs are unavailable. We show that SHNs consistently outperform existing PFL solutions in complex non-IID scenarios. While the baselines' performance fluctuates depending on the task, SHNs show improvements of up to 2.7% in accuracy and 5.3% in lower mean squared error over the best-performing baseline.",
    "authors": [
      "Bao Nguyen",
      "Lorenzo Sani",
      "Xinchi Qiu",
      "Pietro Li\u00f2",
      "Nicholas D. Lane"
    ],
    "url": "http://arxiv.org/abs/2405.20882v1",
    "timestamp": 1717167338,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "66c0bba6-835d-4791-b3a0-922087bcc1d1": {
    "pk": "66c0bba6-835d-4791-b3a0-922087bcc1d1",
    "title": "BackdoorIndicator: Leveraging OOD Data for Proactive Backdoor Detection in Federated Learning",
    "abstract": "In a federated learning (FL) system, decentralized data owners (clients) could upload their locally trained models to a central server, to jointly train a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing misclassification to a target class when encountering attacker-defined triggers. Existing backdoor defenses show inconsistent performance under different system and adversarial settings, especially when the malicious updates are made statistically close to the benign ones. In this paper, we first reveal the fact that planting subsequent backdoors with the same target label could significantly help to maintain the accuracy of previously planted backdoors, and then propose a novel proactive backdoor detection mechanism for FL named BackdoorIndicator, which has the server inject indicator tasks into the global model leveraging out-of-distribution (OOD) data, and then utilizing the fact that any backdoor samples are OOD samples with respect to benign samples, the server, who is completely agnostic of the potential backdoor types and target labels, can accurately detect the presence of backdoors in uploaded models, via evaluating the indicator tasks. We perform systematic and extensive empirical studies to demonstrate the consistently superior performance and practicality of BackdoorIndicator over baseline defenses, across a wide range of system and adversarial settings.",
    "authors": [
      "Songze Li",
      "Yanbo Dai"
    ],
    "url": "http://arxiv.org/abs/2405.20862v1",
    "timestamp": 1717166697,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "aac429cb-9b39-475c-bb31-39d1aef6b963": {
    "pk": "aac429cb-9b39-475c-bb31-39d1aef6b963",
    "title": "Pursuing Overall Welfare in Federated Learning through Sequential Decision Making",
    "abstract": "In traditional federated learning, a single global model cannot perform equally well for all clients. Therefore, the need to achieve the client-level fairness in federated system has been emphasized, which can be realized by modifying the static aggregation scheme for updating the global model to an adaptive one, in response to the local signals of the participating clients. Our work reveals that existing fairness-aware aggregation strategies can be unified into an online convex optimization framework, in other words, a central server's sequential decision making process. To enhance the decision making capability, we propose simple and intuitive improvements for suboptimal designs within existing methods, presenting AAggFF. Considering practical requirements, we further subdivide our method tailored for the cross-device and the cross-silo settings, respectively. Theoretical analyses guarantee sublinear regret upper bounds for both settings: $\\mathcal{O}(\\sqrt{T \\log{K}})$ for the cross-device setting, and $\\mathcal{O}(K \\log{T})$ for the cross-silo setting, with $K$ clients and $T$ federation rounds. Extensive experiments demonstrate that the federated system equipped with AAggFF achieves better degree of client-level fairness than existing methods in both practical settings. Code is available at https://github.com/vaseline555/AAggFF",
    "authors": [
      "Seok-Ju Hahn",
      "Gi-Soo Kim",
      "Junghye Lee"
    ],
    "url": "http://arxiv.org/abs/2405.20821v1",
    "timestamp": 1717164944,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b50dcf87-2e4b-4f44-ba2d-614b38a07fa3": {
    "pk": "b50dcf87-2e4b-4f44-ba2d-614b38a07fa3",
    "title": "Share Your Secrets for Privacy! Confidential Forecasting with Vertical Federated Learning",
    "abstract": "Vertical federated learning (VFL) is a promising area for time series forecasting in industrial applications, such as predictive maintenance and machine control. Critical challenges to address in manufacturing include data privacy and over-fitting on small and noisy datasets during both training and inference. Additionally, to increase industry adaptability, such forecasting models must scale well with the number of parties while ensuring strong convergence and low-tuning complexity. We address those challenges and propose 'Secret-shared Time Series Forecasting with VFL' (STV), a novel framework that exhibits the following key features: i) a privacy-preserving algorithm for forecasting with SARIMAX and autoregressive trees on vertically partitioned data; ii) serverless forecasting using secret sharing and multi-party computation; iii) novel N-party algorithms for matrix multiplication and inverse operations for direct parameter optimization, giving strong convergence with minimal hyperparameter tuning complexity. We conduct evaluations on six representative datasets from public and industry-specific contexts. Our results demonstrate that STV's forecasting accuracy is comparable to those of centralized approaches. They also show that our direct optimization can outperform centralized methods, which include state-of-the-art diffusion models and long-short-term memory, by 23.81% on forecasting accuracy. We also conduct a scalability analysis by examining the communication costs of direct and iterative optimization to navigate the choice between the two. Code and appendix are available: https://github.com/adis98/STV",
    "authors": [
      "Aditya Shankar",
      "Lydia Y. Chen",
      "J\u00e9r\u00e9mie Decouchant",
      "Dimitra Gkorou",
      "Rihan Hai"
    ],
    "url": "http://arxiv.org/abs/2405.20761v1",
    "timestamp": 1717158458,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7ed2d1f1-925a-4bd1-98d4-3276da9a60bd": {
    "pk": "7ed2d1f1-925a-4bd1-98d4-3276da9a60bd",
    "title": "GANcrop: A Contrastive Defense Against Backdoor Attacks in Federated Learning",
    "abstract": "With heightened awareness of data privacy protection, Federated Learning (FL) has attracted widespread attention as a privacy-preserving distributed machine learning method. However, the distributed nature of federated learning also provides opportunities for backdoor attacks, where attackers can guide the model to produce incorrect predictions without affecting the global model training process.   This paper introduces a novel defense mechanism against backdoor attacks in federated learning, named GANcrop. This approach leverages contrastive learning to deeply explore the disparities between malicious and benign models for attack identification, followed by the utilization of Generative Adversarial Networks (GAN) to recover backdoor triggers and implement targeted mitigation strategies. Experimental findings demonstrate that GANcrop effectively safeguards against backdoor attacks, particularly in non-IID scenarios, while maintaining satisfactory model accuracy, showcasing its remarkable defensive efficacy and practical utility.",
    "authors": [
      "Xiaoyun Gan",
      "Shanyu Gan",
      "Taizhi Su",
      "Peng Liu"
    ],
    "url": "http://arxiv.org/abs/2405.20727v1",
    "timestamp": 1717147996,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "63b63f90-bf3e-4945-a502-4c3d78fb4ff8": {
    "pk": "63b63f90-bf3e-4945-a502-4c3d78fb4ff8",
    "title": "Prune at the Clients, Not the Server: Accelerated Sparse Training in Federated Learning",
    "abstract": "In the recent paradigm of Federated Learning (FL), multiple clients train a shared model while keeping their local data private. Resource constraints of clients and communication costs pose major problems for training large models in FL. On the one hand, addressing the resource limitations of the clients, sparse training has proven to be a powerful tool in the centralized setting. On the other hand, communication costs in FL can be addressed by local training, where each client takes multiple gradient steps on its local data. Recent work has shown that local training can provably achieve the optimal accelerated communication complexity [Mishchenko et al., 2022]. Hence, one would like an accelerated sparse training algorithm. In this work we show that naive integration of sparse training and acceleration at the server fails, and how to fix it by letting the clients perform these tasks appropriately. We introduce Sparse-ProxSkip, our method developed for the nonconvex setting, inspired by RandProx [Condat and Richt\\'arik, 2022], which provably combines sparse training and acceleration in the convex setting. We demonstrate the good performance of Sparse-ProxSkip in extensive experiments.",
    "authors": [
      "Georg Meinhardt",
      "Kai Yi",
      "Laurent Condat",
      "Peter Richt\u00e1rik"
    ],
    "url": "http://arxiv.org/abs/2405.20623v1",
    "timestamp": 1717132872,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7ff94ef2-030d-4bc4-955d-3e15e7570dbe": {
    "pk": "7ff94ef2-030d-4bc4-955d-3e15e7570dbe",
    "title": "Selective Knowledge Sharing for Personalized Federated Learning Under Capacity Heterogeneity",
    "abstract": "Federated Learning (FL) stands to gain significant advantages from collaboratively training capacity-heterogeneous models, enabling the utilization of private data and computing power from low-capacity devices. However, the focus on personalizing capacity-heterogeneous models based on client-specific data has been limited, resulting in suboptimal local model utility, particularly for low-capacity clients. The heterogeneity in both data and device capacity poses two key challenges for model personalization: 1) accurately retaining necessary knowledge embedded within reduced submodels for each client, and 2) effectively sharing knowledge through aggregating size-varying parameters. To this end, we introduce Pa3dFL, a novel framework designed to enhance local model performance by decoupling and selectively sharing knowledge among capacity-heterogeneous models. First, we decompose each layer of the model into general and personal parameters. Then, we maintain uniform sizes for the general parameters across clients and aggregate them through direct averaging. Subsequently, we employ a hyper-network to generate size-varying personal parameters for clients using learnable embeddings. Finally, we facilitate the implicit aggregation of personal parameters by aggregating client embeddings through a self-attention module. We conducted extensive experiments on three datasets to evaluate the effectiveness of Pa3dFL. Our findings indicate that Pa3dFL consistently outperforms baseline methods across various heterogeneity settings. Moreover, Pa3dFL demonstrates competitive communication and computation efficiency compared to baseline approaches, highlighting its practicality and adaptability in adverse system conditions.",
    "authors": [
      "Zheng Wang",
      "Zheng Wang",
      "Zhaopeng Peng",
      "Zihui Wang",
      "Cheng Wang"
    ],
    "url": "http://arxiv.org/abs/2405.20589v1",
    "timestamp": 1717124365,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "79a821df-67b7-47c7-9f03-70947eecbab4": {
    "pk": "79a821df-67b7-47c7-9f03-70947eecbab4",
    "title": "FCOM: A Federated Collaborative Online Monitoring Framework via Representation Learning",
    "abstract": "Online learning has demonstrated notable potential to dynamically allocate limited resources to monitor a large population of processes, effectively balancing the exploitation of processes yielding high rewards, and the exploration of uncertain processes. However, most online learning algorithms were designed under 1) a centralized setting that requires data sharing across processes to obtain an accurate prediction or 2) a homogeneity assumption that estimates a single global model from the decentralized data. To facilitate the online learning of heterogeneous processes from the decentralized data, we propose a federated collaborative online monitoring method, which captures the latent representative models inherent in the population through representation learning and designs a novel federated collaborative UCB algorithm to estimate the representative models from sequentially observed decentralized data. The efficiency of our method is illustrated through theoretical analysis, simulation studies, and decentralized cognitive degradation monitoring in Alzheimer's disease.",
    "authors": [
      "Tanapol Kosolwattana",
      "Huazheng Wang",
      "Raed Al Kontar",
      "Ying Lin"
    ],
    "url": "http://arxiv.org/abs/2405.20504v1",
    "timestamp": 1717105754,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e822f616-567c-4fc2-9a9f-491786baaaa6": {
    "pk": "e822f616-567c-4fc2-9a9f-491786baaaa6",
    "title": "Exploring the Practicality of Federated Learning: A Survey Towards the Communication Perspective",
    "abstract": "Federated Learning (FL) is a promising paradigm that offers significant advancements in privacy-preserving, decentralized machine learning by enabling collaborative training of models across distributed devices without centralizing data. However, the practical deployment of FL systems faces a significant bottleneck: the communication overhead caused by frequently exchanging large model updates between numerous devices and a central server. This communication inefficiency can hinder training speed, model performance, and the overall feasibility of real-world FL applications. In this survey, we investigate various strategies and advancements made in communication-efficient FL, highlighting their impact and potential to overcome the communication challenges inherent in FL systems. Specifically, we define measures for communication efficiency, analyze sources of communication inefficiency in FL systems, and provide a taxonomy and comprehensive review of state-of-the-art communication-efficient FL methods. Additionally, we discuss promising future research directions for enhancing the communication efficiency of FL systems. By addressing the communication bottleneck, FL can be effectively applied and enable scalable and practical deployment across diverse applications that require privacy-preserving, decentralized machine learning, such as IoT, healthcare, or finance.",
    "authors": [
      "Khiem Le",
      "Nhan Luong-Ha",
      "Manh Nguyen-Duc",
      "Danh Le-Phuoc",
      "Cuong Do",
      "Kok-Seng Wong"
    ],
    "url": "http://arxiv.org/abs/2405.20431v1",
    "timestamp": 1717096893,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3c9e7563-8ccb-42a0-8ddb-066a0bf09e38": {
    "pk": "3c9e7563-8ccb-42a0-8ddb-066a0bf09e38",
    "title": "Enhancing Performance for Highly Imbalanced Medical Data via Data Regularization in a Federated Learning Setting",
    "abstract": "The increased availability of medical data has significantly impacted healthcare by enabling the application of machine / deep learning approaches in various instances. However, medical datasets are usually small and scattered across multiple providers, suffer from high class-imbalance, and are subject to stringent data privacy constraints. In this paper, the application of a data regularization algorithm, suitable for learning under high class-imbalance, in a federated learning setting is proposed. Specifically, the goal of the proposed method is to enhance model performance for cardiovascular disease prediction by tackling the class-imbalance that typically characterizes datasets used for this purpose, as well as by leveraging patient data available in different nodes of a federated ecosystem without compromising their privacy and enabling more resource sensitive allocation. The method is evaluated across four datasets for cardiovascular disease prediction, which are scattered across different clients, achieving improved performance. Meanwhile, its robustness under various hyperparameter settings, as well as its ability to adapt to different resource allocation scenarios, is verified.",
    "authors": [
      "Georgios Tsoumplekas",
      "Ilias Siniosoglou",
      "Vasileios Argyriou",
      "Ioannis D. Moscholios",
      "Panagiotis Sarigiannidis"
    ],
    "url": "http://arxiv.org/abs/2405.20430v1",
    "timestamp": 1717096538,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "da3a9ad3-7192-41c9-8cb7-c02802adb126": {
    "pk": "da3a9ad3-7192-41c9-8cb7-c02802adb126",
    "title": "Federated and Transfer Learning for Cancer Detection Based on Image Analysis",
    "abstract": "This review article discusses the roles of federated learning (FL) and transfer learning (TL) in cancer detection based on image analysis. These two strategies powered by machine learning have drawn a lot of attention due to their potential to increase the precision and effectiveness of cancer diagnosis in light of the growing importance of machine learning techniques in cancer detection. FL enables the training of machine learning models on data distributed across multiple sites without the need for centralized data sharing, while TL allows for the transfer of knowledge from one task to another. A comprehensive assessment of the two methods, including their strengths, and weaknesses is presented. Moving on, their applications in cancer detection are discussed, including potential directions for the future. Finally, this article offers a thorough description of the functions of TL and FL in image-based cancer detection. The authors also make insightful suggestions for additional study in this rapidly developing area.",
    "authors": [
      "Amine Bechar",
      "Youssef Elmir",
      "Yassine Himeur",
      "Rafik Medjoudj",
      "Abbes Amira"
    ],
    "url": "http://arxiv.org/abs/2405.20126v1",
    "timestamp": 1717081650,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "33da3877-29ed-42e9-9239-d26d016677cf": {
    "pk": "33da3877-29ed-42e9-9239-d26d016677cf",
    "title": "SPAM: Stochastic Proximal Point Method with Momentum Variance Reduction for Non-convex Cross-Device Federated Learning",
    "abstract": "Cross-device training is a crucial subfield of federated learning, where the number of clients can reach into the billions. Standard approaches and local methods are prone to issues such as client drift and insensitivity to data similarities. We propose a novel algorithm (SPAM) for cross-device federated learning with non-convex losses, which solves both issues. We provide sharp analysis under second-order (Hessian) similarity, a condition satisfied by a variety of machine learning problems in practice. Additionally, we extend our results to the partial participation setting, where a cohort of selected clients communicate with the server at each communication round. Our method is the first in its kind, that does not require the smoothness of the objective and provably benefits from clients having similar data.",
    "authors": [
      "Avetik Karagulyan",
      "Egor Shulgin",
      "Abdurakhmon Sadiev",
      "Peter Richt\u00e1rik"
    ],
    "url": "http://arxiv.org/abs/2405.20127v1",
    "timestamp": 1717081650,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.OC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3474fa97-52b0-4805-93c1-32d488d94689": {
    "pk": "3474fa97-52b0-4805-93c1-32d488d94689",
    "title": "Cross-Training with Multi-View Knowledge Fusion for Heterogenous Federated Learning",
    "abstract": "Federated learning benefits from cross-training strategies, which enables models to train on data from distinct sources to improve the generalization capability. However, the data heterogeneity between sources may lead models to gradually forget previously acquired knowledge when undergoing cross-training to adapt to new tasks or data sources. We argue that integrating personalized and global knowledge to gather information from multiple perspectives could potentially improve performance. To achieve this goal, this paper presents a novel approach that enhances federated learning through a cross-training scheme incorporating multi-view information. Specifically, the proposed method, termed FedCT, includes three main modules, where the consistency-aware knowledge broadcasting module aims to optimize model assignment strategies, which enhances collaborative advantages between clients and achieves an efficient federated learning process. The multi-view knowledge-guided representation learning module leverages fused prototypical knowledge from both global and local views to enhance the preservation of local knowledge before and after model exchange, as well as to ensure consistency between local and global knowledge. The mixup-based feature augmentation module aggregates rich information to further increase the diversity of feature spaces, which enables the model to better discriminate complex samples. Extensive experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study. The results demonstrated that FedCT alleviates knowledge forgetting from both local and global views, which enables it outperform state-of-the-art methods.",
    "authors": [
      "Zhuang Qi",
      "Lei Meng",
      "Weihao He",
      "Ruohan Zhang",
      "Yu Wang",
      "Xin Qi",
      "Xiangxu Meng"
    ],
    "url": "http://arxiv.org/abs/2405.20046v1",
    "timestamp": 1717075650,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a67fad4d-28be-46fe-a811-6beeaeb5e667": {
    "pk": "a67fad4d-28be-46fe-a811-6beeaeb5e667",
    "title": "subMFL: Compatiple subModel Generation for Federated Learning in Device Heterogenous Environment",
    "abstract": "Federated Learning (FL) is commonly used in systems with distributed and heterogeneous devices with access to varying amounts of data and diverse computing and storage capacities. FL training process enables such devices to update the weights of a shared model locally using their local data and then a trusted central server combines all of those models to generate a global model. In this way, a global model is generated while the data remains local to devices to preserve privacy. However, training large models such as Deep Neural Networks (DNNs) on resource-constrained devices can take a prohibitively long time and consume a large amount of energy. In the current process, the low-capacity devices are excluded from the training process, although they might have access to unseen data. To overcome this challenge, we propose a model compression approach that enables heterogeneous devices with varying computing capacities to participate in the FL process. In our approach, the server shares a dense model with all devices to train it: Afterwards, the trained model is gradually compressed to obtain submodels with varying levels of sparsity to be used as suitable initial global models for resource-constrained devices that were not capable of train the first dense model. This results in an increased participation rate of resource-constrained devices while the transferred weights from the previous round of training are preserved. Our validation experiments show that despite reaching about 50 per cent global sparsity, generated submodels maintain their accuracy while can be shared to increase participation by around 50 per cent.",
    "authors": [
      "Zeyneddin Oz",
      "Ceylan Soygul Oz",
      "Abdollah Malekjafarian",
      "Nima Afraz",
      "Fatemeh Golpayegani"
    ],
    "url": "http://arxiv.org/abs/2405.20014v1",
    "timestamp": 1717073374,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "61ff1aef-c3e1-437d-a4be-579cc0b9d6c5": {
    "pk": "61ff1aef-c3e1-437d-a4be-579cc0b9d6c5",
    "title": "Federated Learning with Multi-resolution Model Broadcast",
    "abstract": "In federated learning, a server must periodically broadcast a model to the agents. We propose to use multi-resolution coding and modulation (also known as non-uniform modulation) for this purpose. In the simplest instance, broadcast transmission is used, whereby all agents are targeted with one and the same transmission (typically without any particular favored beam direction), which is coded using multi-resolution coding/modulation. This enables high-SNR agents, with high path gains to the server, to receive a more accurate model than the low-SNR agents do, without consuming more downlink resources. As one implementation, we use transmission with a non-uniform 8-PSK constellation, where a high-SNR receiver (agent) can separate all 8 constellation points (hence receive 3 bits) whereas a low-SNR receiver can only separate 4 points (hence receive 2 bits). By encoding the least significant information in the third bit, the high-SNR receivers can obtain the model with higher accuracy, while the low-SNR receiver can still obtain the model although with reduced accuracy, thereby facilitating at least some basic participation of the low-SNR receiver. We show the effectiveness of our proposed scheme via experimentation using federated learning with the MNIST data-set.",
    "authors": [
      "Henrik Ryd\u00e9n",
      "Reza Moosavi",
      "Erik G. Larsson"
    ],
    "url": "http://arxiv.org/abs/2405.19886v1",
    "timestamp": 1717062318,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.NI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f2e74321-ac28-4479-a636-334f9a5eed80": {
    "pk": "f2e74321-ac28-4479-a636-334f9a5eed80",
    "title": "On Vessel Location Forecasting and the Effect of Federated Learning",
    "abstract": "The wide spread of Automatic Identification System (AIS) has motivated several maritime analytics operations. Vessel Location Forecasting (VLF) is one of the most critical operations for maritime awareness. However, accurate VLF is a challenging problem due to the complexity and dynamic nature of maritime traffic conditions. Furthermore, as privacy concerns and restrictions have grown, training data has become increasingly fragmented, resulting in dispersed databases of several isolated data silos among different organizations, which in turn decreases the quality of learning models. In this paper, we propose an efficient VLF solution based on LSTM neural networks, in two variants, namely Nautilus and FedNautilus for the centralized and the federated learning approach, respectively. We also demonstrate the superiority of the centralized approach with respect to current state of the art and discuss the advantages and disadvantages of the federated against the centralized approach.",
    "authors": [
      "Andreas Tritsarolis",
      "Nikos Pelekis",
      "Konstantina Bereta",
      "Dimitris Zissis",
      "Yannis Theodoridis"
    ],
    "url": "http://arxiv.org/abs/2405.19870v1",
    "timestamp": 1717061028,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e6ad96b4-b6b5-48c3-9a38-830e340cfc97": {
    "pk": "e6ad96b4-b6b5-48c3-9a38-830e340cfc97",
    "title": "Estimating before Debiasing: A Bayesian Approach to Detaching Prior Bias in Federated Semi-Supervised Learning",
    "abstract": "Federated Semi-Supervised Learning (FSSL) leverages both labeled and unlabeled data on clients to collaboratively train a model.In FSSL, the heterogeneous data can introduce prediction bias into the model, causing the model's prediction to skew towards some certain classes. Existing FSSL methods primarily tackle this issue by enhancing consistency in model parameters or outputs. However, as the models themselves are biased, merely constraining their consistency is not sufficient to alleviate prediction bias. In this paper, we explore this bias from a Bayesian perspective and demonstrate that it principally originates from label prior bias within the training data. Building upon this insight, we propose a debiasing method for FSSL named FedDB. FedDB utilizes the Average Prediction Probability of Unlabeled Data (APP-U) to approximate the biased prior.During local training, FedDB employs APP-U to refine pseudo-labeling through Bayes' theorem, thereby significantly reducing the label prior bias. Concurrently, during the model aggregation, FedDB uses APP-U from participating clients to formulate unbiased aggregate weights, thereby effectively diminishing bias in the global model. Experimental results show that FedDB can surpass existing FSSL methods. The code is available at https://github.com/GuogangZhu/FedDB.",
    "authors": [
      "Guogang Zhu",
      "Xuefeng Liu",
      "Xinghao Wu",
      "Shaojie Tang",
      "Chao Tang",
      "Jianwei Niu",
      "Hao Su"
    ],
    "url": "http://arxiv.org/abs/2405.19789v1",
    "timestamp": 1717055881,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a0de1212-a231-4f7e-ba79-1c65f48e3e05": {
    "pk": "a0de1212-a231-4f7e-ba79-1c65f48e3e05",
    "title": "Momentum for the Win: Collaborative Federated Reinforcement Learning across Heterogeneous Environments",
    "abstract": "We explore a Federated Reinforcement Learning (FRL) problem where $N$ agents collaboratively learn a common policy without sharing their trajectory data. To date, existing FRL work has primarily focused on agents operating in the same or ``similar\" environments. In contrast, our problem setup allows for arbitrarily large levels of environment heterogeneity. To obtain the optimal policy which maximizes the average performance across all potentially completely different environments, we propose two algorithms: FedSVRPG-M and FedHAPG-M. In contrast to existing results, we demonstrate that both FedSVRPG-M and FedHAPG-M, both of which leverage momentum mechanisms, can exactly converge to a stationary point of the average performance function, regardless of the magnitude of environment heterogeneity. Furthermore, by incorporating the benefits of variance-reduction techniques or Hessian approximation, both algorithms achieve state-of-the-art convergence results, characterized by a sample complexity of $\\mathcal{O}\\left(\\epsilon^{-\\frac{3}{2}}/N\\right)$. Notably, our algorithms enjoy linear convergence speedups with respect to the number of agents, highlighting the benefit of collaboration among agents in finding a common policy.",
    "authors": [
      "Han Wang",
      "Sihong He",
      "Zhili Zhang",
      "Fei Miao",
      "James Anderson"
    ],
    "url": "http://arxiv.org/abs/2405.19499v1",
    "timestamp": 1717014282,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5542802b-d3ca-492c-903d-ce7c92308ea0": {
    "pk": "5542802b-d3ca-492c-903d-ce7c92308ea0",
    "title": "Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering",
    "abstract": "Federated Learning (FL) is a decentralized machine learning (ML) approach that keeps data localized and often incorporates Differential Privacy (DP) to enhance privacy guarantees. Similar to previous work on DP in ML, we observed that differentially private federated learning (DPFL) introduces performance disparities, particularly affecting minority groups. Recent work has attempted to address performance fairness in vanilla FL through clustering, but this method remains sensitive and prone to errors, which are further exacerbated by the DP noise in DPFL. To fill this gap, in this paper, we propose a novel clustered DPFL algorithm designed to effectively identify clients' clusters in highly heterogeneous settings while maintaining high accuracy with DP guarantees. To this end, we propose to cluster clients based on both their model updates and training loss values. Our proposed approach also addresses the server's uncertainties in clustering clients' model updates by employing larger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the impact of noise and potential clustering errors, especially in privacy-sensitive scenarios. We provide theoretical analysis of the effectiveness of our proposed approach. We also extensively evaluate our approach across diverse data distributions and privacy budgets and show its effectiveness in mitigating the disparate impact of DP in FL settings with a small computational cost.",
    "authors": [
      "Saber Malekmohammadi",
      "Afaf Taik",
      "Golnoosh Farnadi"
    ],
    "url": "http://arxiv.org/abs/2405.19272v1",
    "timestamp": 1717002211,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "855ca813-2cc8-41e5-aa60-65815f95e32d": {
    "pk": "855ca813-2cc8-41e5-aa60-65815f95e32d",
    "title": "LoByITFL: Low Communication Secure and Private Federated Learning",
    "abstract": "Federated Learning (FL) faces several challenges, such as the privacy of the clients data and security against Byzantine clients. Existing works treating privacy and security jointly make sacrifices on the privacy guarantee. In this work, we introduce LoByITFL, the first communication-efficient Information-Theoretic (IT) private and secure FL scheme that makes no sacrifices on the privacy guarantees while ensuring security against Byzantine adversaries. The key ingredients are a small and representative dataset available to the federator, a careful transformation of the FLTrust algorithm and the use of a trusted third party only in a one-time preprocessing phase before the start of the learning algorithm. We provide theoretical guarantees on privacy and Byzantine-resilience, and provide convergence guarantee and experimental results validating our theoretical findings.",
    "authors": [
      "Yue Xia",
      "Christoph Hofmeister",
      "Maximilian Egger",
      "Rawad Bitar"
    ],
    "url": "http://arxiv.org/abs/2405.19217v1",
    "timestamp": 1716998419,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IT",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b0e823de-52aa-4662-b8f6-04bea505c4b8": {
    "pk": "b0e823de-52aa-4662-b8f6-04bea505c4b8",
    "title": "FedMAP: Unlocking Potential in Personalized Federated Learning through Bi-Level MAP Optimization",
    "abstract": "Federated Learning (FL) enables collaborative training of machine learning models on decentralized data while preserving data privacy. However, data across clients often differs significantly due to class imbalance, feature distribution skew, sample size imbalance, and other phenomena. Leveraging information from these not identically distributed (non-IID) datasets poses substantial challenges. FL methods based on a single global model cannot effectively capture the variations in client data and underperform in non-IID settings. Consequently, Personalized FL (PFL) approaches that adapt to each client's data distribution but leverage other clients' data are essential but currently underexplored. We propose a novel Bayesian PFL framework using bi-level optimization to tackle the data heterogeneity challenges. Our proposed framework utilizes the global model as a prior distribution within a Maximum A Posteriori (MAP) estimation of personalized client models. This approach facilitates PFL by integrating shared knowledge from the prior, thereby enhancing local model performance, generalization ability, and communication efficiency. We extensively evaluated our bi-level optimization approach on real-world and synthetic datasets, demonstrating significant improvements in model accuracy compared to existing methods while reducing communication overhead. This study contributes to PFL by establishing a solid theoretical foundation for the proposed method and offering a robust, ready-to-use framework that effectively addresses the challenges posed by non-IID data in FL.",
    "authors": [
      "Fan Zhang",
      "Carlos Esteve-Yag\u00fce",
      "S\u00f6ren Dittmer",
      "Carola-Bibiane Sch\u00f6nlieb",
      "Michael Roberts"
    ],
    "url": "http://arxiv.org/abs/2405.19000v1",
    "timestamp": 1716982086,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "37886768-9e05-4a1d-9d4c-a9d0e570a8c3": {
    "pk": "37886768-9e05-4a1d-9d4c-a9d0e570a8c3",
    "title": "Federated Learning under Partially Class-Disjoint Data via Manifold Reshaping",
    "abstract": "Statistical heterogeneity severely limits the performance of federated learning (FL), motivating several explorations e.g., FedProx, MOON and FedDyn, to alleviate this problem. Despite effectiveness, their considered scenario generally requires samples from almost all classes during the local training of each client, although some covariate shifts may exist among clients. In fact, the natural case of partially class-disjoint data (PCDD), where each client contributes a few classes (instead of all classes) of samples, is practical yet underexplored. Specifically, the unique collapse and invasion characteristics of PCDD can induce the biased optimization direction in local training, which prevents the efficiency of federated learning. To address this dilemma, we propose a manifold reshaping approach called FedMR to calibrate the feature space of local training. Our FedMR adds two interplaying losses to the vanilla federated learning: one is intra-class loss to decorrelate feature dimensions for anti-collapse; and the other one is inter-class loss to guarantee the proper margin among categories in the feature expansion. We conduct extensive experiments on a range of datasets to demonstrate that our FedMR achieves much higher accuracy and better communication efficiency. Source code is available at: https://github.com/MediaBrain-SJTU/FedMR.git.",
    "authors": [
      "Ziqing Fan",
      "Jiangchao Yao",
      "Ruipeng Zhang",
      "Lingjuan Lyu",
      "Ya Zhang",
      "Yanfeng Wang"
    ],
    "url": "http://arxiv.org/abs/2405.18983v2",
    "timestamp": 1716980173,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b3aa6bd9-2d10-4a1e-b944-6afb432ee561": {
    "pk": "b3aa6bd9-2d10-4a1e-b944-6afb432ee561",
    "title": "Federated Learning with Bilateral Curation for Partially Class-Disjoint Data",
    "abstract": "Partially class-disjoint data (PCDD), a common yet under-explored data formation where each client contributes a part of classes (instead of all classes) of samples, severely challenges the performance of federated algorithms. Without full classes, the local objective will contradict the global objective, yielding the angle collapse problem for locally missing classes and the space waste problem for locally existing classes. As far as we know, none of the existing methods can intrinsically mitigate PCDD challenges to achieve holistic improvement in the bilateral views (both global view and local view) of federated learning. To address this dilemma, we are inspired by the strong generalization of simplex Equiangular Tight Frame~(ETF) on the imbalanced data, and propose a novel approach called FedGELA where the classifier is globally fixed as a simplex ETF while locally adapted to the personal distributions. Globally, FedGELA provides fair and equal discrimination for all classes and avoids inaccurate updates of the classifier, while locally it utilizes the space of locally missing classes for locally existing classes. We conduct extensive experiments on a range of datasets to demonstrate that our FedGELA achieves promising performance~(averaged improvement of 3.9% to FedAvg and 1.5% to best baselines) and provide both local and global convergence guarantees. Source code is available at:https://github.com/MediaBrain-SJTU/FedGELA.git.",
    "authors": [
      "Ziqing Fan",
      "Ruipeng Zhang",
      "Jiangchao Yao",
      "Bo Han",
      "Ya Zhang",
      "Yanfeng Wang"
    ],
    "url": "http://arxiv.org/abs/2405.18972v1",
    "timestamp": 1716978884,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "aad88427-0010-4ceb-9fee-acc7b1cf4ffe": {
    "pk": "aad88427-0010-4ceb-9fee-acc7b1cf4ffe",
    "title": "Federated Continual Learning Goes Online: Leveraging Uncertainty for Modality-Agnostic Class-Incremental Learning",
    "abstract": "Given the ability to model more realistic and dynamic problems, Federated Continual Learning (FCL) has been increasingly investigated recently. A well-known problem encountered in this setting is the so-called catastrophic forgetting, for which the learning model is inclined to focus on more recent tasks while forgetting the previously learned knowledge. The majority of the current approaches in FCL propose generative-based solutions to solve said problem. However, this setting requires multiple training epochs over the data, implying an offline setting where datasets are stored locally and remain unchanged over time. Furthermore, the proposed solutions are tailored for vision tasks solely. To overcome these limitations, we propose a new modality-agnostic approach to deal with the online scenario where new data arrive in streams of mini-batches that can only be processed once. To solve catastrophic forgetting, we propose an uncertainty-aware memory-based approach. In particular, we suggest using an estimator based on the Bregman Information (BI) to compute the model's variance at the sample level. Through measures of predictive uncertainty, we retrieve samples with specific characteristics, and - by retraining the model on such samples - we demonstrate the potential of this approach to reduce the forgetting effect in realistic settings.",
    "authors": [
      "Giuseppe Serra",
      "Florian Buettner"
    ],
    "url": "http://arxiv.org/abs/2405.18925v1",
    "timestamp": 1716974979,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "768f8d04-f5fa-4368-8b6d-12c2c094a128": {
    "pk": "768f8d04-f5fa-4368-8b6d-12c2c094a128",
    "title": "Enhancing Security and Privacy in Federated Learning using Update Digests and Voting-Based Defense",
    "abstract": "Federated Learning (FL) is a promising privacy-preserving machine learning paradigm that allows data owners to collaboratively train models while keeping their data localized. Despite its potential, FL faces challenges related to the trustworthiness of both clients and servers, especially in the presence of curious or malicious adversaries. In this paper, we introduce a novel framework named \\underline{\\textbf{F}}ederated \\underline{\\textbf{L}}earning with \\underline{\\textbf{U}}pdate \\underline{\\textbf{D}}igest (FLUD), which addresses the critical issues of privacy preservation and resistance to Byzantine attacks within distributed learning environments. FLUD utilizes an innovative approach, the $\\mathsf{LinfSample}$ method, allowing clients to compute the $l_{\\infty}$ norm across sliding windows of updates as an update digest. This digest enables the server to calculate a shared distance matrix, significantly reducing the overhead associated with Secure Multi-Party Computation (SMPC) by three orders of magnitude while effectively distinguishing between benign and malicious updates. Additionally, FLUD integrates a privacy-preserving, voting-based defense mechanism that employs optimized SMPC protocols to minimize communication rounds. Our comprehensive experiments demonstrate FLUD's effectiveness in countering Byzantine adversaries while incurring low communication and runtime overhead. FLUD offers a scalable framework for secure and reliable FL in distributed environments, facilitating its application in scenarios requiring robust data management and security.",
    "authors": [
      "Wenjie Li",
      "Kai Fan",
      "Jingyuan Zhang",
      "Hui Li",
      "Wei Yang Bryan Lim",
      "Qiang Yang"
    ],
    "url": "http://arxiv.org/abs/2405.18802v1",
    "timestamp": 1716965170,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "859e8ca6-5561-4482-9f6f-3c01f8b00313": {
    "pk": "859e8ca6-5561-4482-9f6f-3c01f8b00313",
    "title": "FlocOff: Data Heterogeneity Resilient Federated Learning with Communication-Efficient Edge Offloading",
    "abstract": "Federated Learning (FL) has emerged as a fundamental learning paradigm to harness massive data scattered at geo-distributed edge devices in a privacy-preserving way. Given the heterogeneous deployment of edge devices, however, their data are usually Non-IID, introducing significant challenges to FL including degraded training accuracy, intensive communication costs, and high computing complexity. Towards that, traditional approaches typically utilize adaptive mechanisms, which may suffer from scalability issues, increased computational overhead, and limited adaptability to diverse edge environments. To address that, this paper instead leverages the observation that the computation offloading involves inherent functionalities such as node matching and service correlation to achieve data reshaping and proposes Federated learning based on computing Offloading (FlocOff) framework, to address data heterogeneity and resource-constrained challenges. Specifically, FlocOff formulates the FL process with Non-IID data in edge scenarios and derives rigorous analysis on the impact of imbalanced data distribution. Based on this, FlocOff decouples the optimization in two steps, namely : (1) Minimizes the Kullback-Leibler (KL) divergence via Computation Offloading scheduling (MKL-CO); (2) Minimizes the Communication Cost through Resource Allocation (MCC-RA). Extensive experimental results demonstrate that the proposed FlocOff effectively improves model convergence and accuracy by 14.3\\%-32.7\\% while reducing data heterogeneity under various data distributions.",
    "authors": [
      "Mulei Ma",
      "Chenyu Gong",
      "Liekang Zeng",
      "Yang Yang",
      "Liantao Wu"
    ],
    "url": "http://arxiv.org/abs/2405.18739v1",
    "timestamp": 1716954832,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.NI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4238f3f1-a90f-4a64-a1db-f9fdfa1ef5f2": {
    "pk": "4238f3f1-a90f-4a64-a1db-f9fdfa1ef5f2",
    "title": "Adaptive and Parallel Split Federated Learning in Vehicular Edge Computing",
    "abstract": "Vehicular edge intelligence (VEI) is a promising paradigm for enabling future intelligent transportation systems by accommodating artificial intelligence (AI) at the vehicular edge computing (VEC) system. Federated learning (FL) stands as one of the fundamental technologies facilitating collaborative model training locally and aggregation, while safeguarding the privacy of vehicle data in VEI. However, traditional FL faces challenges in adapting to vehicle heterogeneity, training large models on resource-constrained vehicles, and remaining susceptible to model weight privacy leakage. Meanwhile, split learning (SL) is proposed as a promising collaborative learning framework which can mitigate the risk of model wights leakage, and release the training workload on vehicles. SL sequentially trains a model between a vehicle and an edge cloud (EC) by dividing the entire model into a vehicle-side model and an EC-side model at a given cut layer. In this work, we combine the advantages of SL and FL to develop an Adaptive Split Federated Learning scheme for Vehicular Edge Computing (ASFV). The ASFV scheme adaptively splits the model and parallelizes the training process, taking into account mobile vehicle selection and resource allocation. Our extensive simulations, conducted on non-independent and identically distributed data, demonstrate that the proposed ASFV solution significantly reduces training latency compared to existing benchmarks, while adapting to network dynamics and vehicles' mobility.",
    "authors": [
      "Xianke Qiang",
      "Zheng Chang",
      "Yun Hu",
      "Lei Liu",
      "Timo Hamalainen"
    ],
    "url": "http://arxiv.org/abs/2405.18707v1",
    "timestamp": 1716950078,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "51cbc6f6-6c14-46c1-985b-05f8cce1d6cb": {
    "pk": "51cbc6f6-6c14-46c1-985b-05f8cce1d6cb",
    "title": "FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning",
    "abstract": "Collaborative fairness stands as an essential element in federated learning to encourage client participation by equitably distributing rewards based on individual contributions. Existing methods primarily focus on adjusting gradient allocations among clients to achieve collaborative fairness. However, they frequently overlook crucial factors such as maintaining consistency across local models and catering to the diverse requirements of high-contributing clients. This oversight inevitably decreases both fairness and model accuracy in practice. To address these issues, we propose FedSAC, a novel Federated learning framework with dynamic Submodel Allocation for Collaborative fairness, backed by a theoretical convergence guarantee. First, we present the concept of \"bounded collaborative fairness (BCF)\", which ensures fairness by tailoring rewards to individual clients based on their contributions. Second, to implement the BCF, we design a submodel allocation module with a theoretical guarantee of fairness. This module incentivizes high-contributing clients with high-performance submodels containing a diverse range of crucial neurons, thereby preserving consistency across local models. Third, we further develop a dynamic aggregation module to adaptively aggregate submodels, ensuring the equitable treatment of low-frequency neurons and consequently enhancing overall model accuracy. Extensive experiments conducted on three public benchmarks demonstrate that FedSAC outperforms all baseline methods in both fairness and model accuracy. We see this work as a significant step towards incentivizing broader client participation in federated learning. The source code is available at https://github.com/wangzihuixmu/FedSAC.",
    "authors": [
      "Zihui Wang",
      "Zheng Wang",
      "Lingjuan Lyu",
      "Zhaopeng Peng",
      "Zhicheng Yang",
      "Chenglu Wen",
      "Rongshan Yu",
      "Cheng Wang",
      "Xiaoliang Fan"
    ],
    "url": "http://arxiv.org/abs/2405.18291v1",
    "timestamp": 1716911009,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7982b673-4fe7-443c-828f-88ebc9a4ad30": {
    "pk": "7982b673-4fe7-443c-828f-88ebc9a4ad30",
    "title": "Towards Communication-efficient Federated Learning via Sparse and Aligned Adaptive Optimization",
    "abstract": "Adaptive moment estimation (Adam), as a Stochastic Gradient Descent (SGD) variant, has gained widespread popularity in federated learning (FL) due to its fast convergence. However, federated Adam (FedAdam) algorithms suffer from a threefold increase in uplink communication overhead compared to federated SGD (FedSGD) algorithms, which arises from the necessity to transmit both local model updates and first and second moment estimates from distributed devices to the centralized server for aggregation. Driven by this issue, we propose a novel sparse FedAdam algorithm called FedAdam-SSM, wherein distributed devices sparsify the updates of local model parameters and moment estimates and subsequently upload the sparse representations to the centralized server. To further reduce the communication overhead, the updates of local model parameters and moment estimates incorporate a shared sparse mask (SSM) into the sparsification process, eliminating the need for three separate sparse masks. Theoretically, we develop an upper bound on the divergence between the local model trained by FedAdam-SSM and the desired model trained by centralized Adam, which is related to sparsification error and imbalanced data distribution. By minimizing the divergence bound between the model trained by FedAdam-SSM and centralized Adam, we optimize the SSM to mitigate the learning performance degradation caused by sparsification error. Additionally, we provide convergence bounds for FedAdam-SSM in both convex and non-convex objective function settings, and investigate the impact of local epoch, learning rate and sparsification ratio on the convergence rate of FedAdam-SSM. Experimental results show that FedAdam-SSM outperforms baselines in terms of convergence rate (over 1.1$\\times$ faster than the sparse FedAdam baselines) and test accuracy (over 14.5\\% ahead of the quantized FedAdam baselines).",
    "authors": [
      "Xiumei Deng",
      "Jun Li",
      "Kang Wei",
      "Long Shi",
      "Zeihui Xiong",
      "Ming Ding",
      "Wen Chen",
      "Shi Jin",
      "H. Vincent Poor"
    ],
    "url": "http://arxiv.org/abs/2405.17932v1",
    "timestamp": 1716883009,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8fc0ea71-0266-4ef5-ac08-18a32c3e0f31": {
    "pk": "8fc0ea71-0266-4ef5-ac08-18a32c3e0f31",
    "title": "Decentralized Directed Collaboration for Personalized Federated Learning",
    "abstract": "Personalized Federated Learning (PFL) is proposed to find the greatest personalized models for each client. To avoid the central failure and communication bottleneck in the server-based FL, we concentrate on the Decentralized Personalized Federated Learning (DPFL) that performs distributed model training in a Peer-to-Peer (P2P) manner. Most personalized works in DPFL are based on undirected and symmetric topologies, however, the data, computation and communication resources heterogeneity result in large variances in the personalized models, which lead the undirected aggregation to suboptimal personalized performance and unguaranteed convergence. To address these issues, we propose a directed collaboration DPFL framework by incorporating stochastic gradient push and partial model personalized, called \\textbf{D}ecentralized \\textbf{Fed}erated \\textbf{P}artial \\textbf{G}radient \\textbf{P}ush (\\textbf{DFedPGP}). It personalizes the linear classifier in the modern deep model to customize the local solution and learns a consensus representation in a fully decentralized manner. Clients only share gradients with a subset of neighbors based on the directed and asymmetric topologies, which guarantees flexible choices for resource efficiency and better convergence. Theoretically, we show that the proposed DFedPGP achieves a superior convergence rate of $\\mathcal{O}(\\frac{1}{\\sqrt{T}})$ in the general non-convex setting, and prove the tighter connectivity among clients will speed up the convergence. The proposed method achieves state-of-the-art (SOTA) accuracy in both data and computation heterogeneity scenarios, demonstrating the efficiency of the directed collaboration and partial gradient push.",
    "authors": [
      "Yingqi Liu",
      "Yifan Shi",
      "Qinglun Li",
      "Baoyuan Wu",
      "Xueqian Wang",
      "Li Shen"
    ],
    "url": "http://arxiv.org/abs/2405.17876v1",
    "timestamp": 1716879139,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4eadaa59-a107-4019-adf4-f2962edf5da8": {
    "pk": "4eadaa59-a107-4019-adf4-f2962edf5da8",
    "title": "PeerFL: A Simulator for Peer-to-Peer Federated Learning at Scale",
    "abstract": "This work integrates peer-to-peer federated learning tools with NS3, a widely used network simulator, to create a novel simulator designed to allow heterogeneous device experiments in federated learning. This cross-platform adaptability addresses a critical gap in existing simulation tools, enhancing the overall utility and user experience. NS3 is leveraged to simulate WiFi dynamics to facilitate federated learning experiments with participants that move around physically during training, leading to dynamic network characteristics. Our experiments showcase the simulator's efficiency in computational resource utilization at scale, with a maximum of 450 heterogeneous devices modelled as participants in federated learning. This positions it as a valuable tool for simulation-based investigations in peer-to-peer federated learning. The framework is open source and available for use and extension to the community.",
    "authors": [
      "Alka Luqman",
      "Shivanshu Shekhar",
      "Anupam Chattopadhyay"
    ],
    "url": "http://arxiv.org/abs/2405.17839v1",
    "timestamp": 1716874218,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fa9fadcc-2a9e-4c3e-b6c0-f8bf9b58c72d": {
    "pk": "fa9fadcc-2a9e-4c3e-b6c0-f8bf9b58c72d",
    "title": "An Innovative Networks in Federated Learning",
    "abstract": "This paper presents the development and application of Wavelet Kolmogorov-Arnold Networks (Wav-KAN) in federated learning. We implemented Wav-KAN \\cite{wav-kan} in the clients. Indeed, we have considered both continuous wavelet transform (CWT) and also discrete wavelet transform (DWT) to enable multiresolution capabaility which helps in heteregeneous data distribution across clients. Extensive experiments were conducted on different datasets, demonstrating Wav-KAN's superior performance in terms of interpretability, computational speed, training and test accuracy. Our federated learning algorithm integrates wavelet-based activation functions, parameterized by weight, scale, and translation, to enhance local and global model performance. Results show significant improvements in computational efficiency, robustness, and accuracy, highlighting the effectiveness of wavelet selection in scalable neural network design.",
    "authors": [
      "Zavareh Bozorgasl",
      "Hao Chen"
    ],
    "url": "http://arxiv.org/abs/2405.17836v1",
    "timestamp": 1716873601,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SP",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5377347c-93bb-43c9-98bc-2b1e7fa3a7ff": {
    "pk": "5377347c-93bb-43c9-98bc-2b1e7fa3a7ff",
    "title": "Post-Fair Federated Learning: Achieving Group and Community Fairness in Federated Learning via Post-processing",
    "abstract": "Federated Learning (FL) is a distributed machine learning framework in which a set of local communities collaboratively learn a shared global model while retaining all training data locally within each community. Two notions of fairness have recently emerged as important issues for federated learning: group fairness and community fairness. Group fairness requires that a model's decisions do not favor any particular group based on a set of legally protected attributes such as race or gender. Community fairness requires that global models exhibit similar levels of performance (accuracy) across all collaborating communities. Both fairness concepts can coexist within an FL framework, but the existing literature has focused on either one concept or the other. This paper proposes and analyzes a post-processing fair federated learning (FFL) framework called post-FFL. Post-FFL uses a linear program to simultaneously enforce group and community fairness while maximizing the utility of the global model. Because Post-FFL is a post-processing approach, it can be used with existing FL training pipelines whose convergence properties are well understood. This paper uses post-FFL on real-world datasets to mimic how hospital networks, for example, use federated learning to deliver community health care. Theoretical results bound the accuracy lost when post-FFL enforces both notion of fairness. Experimental results illustrate that post-FFL simultaneously improves both group and community fairness in FL. Moreover, post-FFL outperforms the existing in-processing fair federated learning in terms of improving both notions of fairness, communication efficiency and computation cost.",
    "authors": [
      "Yuying Duan",
      "Yijun Tian",
      "Nitesh Chawla",
      "Michael Lemmon"
    ],
    "url": "http://arxiv.org/abs/2405.17782v1",
    "timestamp": 1716866760,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "467f5dd9-13aa-41e5-95e4-0a7829938022": {
    "pk": "467f5dd9-13aa-41e5-95e4-0a7829938022",
    "title": "Wireless Federated Learning over Resource-Constrained Networks: Digital versus Analog Transmissions",
    "abstract": "To enable wireless federated learning (FL) in communication resource-constrained networks, two communication schemes, i.e., digital and analog ones, are effective solutions. In this paper, we quantitatively compare these two techniques, highlighting their essential differences as well as respectively suitable scenarios. We first examine both digital and analog transmission schemes, together with a unified and fair comparison framework under imbalanced device sampling, strict latency targets, and transmit power constraints. A universal convergence analysis under various imperfections is established for evaluating the performance of FL over wireless networks. These analytical results reveal that the fundamental difference between the digital and analog communications lies in whether communication and computation are jointly designed or not. The digital scheme decouples the communication design from FL computing tasks, making it difficult to support uplink transmission from massive devices with limited bandwidth and hence the performance is mainly communication-limited. In contrast, the analog communication allows over-the-air computation (AirComp) and achieves better spectrum utilization. However, the computation-oriented analog transmission reduces power efficiency, and its performance is sensitive to computation errors from imperfect channel state information (CSI). Furthermore, device sampling for both schemes are optimized and differences in sampling optimization are analyzed. Numerical results verify the theoretical analysis and affirm the superior performance of the sampling optimization.",
    "authors": [
      "Jiacheng Yao",
      "Wei Xu",
      "Zhaohui Yang",
      "Xiaohu You",
      "Mehdi Bennis",
      "H. Vincent Poor"
    ],
    "url": "http://arxiv.org/abs/2405.17759v1",
    "timestamp": 1716863028,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IT",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d2b96b71-8fcb-41b1-8705-03d958a7b73f": {
    "pk": "d2b96b71-8fcb-41b1-8705-03d958a7b73f",
    "title": "FedHPL: Efficient Heterogeneous Federated Learning with Prompt Tuning and Logit Distillation",
    "abstract": "Federated learning (FL) is a popular privacy-preserving paradigm that enables distributed clients to collaboratively train models with a central server while keeping raw data locally. In practice, distinct model architectures, varying data distributions, and limited resources across local clients inevitably cause model performance degradation and a slowdown in convergence speed. However, existing FL methods can only solve some of the above heterogeneous challenges and have obvious performance limitations. Notably, a unified framework has not yet been explored to overcome these challenges. Accordingly, we propose FedHPL, a parameter-efficient unified $\\textbf{Fed}$erated learning framework for $\\textbf{H}$eterogeneous settings based on $\\textbf{P}$rompt tuning and $\\textbf{L}$ogit distillation. Specifically, we employ a local prompt tuning scheme that leverages a few learnable visual prompts to efficiently fine-tune the frozen pre-trained foundation model for downstream tasks, thereby accelerating training and improving model performance under limited local resources and data heterogeneity. Moreover, we design a global logit distillation scheme to handle the model heterogeneity and guide the local training. In detail, we leverage logits to implicitly capture local knowledge and design a weighted knowledge aggregation mechanism to generate global client-specific logits. We provide a theoretical guarantee on the generalization error bound for FedHPL. The experiments on various benchmark datasets under diverse settings of models and data demonstrate that our framework outperforms state-of-the-art FL approaches, with less computation overhead and training rounds.",
    "authors": [
      "Yuting Ma",
      "Lechao Cheng",
      "Yaxiong Wang",
      "Zhun Zhong",
      "Xiaohua Xu",
      "Meng Wang"
    ],
    "url": "http://arxiv.org/abs/2405.17267v1",
    "timestamp": 1716823532,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8ecc1e9f-8986-4ab9-9b67-0850dcfe1835": {
    "pk": "8ecc1e9f-8986-4ab9-9b67-0850dcfe1835",
    "title": "Efficient Model Compression for Hierarchical Federated Learning",
    "abstract": "Federated learning (FL), as an emerging collaborative learning paradigm, has garnered significant attention due to its capacity to preserve privacy within distributed learning systems. In these systems, clients collaboratively train a unified neural network model using their local datasets and share model parameters rather than raw data, enhancing privacy. Predominantly, FL systems are designed for mobile and edge computing environments where training typically occurs over wireless networks. Consequently, as model sizes increase, the conventional FL frameworks increasingly consume substantial communication resources. To address this challenge and improve communication efficiency, this paper introduces a novel hierarchical FL framework that integrates the benefits of clustered FL and model compression. We present an adaptive clustering algorithm that identifies a core client and dynamically organizes clients into clusters. Furthermore, to enhance transmission efficiency, each core client implements a local aggregation with compression (LC aggregation) algorithm after collecting compressed models from other clients within the same cluster. Simulation results affirm that our proposed algorithms not only maintain comparable predictive accuracy but also significantly reduce energy consumption relative to existing FL mechanisms.",
    "authors": [
      "Xi Zhu",
      "Songcan Yu",
      "Junbo Wang",
      "Qinglin Yang"
    ],
    "url": "http://arxiv.org/abs/2405.17522v1",
    "timestamp": 1716812267,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "410ffc58-9a90-4b04-9616-44c51b0136b2": {
    "pk": "410ffc58-9a90-4b04-9616-44c51b0136b2",
    "title": "LabObf: A Label Protection Scheme for Vertical Federated Learning Through Label Obfuscation",
    "abstract": "Split learning, as one of the most common architectures in vertical federated learning, has gained widespread use in industry due to its privacy-preserving characteristics. In this architecture, the party holding the labels seeks cooperation from other parties to improve model performance due to insufficient feature data. Each of these participants has a self-defined bottom model to learn hidden representations from its own feature data and uploads the embedding vectors to the top model held by the label holder for final predictions. This design allows participants to conduct joint training without directly exchanging data. However, existing research points out that malicious participants may still infer label information from the uploaded embeddings, leading to privacy leakage. In this paper, we first propose an embedding extension attack that manually modifies embeddings to undermine existing defense strategies, which rely on constraining the correlation between the embeddings uploaded by participants and the labels. Subsequently, we propose a new label obfuscation defense strategy, called `LabObf', which randomly maps each original one-hot vector label to multiple numerical soft labels with values intertwined, significantly increasing the difficulty for attackers to infer the labels. We conduct experiments on four different types of datasets, and the results show that LabObf can reduce the attacker's success rate to near random guessing while maintaining an acceptable model accuracy.",
    "authors": [
      "Ying He",
      "Mingyang Niu",
      "Jingyu Hua",
      "Yunlong Mao",
      "Xu Huang",
      "Chen Li",
      "Sheng Zhong"
    ],
    "url": "http://arxiv.org/abs/2405.17042v1",
    "timestamp": 1716807282,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "87af24d6-81ab-4d00-bcfc-1c25fc9ed4d4": {
    "pk": "87af24d6-81ab-4d00-bcfc-1c25fc9ed4d4",
    "title": "Federated Learning with Blockchain-Enhanced Machine Unlearning: A Trustworthy Approach",
    "abstract": "With the growing need to comply with privacy regulations and respond to user data deletion requests, integrating machine unlearning into IoT-based federated learning has become imperative. Traditional unlearning methods, however, often lack verifiable mechanisms, leading to challenges in establishing trust. This paper delves into the innovative integration of blockchain technology with federated learning to surmount these obstacles. Blockchain fortifies the unlearning process through its inherent qualities of immutability, transparency, and robust security. It facilitates verifiable certification, harmonizes security with privacy, and sustains system efficiency. We introduce a framework that melds blockchain with federated learning, thereby ensuring an immutable record of unlearning requests and actions. This strategy not only bolsters the trustworthiness and integrity of the federated learning model but also adeptly addresses efficiency and security challenges typical in IoT environments. Our key contributions encompass a certification mechanism for the unlearning process, the enhancement of data security and privacy, and the optimization of data management to ensure system responsiveness in IoT scenarios.",
    "authors": [
      "Xuhan Zuo",
      "Minghao Wang",
      "Tianqing Zhu",
      "Lefeng Zhang",
      "Shui Yu",
      "Wanlei Zhou"
    ],
    "url": "http://arxiv.org/abs/2405.20776v1",
    "timestamp": 1716784549,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f19bcab5-b7c8-4a96-81b8-d3ac79bbd1b4": {
    "pk": "f19bcab5-b7c8-4a96-81b8-d3ac79bbd1b4",
    "title": "Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning",
    "abstract": "This demo paper examines the susceptibility of Federated Learning (FL) systems to targeted data poisoning attacks, presenting a novel system for visualizing and mitigating such threats. We simulate targeted data poisoning attacks via label flipping and analyze the impact on model performance, employing a five-component system that includes Simulation and Data Generation, Data Collection and Upload, User-friendly Interface, Analysis and Insight, and Advisory System. Observations from three demo modules: label manipulation, attack timing, and malicious attack availability, and two analysis components: utility and analytical behavior of local model updates highlight the risks to system integrity and offer insight into the resilience of FL systems. The demo is available at https://github.com/CathyXueqingZhang/DataPoisoningVis.",
    "authors": [
      "Xueqing Zhang",
      "Junkai Zhang",
      "Ka-Ho Chow",
      "Juntao Chen",
      "Ying Mao",
      "Mohamed Rahouti",
      "Xiang Li",
      "Yuchen Liu",
      "Wenqi Wei"
    ],
    "url": "http://arxiv.org/abs/2405.16707v1",
    "timestamp": 1716760712,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "83b5d334-8189-4a5d-a0bd-1444c31dd2a6": {
    "pk": "83b5d334-8189-4a5d-a0bd-1444c31dd2a6",
    "title": "Fair Federated Learning under Domain Skew with Local Consistency and Domain Diversity",
    "abstract": "Federated learning (FL) has emerged as a new paradigm for privacy-preserving collaborative training. Under domain skew, the current FL approaches are biased and face two fairness problems. 1) Parameter Update Conflict: data disparity among clients leads to varying parameter importance and inconsistent update directions. These two disparities cause important parameters to potentially be overwhelmed by unimportant ones of dominant updates. It consequently results in significant performance decreases for lower-performing clients. 2) Model Aggregation Bias: existing FL approaches introduce unfair weight allocation and neglect domain diversity. It leads to biased model convergence objective and distinct performance among domains. We discover a pronounced directional update consistency in Federated Learning and propose a novel framework to tackle above issues. First, leveraging the discovered characteristic, we selectively discard unimportant parameter updates to prevent updates from clients with lower performance overwhelmed by unimportant parameters, resulting in fairer generalization performance. Second, we propose a fair aggregation objective to prevent global model bias towards some domains, ensuring that the global model continuously aligns with an unbiased model. The proposed method is generic and can be combined with other existing FL methods to enhance fairness. Comprehensive experiments on Digits and Office-Caltech demonstrate the high fairness and performance of our method.",
    "authors": [
      "Yuhang Chen",
      "Wenke Huang",
      "Mang Ye"
    ],
    "url": "http://arxiv.org/abs/2405.16585v1",
    "timestamp": 1716733750,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "eee0b09a-3fc4-4a93-b730-320dc8499c96": {
    "pk": "eee0b09a-3fc4-4a93-b730-320dc8499c96",
    "title": "Multi-Level Additive Modeling for Structured Non-IID Federated Learning",
    "abstract": "The primary challenge in Federated Learning (FL) is to model non-IID distributions across clients, whose fine-grained structure is important to improve knowledge sharing. For example, some knowledge is globally shared across all clients, some is only transferable within a subgroup of clients, and some are client-specific. To capture and exploit this structure, we train models organized in a multi-level structure, called ``Multi-level Additive Models (MAM)'', for better knowledge-sharing across heterogeneous clients and their personalization. In federated MAM (FeMAM), each client is assigned to at most one model per level and its personalized prediction sums up the outputs of models assigned to it across all levels. For the top level, FeMAM trains one global model shared by all clients as FedAvg. For every mid-level, it learns multiple models each assigned to a subgroup of clients, as clustered FL. Every bottom-level model is trained for one client only. In the training objective, each model aims to minimize the residual of the additive predictions by the other models assigned to each client. To approximate the arbitrary structure of non-IID across clients, FeMAM introduces more flexibility and adaptivity to FL by incrementally adding new models to the prediction of each client and reassigning another if necessary, automatically optimizing the knowledge-sharing structure. Extensive experiments show that FeMAM surpasses existing clustered FL and personalized FL methods in various non-IID settings. Our code is available at https://github.com/shutong043/FeMAM.",
    "authors": [
      "Shutong Chen",
      "Tianyi Zhou",
      "Guodong Long",
      "Jie Ma",
      "Jing Jiang",
      "Chengqi Zhang"
    ],
    "url": "http://arxiv.org/abs/2405.16472v1",
    "timestamp": 1716710093,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7685cdb7-1b3c-4665-a74b-fd8a8b4dab0d": {
    "pk": "7685cdb7-1b3c-4665-a74b-fd8a8b4dab0d",
    "title": "Secure Hierarchical Federated Learning in Vehicular Networks Using Dynamic Client Selection and Anomaly Detection",
    "abstract": "Hierarchical Federated Learning (HFL) faces the significant challenge of adversarial or unreliable vehicles in vehicular networks, which can compromise the model's integrity through misleading updates. Addressing this, our study introduces a novel framework that integrates dynamic vehicle selection and robust anomaly detection mechanisms, aiming to optimize participant selection and mitigate risks associated with malicious contributions. Our approach involves a comprehensive vehicle reliability assessment, considering historical accuracy, contribution frequency, and anomaly records. An anomaly detection algorithm is utilized to identify anomalous behavior by analyzing the cosine similarity of local or model parameters during the federated learning (FL) process. These anomaly records are then registered and combined with past performance for accuracy and contribution frequency to identify the most suitable vehicles for each learning round. Dynamic client selection and anomaly detection algorithms are deployed at different levels, including cluster heads (CHs), cluster members (CMs), and the Evolving Packet Core (EPC), to detect and filter out spurious updates. Through simulation-based performance evaluation, our proposed algorithm demonstrates remarkable resilience even under intense attack conditions. Even in the worst-case scenarios, it achieves convergence times at $63$\\% as effective as those in scenarios without any attacks. Conversely, in scenarios without utilizing our proposed algorithm, there is a high likelihood of non-convergence in the FL process.",
    "authors": [
      "M. Saeid HaghighiFard",
      "Sinem Coleri"
    ],
    "url": "http://arxiv.org/abs/2405.17497v1",
    "timestamp": 1716661880,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "02602fc5-21fc-4b25-87c6-7c9003e12c0c": {
    "pk": "02602fc5-21fc-4b25-87c6-7c9003e12c0c",
    "title": "Vertical Federated Learning for Effectiveness, Security, Applicability: A Survey",
    "abstract": "Vertical Federated Learning (VFL) is a privacy-preserving distributed learning paradigm where different parties collaboratively learn models using partitioned features of shared samples, without leaking private data. Recent research has shown promising results addressing various challenges in VFL, highlighting its potential for practical applications in cross-domain collaboration. However, the corresponding research is scattered and lacks organization. To advance VFL research, this survey offers a systematic overview of recent developments. First, we provide a history and background introduction, along with a summary of the general training protocol of VFL. We then revisit the taxonomy in recent reviews and analyze limitations in-depth. For a comprehensive and structured discussion, we synthesize recent research from three fundamental perspectives: effectiveness, security, and applicability. Finally, we discuss several critical future research directions in VFL, which will facilitate the developments in this field. We provide a collection of research lists and periodically update them at https://github.com/shentt67/VFL_Survey.",
    "authors": [
      "Mang Ye",
      "Wei Shen",
      "Bo Du",
      "Eduard Snezhko",
      "Vassili Kovalev",
      "Pong C. Yuen"
    ],
    "url": "http://arxiv.org/abs/2405.17495v2",
    "timestamp": 1716653106,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8ce946d7-3209-4cd5-bd16-5cfec27a0113": {
    "pk": "8ce946d7-3209-4cd5-bd16-5cfec27a0113",
    "title": "Analytic Federated Learning",
    "abstract": "In this paper, we introduce analytic federated learning (AFL), a new training paradigm that brings analytical (i.e., closed-form) solutions to the federated learning (FL) community. Our AFL draws inspiration from analytic learning -- a gradient-free technique that trains neural networks with analytical solutions in one epoch. In the local client training stage, the AFL facilitates a one-epoch training, eliminating the necessity for multi-epoch updates. In the aggregation stage, we derive an absolute aggregation (AA) law. This AA law allows a single-round aggregation, removing the need for multiple aggregation rounds. More importantly, the AFL exhibits a \\textit{weight-invariant} property, meaning that regardless of how the full dataset is distributed among clients, the aggregated result remains identical. This could spawn various potentials, such as data heterogeneity invariance, client-number invariance, absolute convergence, and being hyperparameter-free (our AFL is the first hyperparameter-free method in FL history). We conduct experiments across various FL settings including extremely non-IID ones, and scenarios with a large number of clients (e.g., $\\ge 1000$). In all these settings, our AFL constantly performs competitively while existing FL techniques encounter various obstacles. Code is available at \\url{https://github.com/ZHUANGHP/Analytic-federated-learning}",
    "authors": [
      "Huiping Zhuang",
      "Run He",
      "Kai Tong",
      "Di Fang",
      "Han Sun",
      "Haoran Li",
      "Tianyi Chen",
      "Ziqian Zeng"
    ],
    "url": "http://arxiv.org/abs/2405.16240v1",
    "timestamp": 1716645518,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "95c4305b-2d87-4d6e-a1a4-78a121bacc7e": {
    "pk": "95c4305b-2d87-4d6e-a1a4-78a121bacc7e",
    "title": "Client2Vec: Improving Federated Learning by Distribution Shifts Aware Client Indexing",
    "abstract": "Federated Learning (FL) is a privacy-preserving distributed machine learning paradigm. Nonetheless, the substantial distribution shifts among clients pose a considerable challenge to the performance of current FL algorithms. To mitigate this challenge, various methods have been proposed to enhance the FL training process. This paper endeavors to tackle the issue of data heterogeneity from another perspective -- by improving FL algorithms prior to the actual training stage. Specifically, we introduce the Client2Vec mechanism, which generates a unique client index for each client before the commencement of FL training. Subsequently, we leverage the generated client index to enhance the subsequent FL training process. To demonstrate the effectiveness of the proposed Client2Vec method, we conduct three case studies that assess the impact of the client index on the FL training process. These case studies encompass enhanced client sampling, model aggregation, and local training. Extensive experiments conducted on diverse datasets and model architectures show the efficacy of Client2Vec across all three case studies. Our code is avaliable at \\url{https://github.com/LINs-lab/client2vec}.",
    "authors": [
      "Yongxin Guo",
      "Lin Wang",
      "Xiaoying Tang",
      "Tao Lin"
    ],
    "url": "http://arxiv.org/abs/2405.16233v1",
    "timestamp": 1716644963,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1e34952d-6413-42f9-b7f3-657f72eb2af2": {
    "pk": "1e34952d-6413-42f9-b7f3-657f72eb2af2",
    "title": "An Experimental Study of Different Aggregation Schemes in Semi-Asynchronous Federated Learning",
    "abstract": "Federated learning is highly valued due to its high-performance computing in distributed environments while safeguarding data privacy. To address resource heterogeneity, researchers have proposed a semi-asynchronous federated learning (SAFL) architecture. However, the performance gap between different aggregation targets in SAFL remain unexplored.   In this paper, we systematically compare the performance between two algorithm modes, FedSGD and FedAvg that correspond to aggregating gradients and models, respectively. Our results across various task scenarios indicate these two modes exhibit a substantial performance gap. Specifically, FedSGD achieves higher accuracy and faster convergence but experiences more severe fluctuates in accuracy, whereas FedAvg excels in handling straggler issues but converges slower with reduced accuracy.",
    "authors": [
      "Yunbo Li",
      "Jiaping Gui",
      "Yue Wu"
    ],
    "url": "http://arxiv.org/abs/2405.16086v1",
    "timestamp": 1716618823,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5aa02641-c9d8-4960-96b3-288f21bab0e4": {
    "pk": "5aa02641-c9d8-4960-96b3-288f21bab0e4",
    "title": "A Novel Privacy Enhancement Scheme with Dynamic Quantization for Federated Learning",
    "abstract": "Federated learning (FL) has been widely regarded as a promising paradigm for privacy preservation of raw data in machine learning. Although, the data privacy in FL is locally protected to some extent, it is still a desideratum to enhance privacy and alleviate communication overhead caused by repetitively transmitting model parameters. Typically, these challenges are addressed separately, or jointly via a unified scheme that consists of noise-injected privacy mechanism and communication compression, which may lead to model corruption due to the introduced composite noise. In this work, we propose a novel model-splitting privacy-preserving FL (MSP-FL) scheme to achieve private FL with precise accuracy guarantee. Based upon MSP-FL, we further propose a model-splitting privacy-preserving FL with dynamic quantization (MSPDQ-FL) to mitigate the communication overhead, which incorporates a shrinking quantization interval to reduce the quantization error. We provide privacy and convergence analysis for both MSP-FL and MSPDQ-FL under non-i.i.d. dataset, partial clients participation and finite quantization level. Numerical results are presented to validate the superiority of the proposed schemes.",
    "authors": [
      "Yifan Wang",
      "Xianghui Cao",
      "Shi Jin",
      "Mo-Yuen Chow"
    ],
    "url": "http://arxiv.org/abs/2405.16058v2",
    "timestamp": 1716613014,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.OC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "83b072b2-74d4-4bf7-9af1-16dcd0ab8931": {
    "pk": "83b072b2-74d4-4bf7-9af1-16dcd0ab8931",
    "title": "FedSheafHN: Personalized Federated Learning on Graph-structured Data",
    "abstract": "Personalized subgraph Federated Learning (FL) is a task that customizes Graph Neural Networks (GNNs) to individual client needs, accommodating diverse data distributions. However, applying hypernetworks in FL, while aiming to facilitate model personalization, often encounters challenges due to inadequate representation of client-specific characteristics. To overcome these limitations, we propose a model called FedSheafHN, using enhanced collaboration graph embedding and efficient personalized model parameter generation. Specifically, our model embeds each client's local subgraph into a server-constructed collaboration graph. We utilize sheaf diffusion in the collaboration graph to learn client representations. Our model improves the integration and interpretation of complex client characteristics. Furthermore, our model ensures the generation of personalized models through advanced hypernetworks optimized for parallel operations across clients. Empirical evaluations demonstrate that FedSheafHN outperforms existing methods in most scenarios, in terms of client model performance on various graph-structured datasets. It also has fast model convergence and effective new clients generalization.",
    "authors": [
      "Wenfei Liang",
      "Yanan Zhao",
      "Rui She",
      "Yiming Li",
      "Wee Peng Tay"
    ],
    "url": "http://arxiv.org/abs/2405.16056v3",
    "timestamp": 1716612701,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b8c1ff69-9327-4ff3-8b77-662fdd2be00e": {
    "pk": "b8c1ff69-9327-4ff3-8b77-662fdd2be00e",
    "title": "Federated Learning for Non-factorizable Models using Deep Generative Prior Approximations",
    "abstract": "Federated learning (FL) allows for collaborative model training across decentralized clients while preserving privacy by avoiding data sharing. However, current FL methods assume conditional independence between client models, limiting the use of priors that capture dependence, such as Gaussian processes (GPs). We introduce the Structured Independence via deep Generative Model Approximation (SIGMA) prior which enables FL for non-factorizable models across clients, expanding the applicability of FL to fields such as spatial statistics, epidemiology, environmental science, and other domains where modeling dependencies is crucial. The SIGMA prior is a pre-trained deep generative model that approximates the desired prior and induces a specified conditional independence structure in the latent variables, creating an approximate model suitable for FL settings. We demonstrate the SIGMA prior's effectiveness on synthetic data and showcase its utility in a real-world example of FL for spatial data, using a conditional autoregressive prior to model spatial dependence across Australia. Our work enables new FL applications in domains where modeling dependent data is essential for accurate predictions and decision-making.",
    "authors": [
      "Conor Hassan",
      "Joshua J Bon",
      "Elizaveta Semenova",
      "Antonietta Mira",
      "Kerrie Mengersen"
    ],
    "url": "http://arxiv.org/abs/2405.16055v1",
    "timestamp": 1716612246,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ML",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8ac522aa-8e0d-4e06-9879-1e820efab4db": {
    "pk": "8ac522aa-8e0d-4e06-9879-1e820efab4db",
    "title": "Exploring Age-of-Information Weighting in Federated Learning under Data Heterogeneity",
    "abstract": "This paper investigates federated learning in a wireless communication system, where random device selection is employed with non-independent and identically distributed (non-IID) data. The analysis indicates that while training deep learning networks using federated stochastic gradient descent (FedSGD) on non-IID datasets, device selection can generate gradient errors that accumulate, leading to potential weight divergence. To mitigate training divergence, we design an age-weighted FedSGD to scale local gradients according to the previous state of devices. To further improve learning performance by increasing device participation under the maximum time consumption constraint, we formulate an energy consumption minimization problem by including resource allocation and sub-channel assignment. By transforming the resource allocation problem into convex and utilizing KKT conditions, we derived the optimal resource allocation solution. Moreover, this paper develops a matching based algorithm to generate the enhanced sub-channel assignment. Simulation results indicate that i) age-weighted FedSGD is able to outperform conventional FedSGD in terms of convergence rate and achievable accuracy, and ii) the proposed resource allocation and sub-channel assignment strategies can significantly reduce energy consumption and improve learning performance by increasing the number of selected devices.",
    "authors": [
      "Kaidi Wang",
      "Zhiguo Ding",
      "Daniel K. C. So",
      "Zhi Ding"
    ],
    "url": "http://arxiv.org/abs/2405.15978v1",
    "timestamp": 1716593788,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SP",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "15be539e-ca7b-494b-8e3f-a2b194f4143a": {
    "pk": "15be539e-ca7b-494b-8e3f-a2b194f4143a",
    "title": "Massive Digital Over-the-Air Computation for Communication-Efficient Federated Edge Learning",
    "abstract": "Over-the-air computation (AirComp) is a promising technology converging communication and computation over wireless networks, which can be particularly effective in model training, inference, and more emerging edge intelligence applications. AirComp relies on uncoded transmission of individual signals, which are added naturally over the multiple access channel thanks to the superposition property of the wireless medium. Despite significantly improved communication efficiency, how to accommodate AirComp in the existing and future digital communication networks, that are based on discrete modulation schemes, remains a challenge. This paper proposes a massive digital AirComp (MD-AirComp) scheme, that leverages an unsourced massive access protocol, to enhance compatibility with both current and next-generation wireless networks. MD-AirComp utilizes vector quantization to reduce the uplink communication overhead, and employs shared quantization and modulation codebooks. At the receiver, we propose a near-optimal approximate message passing-based algorithm to compute the model aggregation results from the superposed sequences, which relies on estimating the number of devices transmitting each code sequence, rather than trying to decode the messages of individual transmitters. We apply MD-AirComp to the federated edge learning (FEEL), and show that it significantly accelerates FEEL convergence compared to state-of-the-art while using the same amount of communication resources. To support further research and ensure reproducibility, we have made our code available at https://github.com/liqiao19/MD-AirComp.",
    "authors": [
      "Li Qiao",
      "Zhen Gao",
      "Mahdi Boloursaz Mashhadi",
      "Deniz G\u00fcnd\u00fcz"
    ],
    "url": "http://arxiv.org/abs/2405.15969v1",
    "timestamp": 1716590498,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IT",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "060bc60c-f946-4c8b-bfdb-a4d737234f0d": {
    "pk": "060bc60c-f946-4c8b-bfdb-a4d737234f0d",
    "title": "Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization",
    "abstract": "Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL pose a significant challenge to its efficiency. Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. Despite various communication efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations. In this paper, we introduce a novel dimension-free communication strategy for FL, leveraging zero-order optimization techniques. We propose a new algorithm, FedDisco, which facilitates the transmission of only a constant number of scalar values between clients and the server in each communication round, thereby reducing the communication cost from $\\mathscr{O}(d)$ to $\\mathscr{O}(1)$, where $d$ is the dimension of the model parameters. Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions and dimension-free rate for low effective rank scenarios. Empirical evaluations through classic deep learning training and large language model fine-tuning substantiate significant reductions in communication overhead compared to traditional FL approaches.",
    "authors": [
      "Zhe Li",
      "Bicheng Ying",
      "Zidong Liu",
      "Haibo Yang"
    ],
    "url": "http://arxiv.org/abs/2405.15861v1",
    "timestamp": 1716574025,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2aac779f-7867-4d67-a043-61971366f3f8": {
    "pk": "2aac779f-7867-4d67-a043-61971366f3f8",
    "title": "CAFe: Cost and Age aware Federated Learning",
    "abstract": "In many federated learning (FL) models, a common strategy employed to ensure the progress in the training process, is to wait for at least $M$ clients out of the total $N$ clients to send back their local gradients based on a reporting deadline $T$, once the parameter server (PS) has broadcasted the global model. If enough clients do not report back within the deadline, the particular round is considered to be a failed round and the training round is restarted from scratch. If enough clients have responded back, the round is deemed successful and the local gradients of all the clients that responded back are used to update the global model. In either case, the clients that failed to report back an update within the deadline would have wasted their computational resources. Having a tighter deadline (small $T$) and waiting for a larger number of participating clients (large $M$) leads to a large number of failed rounds and therefore greater communication cost and computation resource wastage. However, having a larger $T$ leads to longer round durations whereas smaller $M$ may lead to noisy gradients. Therefore, there is a need to optimize the parameters $M$ and $T$ such that communication cost and the resource wastage is minimized while having an acceptable convergence rate. In this regard, we show that the average age of a client at the PS appears explicitly in the theoretical convergence bound, and therefore, can be used as a metric to quantify the convergence of the global model. We provide an analytical scheme to select the parameters $M$ and $T$ in this setting.",
    "authors": [
      "Sahan Liyanaarachchi",
      "Kanchana Thilakarathna",
      "Sennur Ulukus"
    ],
    "url": "http://arxiv.org/abs/2405.15744v1",
    "timestamp": 1716572490,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3fa31e62-96f4-4c91-9fa8-59680ae01ac7": {
    "pk": "3fa31e62-96f4-4c91-9fa8-59680ae01ac7",
    "title": "Harnessing Increased Client Participation with Cohort-Parallel Federated Learning",
    "abstract": "Federated Learning (FL) is a machine learning approach where nodes collaboratively train a global model. As more nodes participate in a round of FL, the effectiveness of individual model updates by nodes also diminishes. In this study, we increase the effectiveness of client updates by dividing the network into smaller partitions, or cohorts. We introduce Cohort-Parallel Federated Learning (CPFL): a novel learning approach where each cohort independently trains a global model using FL, until convergence, and the produced models by each cohort are then unified using one-shot Knowledge Distillation (KD) and a cross-domain, unlabeled dataset. The insight behind CPFL is that smaller, isolated networks converge quicker than in a one-network setting where all nodes participate. Through exhaustive experiments involving realistic traces and non-IID data distributions on the CIFAR-10 and FEMNIST image classification tasks, we investigate the balance between the number of cohorts, model accuracy, training time, and compute and communication resources. Compared to traditional FL, CPFL with four cohorts, non-IID data distribution, and CIFAR-10 yields a 1.9$\\times$ reduction in train time and a 1.3$\\times$ reduction in resource usage, with a minimal drop in test accuracy.",
    "authors": [
      "Akash Dhasade",
      "Anne-Marie Kermarrec",
      "Tuan-Anh Nguyen",
      "Rafael Pires",
      "Martijn de Vos"
    ],
    "url": "http://arxiv.org/abs/2405.15644v1",
    "timestamp": 1716564849,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3e4f739f-0f4e-4362-ba73-29812df5ac41": {
    "pk": "3e4f739f-0f4e-4362-ba73-29812df5ac41",
    "title": "Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning",
    "abstract": "Federated Learning (FL), a privacy-aware approach in distributed deep learning environments, enables many clients to collaboratively train a model without sharing sensitive data, thereby reducing privacy risks. However, enabling human trust and control over FL systems requires understanding the evolving behaviour of clients, whether beneficial or detrimental for the training, which still represents a key challenge in the current literature. To address this challenge, we introduce Federated Behavioural Planes (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space). Our experiments demonstrate that FBPs provide informative trajectories describing the evolving states of clients and their contributions to the global model, thereby enabling the identification of clusters of clients with similar behaviours. Leveraging the patterns identified by FBPs, we propose a robust aggregation technique named Federated Behavioural Shields to detect malicious or noisy client models, thereby enhancing security and surpassing the efficacy of existing state-of-the-art FL defense mechanisms.",
    "authors": [
      "Dario Fenoglio",
      "Gabriele Dominici",
      "Pietro Barbiero",
      "Alberto Tonda",
      "Martin Gjoreski",
      "Marc Langheinrich"
    ],
    "url": "http://arxiv.org/abs/2405.15632v1",
    "timestamp": 1716563871,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6865c149-5536-4085-8441-e42d356f6c92": {
    "pk": "6865c149-5536-4085-8441-e42d356f6c92",
    "title": "Unlearning during Learning: An Efficient Federated Machine Unlearning Method",
    "abstract": "In recent years, Federated Learning (FL) has garnered significant attention as a distributed machine learning paradigm. To facilitate the implementation of the right to be forgotten, the concept of federated machine unlearning (FMU) has also emerged. However, current FMU approaches often involve additional time-consuming steps and may not offer comprehensive unlearning capabilities, which renders them less practical in real FL scenarios. In this paper, we introduce FedAU, an innovative and efficient FMU framework aimed at overcoming these limitations. Specifically, FedAU incorporates a lightweight auxiliary unlearning module into the learning process and employs a straightforward linear operation to facilitate unlearning. This approach eliminates the requirement for extra time-consuming steps, rendering it well-suited for FL. Furthermore, FedAU exhibits remarkable versatility. It not only enables multiple clients to carry out unlearning tasks concurrently but also supports unlearning at various levels of granularity, including individual data samples, specific classes, and even at the client level. We conducted extensive experiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the performance of FedAU. The results demonstrate that FedAU effectively achieves the desired unlearning effect while maintaining model accuracy.",
    "authors": [
      "Hanlin Gu",
      "Gongxi Zhu",
      "Jie Zhang",
      "Xinyuan Zhao",
      "Yuxing Han",
      "Lixin Fan",
      "Qiang Yang"
    ],
    "url": "http://arxiv.org/abs/2405.15474v1",
    "timestamp": 1716551593,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "115beb0f-4540-4801-bdc4-450e0bbb0d57": {
    "pk": "115beb0f-4540-4801-bdc4-450e0bbb0d57",
    "title": "FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler",
    "abstract": "Federated learning (FL) enables collaborative machine learning across distributed data owners, but data heterogeneity poses a challenge for model calibration. While prior work focused on improving accuracy for non-iid data, calibration remains under-explored. This study reveals existing FL aggregation approaches lead to sub-optimal calibration, and theoretical analysis shows despite constraining variance in clients' label distributions, global calibration error is still asymptotically lower bounded. To address this, we propose a novel Federated Calibration (FedCal) approach, emphasizing both local and global calibration. It leverages client-specific scalers for local calibration to effectively correct output misalignment without sacrificing prediction accuracy. These scalers are then aggregated via weight averaging to generate a global scaler, minimizing the global calibration error. Extensive experiments demonstrate FedCal significantly outperforms the best-performing baseline, reducing global calibration error by 47.66% on average.",
    "authors": [
      "Hongyi Peng",
      "Han Yu",
      "Xiaoli Tang",
      "Xiaoxiao Li"
    ],
    "url": "http://arxiv.org/abs/2405.15458v2",
    "timestamp": 1716550438,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "905a373e-defa-4ccb-99c0-56e688b3bfe0": {
    "pk": "905a373e-defa-4ccb-99c0-56e688b3bfe0",
    "title": "Towards Client Driven Federated Learning",
    "abstract": "Conventional federated learning (FL) frameworks follow a server-driven model where the server determines session initiation and client participation, which faces challenges in accommodating clients' asynchronous needs for model updates. We introduce Client-Driven Federated Learning (CDFL), a novel FL framework that puts clients at the driving role. In CDFL, each client independently and asynchronously updates its model by uploading the locally trained model to the server and receiving a customized model tailored to its local task. The server maintains a repository of cluster models, iteratively refining them using received client models. Our framework accommodates complex dynamics in clients' data distributions, characterized by time-varying mixtures of cluster distributions, enabling rapid adaptation to new tasks with superior performance. In contrast to traditional clustered FL protocols that send multiple cluster models to a client to perform distribution estimation, we propose a paradigm that offloads the estimation task to the server and only sends a single model to a client, and novel strategies to improve estimation accuracy. We provide a theoretical analysis of CDFL's convergence. Extensive experiments across various datasets and system settings highlight CDFL's substantial advantages in model performance and computation efficiency over baselines.",
    "authors": [
      "Songze Li",
      "Chenqing Zhu"
    ],
    "url": "http://arxiv.org/abs/2405.15407v1",
    "timestamp": 1716545869,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "96c13d19-2f41-4d08-af1a-a2ef1e5d0b7e": {
    "pk": "96c13d19-2f41-4d08-af1a-a2ef1e5d0b7e",
    "title": "Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification",
    "abstract": "Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients. However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence. In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS). The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels. The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities. On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer.",
    "authors": [
      "Bar\u0131\u015f B\u00fcy\u00fckta\u015f",
      "Kenneth Weitzel",
      "Sebastian V\u00f6lkers",
      "Felix Zailskas",
      "Beg\u00fcm Demir"
    ],
    "url": "http://arxiv.org/abs/2405.15405v1",
    "timestamp": 1716545629,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b6fb96f3-8f78-4967-a1d3-3aee2800180c": {
    "pk": "b6fb96f3-8f78-4967-a1d3-3aee2800180c",
    "title": "Decaf: Data Distribution Decompose Attack against Federated Learning",
    "abstract": "In contrast to prevalent Federated Learning (FL) privacy inference techniques such as generative adversarial networks attacks, membership inference attacks, property inference attacks, and model inversion attacks, we devise an innovative privacy threat: the Data Distribution Decompose Attack on FL, termed Decaf. This attack enables an honest-but-curious FL server to meticulously profile the proportion of each class owned by the victim FL user, divulging sensitive information like local market item distribution and business competitiveness. The crux of Decaf lies in the profound observation that the magnitude of local model gradient changes closely mirrors the underlying data distribution, including the proportion of each class. Decaf addresses two crucial challenges: accurately identify the missing/null class(es) given by any victim user as a premise and then quantify the precise relationship between gradient changes and each remaining non-null class. Notably, Decaf operates stealthily, rendering it entirely passive and undetectable to victim users regarding the infringement of their data distribution privacy. Experimental validation on five benchmark datasets (MNIST, FASHION-MNIST, CIFAR-10, FER-2013, and SkinCancer) employing diverse model architectures, including customized convolutional networks, standardized VGG16, and ResNet18, demonstrates Decaf's efficacy. Results indicate its ability to accurately decompose local user data distribution, regardless of whether it is IID or non-IID distributed. Specifically, the dissimilarity measured using $L_{\\infty}$ distance between the distribution decomposed by Decaf and ground truth is consistently below 5\\% when no null classes exist. Moreover, Decaf achieves 100\\% accuracy in determining any victim user's null classes, validated through formal proof.",
    "authors": [
      "Zhiyang Dai",
      "Chunyi Zhou",
      "Anmin Fu"
    ],
    "url": "http://arxiv.org/abs/2405.15316v1",
    "timestamp": 1716537392,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e262b16e-4edf-45e3-9a05-2297ee34d6b9": {
    "pk": "e262b16e-4edf-45e3-9a05-2297ee34d6b9",
    "title": "RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation",
    "abstract": "Federated learning (FL) allows multiple devices to train a model collaboratively without sharing their data. Despite its benefits, FL is vulnerable to privacy leakage and poisoning attacks. To address the privacy concern, secure aggregation (SecAgg) is often used to obtain the aggregation of gradients on sever without inspecting individual user updates. Unfortunately, existing defense strategies against poisoning attacks rely on the analysis of local updates in plaintext, making them incompatible with SecAgg. To reconcile the conflicts, we propose a robust federated learning framework against poisoning attacks (RFLPA) based on SecAgg protocol. Our framework computes the cosine similarity between local updates and server updates to conduct robust aggregation. Furthermore, we leverage verifiable packed Shamir secret sharing to achieve reduced communication cost of $O(M+N)$ per user, and design a novel dot-product aggregation algorithm to resolve the issue of increased information leakage. Our experimental results show that RFLPA significantly reduces communication and computation overhead by over $75\\%$ compared to the state-of-the-art method, BREA, while maintaining competitive accuracy.",
    "authors": [
      "Peihua Mai",
      "Ran Yan",
      "Yan Pang"
    ],
    "url": "http://arxiv.org/abs/2405.15182v1",
    "timestamp": 1716521470,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bc9ac066-cf70-4f79-a4aa-fa6b2eb7abc2": {
    "pk": "bc9ac066-cf70-4f79-a4aa-fa6b2eb7abc2",
    "title": "Momentum-Based Federated Reinforcement Learning with Interaction and Communication Efficiency",
    "abstract": "Federated Reinforcement Learning (FRL) has garnered increasing attention recently. However, due to the intrinsic spatio-temporal non-stationarity of data distributions, the current approaches typically suffer from high interaction and communication costs. In this paper, we introduce a new FRL algorithm, named $\\texttt{MFPO}$, that utilizes momentum, importance sampling, and additional server-side adjustment to control the shift of stochastic policy gradients and enhance the efficiency of data utilization. We prove that by proper selection of momentum parameters and interaction frequency, $\\texttt{MFPO}$ can achieve $\\tilde{\\mathcal{O}}(H N^{-1}\\epsilon^{-3/2})$ and $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ interaction and communication complexities ($N$ represents the number of agents), where the interaction complexity achieves linear speedup with the number of agents, and the communication complexity aligns the best achievable of existing first-order FL algorithms. Extensive experiments corroborate the substantial performance gains of $\\texttt{MFPO}$ over existing methods on a suite of complex and high-dimensional benchmarks.",
    "authors": [
      "Sheng Yue",
      "Xingyuan Hua",
      "Lili Chen",
      "Ju Ren"
    ],
    "url": "http://arxiv.org/abs/2405.17471v2",
    "timestamp": 1716521017,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "435992cc-7579-44fb-ba8f-fc12eb6e87f7": {
    "pk": "435992cc-7579-44fb-ba8f-fc12eb6e87f7",
    "title": "Recurrent Early Exits for Federated Learning with Heterogeneous Clients",
    "abstract": "Federated learning (FL) has enabled distributed learning of a model across multiple clients in a privacy-preserving manner. One of the main challenges of FL is to accommodate clients with varying hardware capacities; clients have differing compute and memory requirements. To tackle this challenge, recent state-of-the-art approaches leverage the use of early exits. Nonetheless, these approaches fall short of mitigating the challenges of joint learning multiple exit classifiers, often relying on hand-picked heuristic solutions for knowledge distillation among classifiers and/or utilizing additional layers for weaker classifiers. In this work, instead of utilizing multiple classifiers, we propose a recurrent early exit approach named ReeFL that fuses features from different sub-models into a single shared classifier. Specifically, we use a transformer-based early-exit module shared among sub-models to i) better exploit multi-layer feature representations for task-specific prediction and ii) modulate the feature representation of the backbone model for subsequent predictions. We additionally present a per-client self-distillation approach where the best sub-model is automatically selected as the teacher of the other sub-models at each client. Our experiments on standard image and speech classification benchmarks across various emerging federated fine-tuning baselines demonstrate ReeFL's effectiveness over previous works.",
    "authors": [
      "Royson Lee",
      "Javier Fernandez-Marques",
      "Shell Xu Hu",
      "Da Li",
      "Stefanos Laskaridis",
      "\u0141ukasz Dudziak",
      "Timothy Hospedales",
      "Ferenc Husz\u00e1r",
      "Nicholas D. Lane"
    ],
    "url": "http://arxiv.org/abs/2405.14791v2",
    "timestamp": 1716483713,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  }
}