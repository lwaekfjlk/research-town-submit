{
  "40d9910d-3543-4bbc-a039-161e281b6ca2": {
    "pk": "40d9910d-3543-4bbc-a039-161e281b6ca2",
    "title": "Logical Distillation of Graph Neural Networks",
    "abstract": "We present a logic based interpretable model for learning on graphs and an algorithm to distill this model from a Graph Neural Network (GNN). Recent results have shown connections between the expressivity of GNNs and the two-variable fragment of first-order logic with counting quantifiers (C2). We introduce a decision-tree based model which leverages an extension of C2 to distill interpretable logical classifiers from GNNs. We test our approach on multiple GNN architectures. The distilled models are interpretable, succinct, and attain similar accuracy to the underlying GNN. Furthermore, when the ground truth is expressible in C2, our approach outperforms the GNN.",
    "authors": [
      "Alexander Pluska",
      "Pascal Welke",
      "Thomas G\u00e4rtner",
      "Sagar Malhotra"
    ],
    "url": "http://arxiv.org/abs/2406.07126v1",
    "timestamp": 1718101138,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ff8b6c60-f7e4-4511-aeee-a8fbcaf2890a": {
    "pk": "ff8b6c60-f7e4-4511-aeee-a8fbcaf2890a",
    "title": "On the H\u00f6lder Stability of Multiset and Graph Neural Networks",
    "abstract": "Famously, multiset neural networks based on sum-pooling can separate all distinct multisets, and as a result can be used by message passing neural networks (MPNNs) to separate all pairs of graphs that can be separated by the 1-WL graph isomorphism test. However, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" multisets and graphs might even be considered identical when using fixed finite precision.   In this work, we propose to fully analyze the separation quality of multiset models and MPNNs via a novel adaptation of Lipschitz and H\\\"{o}lder continuity to parametric functions. We prove that common sum-based models are lower-H\\\"{o}lder continuous, with a H\\\"{o}lder exponent that decays rapidly with the network's depth. Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful MPNNs. To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz continuous. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.",
    "authors": [
      "Yair Davidson",
      "Nadav Dym"
    ],
    "url": "http://arxiv.org/abs/2406.06984v1",
    "timestamp": 1718087301,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2fe043cb-4ec8-4b76-a6a9-30060968abe6": {
    "pk": "2fe043cb-4ec8-4b76-a6a9-30060968abe6",
    "title": "Spatiotemporal Graph Neural Network Modelling Perfusion MRI",
    "abstract": "Perfusion MRI (pMRI) offers valuable insights into tumor vascularity and promises to predict tumor genotypes, thus benefiting prognosis for glioma patients, yet effective models tailored to 4D pMRI are still lacking. This study presents the first attempt to model 4D pMRI using a GNN-based spatiotemporal model PerfGAT, integrating spatial information and temporal kinetics to predict Isocitrate DeHydrogenase (IDH) mutation status in glioma patients. Specifically, we propose a graph structure learning approach based on edge attention and negative graphs to optimize temporal correlations modeling. Moreover, we design a dual-attention feature fusion module to integrate spatiotemporal features while addressing tumor-related brain regions. Further, we develop a class-balanced augmentation methods tailored to spatiotemporal data, which could mitigate the common label imbalance issue in clinical datasets. Our experimental results demonstrate that the proposed method outperforms other state-of-the-art approaches, promising to model pMRI effectively for patient characterization.",
    "authors": [
      "Ruodan Yan",
      "Carola-Bibiane Sch\u00f6nlieb",
      "Chao Li"
    ],
    "url": "http://arxiv.org/abs/2406.06434v1",
    "timestamp": 1718036686,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "136abdcd-6410-47c3-a2ad-2b6b73bb4305": {
    "pk": "136abdcd-6410-47c3-a2ad-2b6b73bb4305",
    "title": "Explainable Graph Neural Networks Under Fire",
    "abstract": "Predictions made by graph neural networks (GNNs) usually lack interpretability due to their complex computational behavior and the abstract nature of graphs. In an attempt to tackle this, many GNN explanation methods have emerged. Their goal is to explain a model's predictions and thereby obtain trust when GNN models are deployed in decision critical applications. Most GNN explanation methods work in a post-hoc manner and provide explanations in the form of a small subset of important edges and/or nodes. In this paper we demonstrate that these explanations can unfortunately not be trusted, as common GNN explanation methods turn out to be highly susceptible to adversarial perturbations. That is, even small perturbations of the original graph structure that preserve the model's predictions may yield drastically different explanations. This calls into question the trustworthiness and practical utility of post-hoc explanation methods for GNNs. To be able to attack GNN explanation models, we devise a novel attack method dubbed \\textit{GXAttack}, the first \\textit{optimization-based} adversarial attack method for post-hoc GNN explanations under such settings. Due to the devastating effectiveness of our attack, we call for an adversarial evaluation of future GNN explainers to demonstrate their robustness.",
    "authors": [
      "Zhong Li",
      "Simon Geisler",
      "Yuhang Wang",
      "Stephan G\u00fcnnemann",
      "Matthijs van Leeuwen"
    ],
    "url": "http://arxiv.org/abs/2406.06417v1",
    "timestamp": 1718035756,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7c2446ad-e716-4bde-aff9-d7805b507563": {
    "pk": "7c2446ad-e716-4bde-aff9-d7805b507563",
    "title": "Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs",
    "abstract": "Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective precondition.   Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. Despite notable empirical observations, theoretical foundations are still lacking. In this work, we investigate the expressive or representative power of GNNs, a crucial aspect of neural network theory, specifically in the context of QP tasks, with both continuous and mixed-integer settings. We prove the existence of message-passing GNNs that can reliably represent key properties of quadratic programs, including feasibility, optimal objective value, and optimal solution. Our theory is validated by numerical results.",
    "authors": [
      "Ziang Chen",
      "Xiaohan Chen",
      "Jialin Liu",
      "Xinshang Wang",
      "Wotao Yin"
    ],
    "url": "http://arxiv.org/abs/2406.05938v1",
    "timestamp": 1717977467,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9e5a404d-b8d8-4a87-8770-765cf49b8e31": {
    "pk": "9e5a404d-b8d8-4a87-8770-765cf49b8e31",
    "title": "Distributed Combinatorial Optimization of Downlink User Assignment in mmWave Cell-free Massive MIMO Using Graph Neural Networks",
    "abstract": "Millimeter wave (mmWave) cell-free massive MIMO (CF mMIMO) is a promising solution for future wireless communications. However, its optimization is non-trivial due to the challenging channel characteristics. We show that mmWave CF mMIMO optimization is largely an assignment problem between access points (APs) and users due to the high path loss of mmWave channels, the limited output power of the amplifier, and the almost orthogonal channels between users given a large number of AP antennas. The combinatorial nature of the assignment problem, the requirement for scalability, and the distributed implementation of CF mMIMO make this problem difficult. In this work, we propose an unsupervised machine learning (ML) enabled solution. In particular, a graph neural network (GNN) customized for scalability and distributed implementation is introduced. Moreover, the customized GNN architecture is hierarchically permutation-equivariant (HPE), i.e., if the APs or users of an AP are permuted, the output assignment is automatically permuted in the same way. To address the combinatorial problem, we relax it to a continuous problem, and introduce an information entropy-inspired penalty term. The training objective is then formulated using the augmented Lagrangian method (ALM). The test results show that the realized sum-rate outperforms that of the generalized serial dictatorship (GSD) algorithm and is very close to the upper bound in a small network scenario, while the upper bound is impossible to obtain in a large network scenario.",
    "authors": [
      "Bile Peng",
      "Bihan Guo",
      "Karl-Ludwig Besser",
      "Luca Kunz",
      "Ramprasad Raghunath",
      "Anke Schmeink",
      "Eduard A Jorswieck",
      "Giuseppe Caire",
      "H. Vincent Poor"
    ],
    "url": "http://arxiv.org/abs/2406.05652v1",
    "timestamp": 1717910843,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SP",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "facded35-2ade-4300-a390-0268341ce577": {
    "pk": "facded35-2ade-4300-a390-0268341ce577",
    "title": "Transfer Entropy in Graph Convolutional Neural Networks",
    "abstract": "Graph Convolutional Networks (GCN) are Graph Neural Networks where the convolutions are applied over a graph. In contrast to Convolutional Neural Networks, GCN's are designed to perform inference on graphs, where the number of nodes can vary, and the nodes are unordered. In this study, we address two important challenges related to GCNs: i) oversmoothing; and ii) the utilization of node relational properties (i.e., heterophily and homophily). Oversmoothing is the degradation of the discriminative capacity of nodes as a result of repeated aggregations. Heterophily is the tendency for nodes of different classes to connect, whereas homophily is the tendency of similar nodes to connect. We propose a new strategy for addressing these challenges in GCNs based on Transfer Entropy (TE), which measures of the amount of directed transfer of information between two time varying nodes. Our findings indicate that using node heterophily and degree information as a node selection mechanism, along with feature-based TE calculations, enhances accuracy across various GCN models. Our model can be easily modified to improve classification accuracy of a GCN model. As a trade off, this performance boost comes with a significant computational overhead when the TE is computed for many graph nodes.",
    "authors": [
      "Adrian Moldovan",
      "Angel Ca\u0163aron",
      "R\u0103zvan Andonie"
    ],
    "url": "http://arxiv.org/abs/2406.06632v1",
    "timestamp": 1717877357,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "02c7c959-a9c9-4dd2-8bc1-5bda0365dd59": {
    "pk": "02c7c959-a9c9-4dd2-8bc1-5bda0365dd59",
    "title": "Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks",
    "abstract": "In recent years, graph neural networks (GNNs) have emerged as a potent tool for learning on graph-structured data and won fruitful successes in varied fields. The majority of GNNs follow the message-passing paradigm, where representations of each node are learned by recursively aggregating features of its neighbors. However, this mechanism brings severe over-smoothing and efficiency issues over high-degree graphs (HDGs), wherein most nodes have dozens (or even hundreds) of neighbors, such as social networks, transaction graphs, power grids, etc. Additionally, such graphs usually encompass rich and complex structure semantics, which are hard to capture merely by feature aggregations in GNNs. Motivated by the above limitations, we propose TADA, an efficient and effective front-mounted data augmentation framework for GNNs on HDGs. Under the hood, TADA includes two key modules: (i) feature expansion with structure embeddings, and (ii) topology- and attribute-aware graph sparsification. The former obtains augmented node features and enhanced model capacity by encoding the graph structure into high-quality structure embeddings with our highly-efficient sketching method. Further, by exploiting task-relevant features extracted from graph structures and attributes, the second module enables the accurate identification and reduction of numerous redundant/noisy edges from the input graph, thereby alleviating over-smoothing and facilitating faster feature aggregations over HDGs. Empirically, TADA considerably improves the predictive performance of mainstream GNN models on 8 real homophilic/heterophilic HDGs in terms of node classification, while achieving efficient training and inference processes.",
    "authors": [
      "Yurui Lai",
      "Xiaoyang Lin",
      "Renchi Yang",
      "Hongtao Wang"
    ],
    "url": "http://arxiv.org/abs/2406.05482v2",
    "timestamp": 1717856059,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ec2b33fd-aaee-4a30-8b9d-227bdf8ec9a8": {
    "pk": "ec2b33fd-aaee-4a30-8b9d-227bdf8ec9a8",
    "title": "A Manifold Perspective on the Statistical Generalization of Graph Neural Networks",
    "abstract": "Convolutional neural networks have been successfully extended to operate on graphs, giving rise to Graph Neural Networks (GNNs). GNNs combine information from adjacent nodes by successive applications of graph convolutions. GNNs have been implemented successfully in various learning tasks while the theoretical understanding of their generalization capability is still in progress. In this paper, we leverage manifold theory to analyze the statistical generalization gap of GNNs operating on graphs constructed on sampled points from manifolds. We study the generalization gaps of GNNs on both node-level and graph-level tasks. We show that the generalization gaps decrease with the number of nodes in the training graphs, which guarantees the generalization of GNNs to unseen points over manifolds. We validate our theoretical results in multiple real-world datasets.",
    "authors": [
      "Zhiyang Wang",
      "Juan Cervino",
      "Alejandro Ribeiro"
    ],
    "url": "http://arxiv.org/abs/2406.05225v1",
    "timestamp": 1717788302,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "dee521a2-2571-4f7f-92c8-fdef15ce0afb": {
    "pk": "dee521a2-2571-4f7f-92c8-fdef15ce0afb",
    "title": "SpanGNN: Towards Memory-Efficient Graph Neural Networks via Spanning Subgraph Training",
    "abstract": "Graph Neural Networks (GNNs) have superior capability in learning graph data. Full-graph GNN training generally has high accuracy, however, it suffers from large peak memory usage and encounters the Out-of-Memory problem when handling large graphs. To address this memory problem, a popular solution is mini-batch GNN training. However, mini-batch GNN training increases the training variance and sacrifices the model accuracy. In this paper, we propose a new memory-efficient GNN training method using spanning subgraph, called SpanGNN. SpanGNN trains GNN models over a sequence of spanning subgraphs, which are constructed from empty structure. To overcome the excessive peak memory consumption problem, SpanGNN selects a set of edges from the original graph to incrementally update the spanning subgraph between every epoch. To ensure the model accuracy, we introduce two types of edge sampling strategies (i.e., variance-reduced and noise-reduced), and help SpanGNN select high-quality edges for the GNN learning. We conduct experiments with SpanGNN on widely used datasets, demonstrating SpanGNN's advantages in the model performance and low peak memory usage.",
    "authors": [
      "Xizhi Gu",
      "Hongzheng Li",
      "Shihong Gao",
      "Xinyan Zhang",
      "Lei Chen",
      "Yingxia Shao"
    ],
    "url": "http://arxiv.org/abs/2406.04938v1",
    "timestamp": 1717767983,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4a600578-2718-4f6a-8b17-3a027624ed95": {
    "pk": "4a600578-2718-4f6a-8b17-3a027624ed95",
    "title": "GENIE: Watermarking Graph Neural Networks for Link Prediction",
    "abstract": "Graph Neural Networks (GNNs) have advanced the field of machine learning by utilizing graph-structured data, which is ubiquitous in the real world. GNNs have applications in various fields, ranging from social network analysis to drug discovery. GNN training is strenuous, requiring significant computational resources and human expertise. It makes a trained GNN an indispensable Intellectual Property (IP) for its owner. Recent studies have shown GNNs to be vulnerable to model-stealing attacks, which raises concerns over IP rights protection. Watermarking has been shown to be effective at protecting the IP of a GNN model. Existing efforts to develop a watermarking scheme for GNNs have only focused on the node classification and the graph classification tasks.   To the best of our knowledge, we introduce the first-ever watermarking scheme for GNNs tailored to the Link Prediction (LP) task. We call our proposed watermarking scheme GENIE (watermarking Graph nEural Networks for lInk prEdiction). We design GENIE using a novel backdoor attack to create a trigger set for two key methods of LP: (1) node representation-based and (2) subgraph-based. In GENIE, the watermark is embedded into the GNN model by training it on both the trigger set and a modified training set, resulting in a watermarked GNN model. To assess a suspect model, we verify the watermark against the trigger set. We extensively evaluate GENIE across 3 model architectures (i.e., SEAL, GCN, and GraphSAGE) and 7 real-world datasets. Furthermore, we validate the robustness of GENIE against 11 state-of-the-art watermark removal techniques and 3 model extraction attacks. We also demonstrate that GENIE is robust against ownership piracy attack. Our ownership demonstration scheme statistically guarantees both False Positive Rate (FPR) and False Negative Rate (FNR) to be less than $10^{-6}$.",
    "authors": [
      "Venkata Sai Pranav Bachina",
      "Ankit Gangwal",
      "Aaryan Ajay Sharma",
      "Charu Sharma"
    ],
    "url": "http://arxiv.org/abs/2406.04805v1",
    "timestamp": 1717755121,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3dadf364-adc1-45af-b659-4e6105569b8f": {
    "pk": "3dadf364-adc1-45af-b659-4e6105569b8f",
    "title": "Mobile Network Configuration Recommendation using Deep Generative Graph Neural Network",
    "abstract": "There are vast number of configurable parameters in a Radio Access Telecom Network. A significant amount of these parameters is configured by Radio Node or cell based on their deployment setting. Traditional methods rely on domain knowledge for individual parameter configuration, often leading to sub-optimal results. To improve this, a framework using a Deep Generative Graph Neural Network (GNN) is proposed. It encodes the network into a graph, extracts subgraphs for each RAN node, and employs a Siamese GNN (S-GNN) to learn embeddings. The framework recommends configuration parameters for a multitude of parameters and detects misconfigurations, handling both network expansion and existing cell reconfiguration. Tested on real-world data, the model surpasses baselines, demonstrating accuracy, generalizability, and robustness against concept drift.",
    "authors": [
      "Shirwan Piroti",
      "Ashima Chawla",
      "Tahar Zanouda"
    ],
    "url": "http://arxiv.org/abs/2406.04779v1",
    "timestamp": 1717752498,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f976cd37-84e0-4ff0-887c-66afa781c23b": {
    "pk": "f976cd37-84e0-4ff0-887c-66afa781c23b",
    "title": "Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks",
    "abstract": "In recent years, machine learning has established itself as a powerful tool for high-resolution weather forecasting. While most current machine learning models focus on deterministic forecasts, accurately capturing the uncertainty in the chaotic weather system calls for probabilistic modeling. We propose a probabilistic weather forecasting model called Graph-EFM, combining a flexible latent-variable formulation with the successful graph-based forecasting framework. The use of a hierarchical graph construction allows for efficient sampling of spatially coherent forecasts. Requiring only a single forward pass per time step, Graph-EFM allows for fast generation of arbitrarily large ensembles. We experiment with the model on both global and limited area forecasting. Ensemble forecasts from Graph-EFM achieve equivalent or lower errors than comparable deterministic models, with the added benefit of accurately capturing forecast uncertainty.",
    "authors": [
      "Joel Oskarsson",
      "Tomas Landelius",
      "Marc Peter Deisenroth",
      "Fredrik Lindsten"
    ],
    "url": "http://arxiv.org/abs/2406.04759v1",
    "timestamp": 1717750885,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c0b192b0-3319-4b04-b5b1-c25500990ee8": {
    "pk": "c0b192b0-3319-4b04-b5b1-c25500990ee8",
    "title": "Enhancing Size Generalization in Graph Neural Networks through Disentangled Representation Learning",
    "abstract": "Although most graph neural networks (GNNs) can operate on graphs of any size, their classification performance often declines on graphs larger than those encountered during training. Existing methods insufficiently address the removal of size information from graph representations, resulting in sub-optimal performance and reliance on backbone models. In response, we propose DISGEN, a novel and model-agnostic framework designed to disentangle size factors from graph representations. DISGEN employs size- and task-invariant augmentations and introduces a decoupling loss that minimizes shared information in hidden representations, with theoretical guarantees for its effectiveness. Our empirical results show that DISGEN outperforms the state-of-the-art models by up to 6% on real-world datasets, underscoring its effectiveness in enhancing the size generalizability of GNNs. Our codes are available at: https://github.com/GraphmindDartmouth/DISGEN.",
    "authors": [
      "Zheng Huang",
      "Qihui Yang",
      "Dawei Zhou",
      "Yujun Yan"
    ],
    "url": "http://arxiv.org/abs/2406.04601v2",
    "timestamp": 1717730364,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fdfd07c9-8cac-42ac-9d32-d8c6712890f0": {
    "pk": "fdfd07c9-8cac-42ac-9d32-d8c6712890f0",
    "title": "GNNAnatomy: Systematic Generation and Evaluation of Multi-Level Explanations for Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have proven highly effective in various machine learning (ML) tasks involving graphs, such as node/graph classification and link prediction. However, explaining the decisions made by GNNs poses challenges because of the aggregated relational information based on graph structure, leading to complex data transformations. Existing methods for explaining GNNs often face limitations in systematically exploring diverse substructures and evaluating results in the absence of ground truths. To address this gap, we introduce GNNAnatomy, a model- and dataset-agnostic visual analytics system designed to facilitate the generation and evaluation of multi-level explanations for GNNs. In GNNAnatomy, we employ graphlets to elucidate GNN behavior in graph-level classification tasks. By analyzing the associations between GNN classifications and graphlet frequencies, we formulate hypothesized factual and counterfactual explanations. To validate a hypothesized graphlet explanation, we introduce two metrics: (1) the correlation between its frequency and the classification confidence, and (2) the change in classification confidence after removing this substructure from the original graph. To demonstrate the effectiveness of GNNAnatomy, we conduct case studies on both real-world and synthetic graph datasets from various domains. Additionally, we qualitatively compare GNNAnatomy with a state-of-the-art GNN explainer, demonstrating the utility and versatility of our design.",
    "authors": [
      "Hsiao-Ying Lu",
      "Yiran Li",
      "Ujwal Pratap Krishna Kaluvakolanu Thyagarajan",
      "Kwan-Liu Ma"
    ],
    "url": "http://arxiv.org/abs/2406.04548v1",
    "timestamp": 1717715394,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "18c093b6-a00a-47b9-8149-cd304ab1fbb8": {
    "pk": "18c093b6-a00a-47b9-8149-cd304ab1fbb8",
    "title": "On the Expressive Power of Spectral Invariant Graph Neural Networks",
    "abstract": "Incorporating spectral information to enhance Graph Neural Networks (GNNs) has shown promising results but raises a fundamental challenge due to the inherent ambiguity of eigenvectors. Various architectures have been proposed to address this ambiguity, referred to as spectral invariant architectures. Notable examples include GNNs and Graph Transformers that use spectral distances, spectral projection matrices, or other invariant spectral features. However, the potential expressive power of these spectral invariant architectures remains largely unclear. The goal of this work is to gain a deep theoretical understanding of the expressive power obtainable when using spectral features. We first introduce a unified message-passing framework for designing spectral invariant GNNs, called Eigenspace Projection GNN (EPNN). A comprehensive analysis shows that EPNN essentially unifies all prior spectral invariant architectures, in that they are either strictly less expressive or equivalent to EPNN. A fine-grained expressiveness hierarchy among different architectures is also established. On the other hand, we prove that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL. Finally, we discuss whether using spectral features can gain additional expressiveness when combined with more expressive GNNs.",
    "authors": [
      "Bohang Zhang",
      "Lingxiao Zhao",
      "Haggai Maron"
    ],
    "url": "http://arxiv.org/abs/2406.04336v1",
    "timestamp": 1717696781,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "531678a5-4d55-402c-8d7e-c2cb24d924c7": {
    "pk": "531678a5-4d55-402c-8d7e-c2cb24d924c7",
    "title": "NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise",
    "abstract": "Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.",
    "authors": [
      "Zhonghao Wang",
      "Danyu Sun",
      "Sheng Zhou",
      "Haobo Wang",
      "Jiapei Fan",
      "Longtao Huang",
      "Jiajun Bu"
    ],
    "url": "http://arxiv.org/abs/2406.04299v2",
    "timestamp": 1717695900,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "db522265-e3d2-4f99-b263-51a9c5f36216": {
    "pk": "db522265-e3d2-4f99-b263-51a9c5f36216",
    "title": "Multivector Neurons: Better and Faster O(n)-Equivariant Clifford Graph Neural Networks",
    "abstract": "Most current deep learning models equivariant to $O(n)$ or $SO(n)$ either consider mostly scalar information such as distances and angles or have a very high computational complexity. In this work, we test a few novel message passing graph neural networks (GNNs) based on Clifford multivectors, structured similarly to other prevalent equivariant models in geometric deep learning. Our approach leverages efficient invariant scalar features while simultaneously performing expressive learning on multivector representations, particularly through the use of the equivariant geometric product operator. By integrating these elements, our methods outperform established efficient baseline models on an N-Body simulation task and protein denoising task while maintaining a high efficiency. In particular, we push the state-of-the-art error on the N-body dataset to 0.0035 (averaged over 3 runs); an 8% improvement over recent methods. Our implementation is available on Github.",
    "authors": [
      "Cong Liu",
      "David Ruhe",
      "Patrick Forr\u00e9"
    ],
    "url": "http://arxiv.org/abs/2406.04052v1",
    "timestamp": 1717679864,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7ec843c3-3440-4c33-abf1-5e8deaa7bc2f": {
    "pk": "7ec843c3-3440-4c33-abf1-5e8deaa7bc2f",
    "title": "Energy-based Epistemic Uncertainty for Graph Neural Networks",
    "abstract": "In domains with interdependent data, such as graphs, quantifying the epistemic uncertainty of a Graph Neural Network (GNN) is challenging as uncertainty can arise at different structural scales. Existing techniques neglect this issue or only distinguish between structure-aware and structure-agnostic uncertainty without combining them into a single measure. We propose GEBM, an energy-based model (EBM) that provides high-quality uncertainty estimates by aggregating energy at different structural levels that naturally arise from graph diffusion. In contrast to logit-based EBMs, we provably induce an integrable density in the data space by regularizing the energy function. We introduce an evidential interpretation of our EBM that significantly improves the predictive robustness of the GNN. Our framework is a simple and effective post hoc method applicable to any pre-trained GNN that is sensitive to various distribution shifts. It consistently achieves the best separation of in-distribution and out-of-distribution data on 6 out of 7 anomaly types while having the best average rank over shifts on \\emph{all} datasets.",
    "authors": [
      "Dominik Fuchsgruber",
      "Tom Wollschl\u00e4ger",
      "Stephan G\u00fcnnemann"
    ],
    "url": "http://arxiv.org/abs/2406.04043v1",
    "timestamp": 1717679609,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "029c90bc-a29f-4ccd-b3ec-a4861f57ba34": {
    "pk": "029c90bc-a29f-4ccd-b3ec-a4861f57ba34",
    "title": "Exploiting Global Graph Homophily for Generalized Defense in Graph Neural Networks",
    "abstract": "Graph neural network (GNN) models play a pivotal role in numerous tasks involving graph-related data analysis. Despite their efficacy, similar to other deep learning models, GNNs are susceptible to adversarial attacks. Even minor perturbations in graph data can induce substantial alterations in model predictions. While existing research has explored various adversarial defense techniques for GNNs, the challenge of defending against adversarial attacks on real-world scale graph data remains largely unresolved. On one hand, methods reliant on graph purification and preprocessing tend to excessively emphasize local graph information, leading to sub-optimal defensive outcomes. On the other hand, approaches rooted in graph structure learning entail significant time overheads, rendering them impractical for large-scale graphs. In this paper, we propose a new defense method named Talos, which enhances the global, rather than local, homophily of graphs as a defense. Experiments show that the proposed approach notably outperforms state-of-the-art defense approaches, while imposing little computational overhead.",
    "authors": [
      "Duanyu Li",
      "Huijun Wu",
      "Min Xie",
      "Xugang Wu",
      "Zhenwei Wu",
      "Wenzhe Zhang"
    ],
    "url": "http://arxiv.org/abs/2406.03833v1",
    "timestamp": 1717661281,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1fed7836-79e9-41d0-974b-041d50065a3d": {
    "pk": "1fed7836-79e9-41d0-974b-041d50065a3d",
    "title": "Beyond 5G Network Failure Classification for Network Digital Twin Using Graph Neural Network",
    "abstract": "Fifth-generation (5G) core networks in network digital twins (NDTs) are complex systems with numerous components, generating considerable data. Analyzing these data can be challenging due to rare failure types, leading to imbalanced classes in multiclass classification. To address this problem, we propose a novel method of integrating a graph Fourier transform (GFT) into a message-passing neural network (MPNN) designed for NDTs. This approach transforms the data into a graph using the GFT to address class imbalance, whereas the MPNN extracts features and models dependencies between network components. This combined approach identifies failure types in real and simulated NDT environments, demonstrating its potential for accurate failure classification in 5G and beyond (B5G) networks. Moreover, the MPNN is adept at learning complex local structures among neighbors in an end-to-end setting. Extensive experiments have demonstrated that the proposed approach can identify failure types in three multiclass domain datasets at multiple failure points in real networks and NDT environments. The results demonstrate that the proposed GFT-MPNN can accurately classify network failures in B5G networks, especially when employed within NDTs to detect failure types.",
    "authors": [
      "Abubakar Isah",
      "Ibrahim Aliyu",
      "Jaechan Shim",
      "Hoyong Ryu",
      "Jinsul Kim"
    ],
    "url": "http://arxiv.org/abs/2406.06595v1",
    "timestamp": 1717659385,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6af38a0c-5a41-4125-92b5-72fe1052795f": {
    "pk": "6af38a0c-5a41-4125-92b5-72fe1052795f",
    "title": "Decision-focused Graph Neural Networks for Combinatorial Optimization",
    "abstract": "In recent years, there has been notable interest in investigating combinatorial optimization (CO) problems by neural-based framework. An emerging strategy to tackle these challenging problems involves the adoption of graph neural networks (GNNs) as an alternative to traditional algorithms, a subject that has attracted considerable attention. Despite the growing popularity of GNNs and traditional algorithm solvers in the realm of CO, there is limited research on their integrated use and the correlation between them within an end-to-end framework. The primary focus of our work is to formulate a more efficient and precise framework for CO by employing decision-focused learning on graphs. Additionally, we introduce a decision-focused framework that utilizes GNNs to address CO problems with auxiliary support. To realize an end-to-end approach, we have designed two cascaded modules: (a) an unsupervised trained graph predictive model, and (b) a solver for quadratic binary unconstrained optimization. Empirical evaluations are conducted on various classical tasks, including maximum cut, maximum independent set, and minimum vertex cover. The experimental results on classical CO problems (i.e. MaxCut, MIS, and MVC) demonstrate the superiority of our method over both the standalone GNN approach and classical methods.",
    "authors": [
      "Yang Liu",
      "Chuan Zhou",
      "Peng Zhang",
      "Shirui Pan",
      "Zhao Li",
      "Hongyang Chen"
    ],
    "url": "http://arxiv.org/abs/2406.03647v2",
    "timestamp": 1717627947,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "915d93f5-528f-48ec-a728-b7f6c85e45f6": {
    "pk": "915d93f5-528f-48ec-a728-b7f6c85e45f6",
    "title": "Combining Graph Neural Network and Mamba to Capture Local and Global Tissue Spatial Relationships in Whole Slide Images",
    "abstract": "In computational pathology, extracting spatial features from gigapixel whole slide images (WSIs) is a fundamental task, but due to their large size, WSIs are typically segmented into smaller tiles. A critical aspect of this analysis is aggregating information from these tiles to make predictions at the WSI level. We introduce a model that combines a message-passing graph neural network (GNN) with a state space model (Mamba) to capture both local and global spatial relationships among the tiles in WSIs. The model's effectiveness was demonstrated in predicting progression-free survival among patients with early-stage lung adenocarcinomas (LUAD). We compared the model with other state-of-the-art methods for tile-level information aggregation in WSIs, including tile-level information summary statistics-based aggregation, multiple instance learning (MIL)-based aggregation, GNN-based aggregation, and GNN-transformer-based aggregation. Additional experiments showed the impact of different types of node features and different tile sampling strategies on the model performance. This work can be easily extended to any WSI-based analysis. Code: https://github.com/rina-ding/gat-mamba.",
    "authors": [
      "Ruiwen Ding",
      "Kha-Dinh Luong",
      "Erika Rodriguez",
      "Ana Cristina Araujo Lemos da Silva",
      "William Hsu"
    ],
    "url": "http://arxiv.org/abs/2406.04377v1",
    "timestamp": 1717625217,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9127a679-894b-45a3-86d9-81d25cc76c69": {
    "pk": "9127a679-894b-45a3-86d9-81d25cc76c69",
    "title": "Equivariant Graph Neural Networks for Prediction of Tensor Material Properties of Crystals",
    "abstract": "Traditional machine learning methods applied to the material sciences have often predicted invariant, scalar properties of material systems to great effect. Newer, coordinate equivariant models promise to provide a coordinate system dependent output in a well defined manner, but recent applications often neglect a direct prediction of directional (i.e. coordinate system dependent) quantities and instead are used to predict still just invariant quantities. This component-wise prediction of tensorial properties is achieved by decomposing tensors into harmonic subspaces via a \\textit{tensor spherical harmonic decomposition}, by which we may also associate arbitrary tensors with the irreducible representations of the rotation group. This essentially allows us to read off tensors component-wise from the output representations of these equivariant models. In this work, we present results for the prediction of various material property tensors directly from crystalline structures. Namely, given some material's crystalline structure, we may predict tensor components of dielectric, piezoelectric, and elasticity tensors directly from the output of a $SE(3)$ equivariant model.",
    "authors": [
      "Alex Heilman",
      "Claire Schlesinger",
      "Qimin Yan"
    ],
    "url": "http://arxiv.org/abs/2406.03563v1",
    "timestamp": 1717611785,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "physics.comp-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "df662bbe-e0d3-43f1-be9e-237d6184c4b4": {
    "pk": "df662bbe-e0d3-43f1-be9e-237d6184c4b4",
    "title": "Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach",
    "abstract": "Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs.",
    "authors": [
      "Haoyu Han",
      "Juanhui Li",
      "Wei Huang",
      "Xianfeng Tang",
      "Hanqing Lu",
      "Chen Luo",
      "Hui Liu",
      "Jiliang Tang"
    ],
    "url": "http://arxiv.org/abs/2406.03464v1",
    "timestamp": 1717607558,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "eb0bc849-f4c8-4503-b0be-210b53d8fb31": {
    "pk": "eb0bc849-f4c8-4503-b0be-210b53d8fb31",
    "title": "Graph Neural Network Explanations are Fragile",
    "abstract": "Explainable Graph Neural Network (GNN) has emerged recently to foster the trust of using GNNs. Existing GNN explainers are developed from various perspectives to enhance the explanation performance. We take the first step to study GNN explainers under adversarial attack--We found that an adversary slightly perturbing graph structure can ensure GNN model makes correct predictions, but the GNN explainer yields a drastically different explanation on the perturbed graph. Specifically, we first formulate the attack problem under a practical threat model (i.e., the adversary has limited knowledge about the GNN explainer and a restricted perturbation budget). We then design two methods (i.e., one is loss-based and the other is deduction-based) to realize the attack. We evaluate our attacks on various GNN explainers and the results show these explainers are fragile.",
    "authors": [
      "Jiate Li",
      "Meng Pang",
      "Yun Dong",
      "Jinyuan Jia",
      "Binghui Wang"
    ],
    "url": "http://arxiv.org/abs/2406.03193v1",
    "timestamp": 1717590182,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "661013ab-175f-4e70-b74e-38b3bf2ab955": {
    "pk": "661013ab-175f-4e70-b74e-38b3bf2ab955",
    "title": "Enhancing the Resilience of Graph Neural Networks to Topological Perturbations in Sparse Graphs",
    "abstract": "Graph neural networks (GNNs) have been extensively employed in node classification. Nevertheless, recent studies indicate that GNNs are vulnerable to topological perturbations, such as adversarial attacks and edge disruptions. Considerable efforts have been devoted to mitigating these challenges. For example, pioneering Bayesian methodologies, including GraphSS and LlnDT, incorporate Bayesian label transitions and topology-based label sampling to strengthen the robustness of GNNs. However, GraphSS is hindered by slow convergence, while LlnDT faces challenges in sparse graphs. To overcome these limitations, we propose a novel label inference framework, TraTopo, which combines topology-driven label propagation, Bayesian label transitions, and link analysis via random walks. TraTopo significantly surpasses its predecessors on sparse graphs by utilizing random walk sampling, specifically targeting isolated nodes for link prediction, thus enhancing its effectiveness in topological sampling contexts. Additionally, TraTopo employs a shortest-path strategy to refine link prediction, thereby reducing predictive overhead and improving label inference accuracy. Empirical evaluations highlight TraTopo's superiority in node classification, significantly exceeding contemporary GCN models in accuracy.",
    "authors": [
      "Shuqi He",
      "Jun Zhuang",
      "Ding Wang",
      "Luyao Peng",
      "Jun Song"
    ],
    "url": "http://arxiv.org/abs/2406.03097v1",
    "timestamp": 1717580408,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0a38b297-5011-4bb9-9cbc-4fefc03ec175": {
    "pk": "0a38b297-5011-4bb9-9cbc-4fefc03ec175",
    "title": "Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections",
    "abstract": "Despite the remarkable capabilities demonstrated by Graph Neural Networks (GNNs) in graph-related tasks, recent research has revealed the fairness vulnerabilities in GNNs when facing malicious adversarial attacks. However, all existing fairness attacks require manipulating the connectivity between existing nodes, which may be prohibited in reality. To this end, we introduce a Node Injection-based Fairness Attack (NIFA), exploring the vulnerabilities of GNN fairness in such a more realistic setting. In detail, NIFA first designs two insightful principles for node injection operations, namely the uncertainty-maximization principle and homophily-increase principle, and then optimizes injected nodes' feature matrix to further ensure the effectiveness of fairness attacks. Comprehensive experiments on three real-world datasets consistently demonstrate that NIFA can significantly undermine the fairness of mainstream GNNs, even including fairness-aware GNNs, by injecting merely 1% of nodes. We sincerely hope that our work can stimulate increasing attention from researchers on the vulnerability of GNN fairness, and encourage the development of corresponding defense mechanisms.",
    "authors": [
      "Zihan Luo",
      "Hong Huang",
      "Yongkang Zhou",
      "Jiping Zhang",
      "Nuo Chen"
    ],
    "url": "http://arxiv.org/abs/2406.03052v1",
    "timestamp": 1717576013,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c0a0cb90-e64a-4d06-86e0-830cb303865f": {
    "pk": "c0a0cb90-e64a-4d06-86e0-830cb303865f",
    "title": "Efficient User Sequence Learning for Online Services via Compressed Graph Neural Networks",
    "abstract": "Learning representations of user behavior sequences is crucial for various online services, such as online fraudulent transaction detection mechanisms. Graph Neural Networks (GNNs) have been extensively applied to model sequence relationships, and extract information from similar sequences. While user behavior sequence data volume is usually huge for online applications, directly applying GNN models may lead to substantial computational overhead during both the training and inference stages and make it challenging to meet real-time requirements for online services. In this paper, we leverage graph compression techniques to alleviate the efficiency issue. Specifically, we propose a novel unified framework called ECSeq, to introduce graph compression techniques into relation modeling for user sequence representation learning. The key module of ECSeq is sequence relation modeling, which explores relationships among sequences to enhance sequence representation learning, and employs graph compression algorithms to achieve high efficiency and scalability. ECSeq also exhibits plug-and-play characteristics, seamlessly augmenting pre-trained sequence representation models without modifications. Empirical experiments on both sequence classification and regression tasks demonstrate the effectiveness of ECSeq. Specifically, with an additional training time of tens of seconds in total on 100,000+ sequences and inference time preserved within $10^{-4}$ seconds/sample, ECSeq improves the prediction R@P$_{0.9}$ of the widely used LSTM by $\\sim 5\\%$.",
    "authors": [
      "Yucheng Wu",
      "Liyue Chen",
      "Yu Cheng",
      "Shuai Chen",
      "Jinyu Xu",
      "Leye Wang"
    ],
    "url": "http://arxiv.org/abs/2406.02979v1",
    "timestamp": 1717568531,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ce6a9a83-6a64-443b-9308-99fc0dda9cb6": {
    "pk": "ce6a9a83-6a64-443b-9308-99fc0dda9cb6",
    "title": "Real-Time Small-Signal Security Assessment Using Graph Neural Networks",
    "abstract": "Security assessment is one of the most crucial functions of a power system operator. However, growing complexity and unpredictability make this an increasingly complex and computationally difficult task. In recent times, machine learning methods have gained attention for their ability to handle complex modeling applications. Some methods proposed include deep learning using convolutional neural networks, decision trees, etc. While these methods generate promising results, most methods still require long training times and computational resources. This paper proposes a graph neural network (GNN) approach to the small-signal security assessment problem using data from Phasor Measurement Units (PMUs). Leveraging the inherently graphical structure of the power grid using GNNs, training times can be reduced and efficiency improved for real-time application. Also, using graph properties, optimal PMU placement is determined and the proposed method is shown to perform efficiently under partial observability with limited PMU data. Case studies with simulated data from the IEEE 68-bus system and the NPCC 140-bus system are used to verify the effectiveness of the proposed method.",
    "authors": [
      "Glory Justin",
      "Santiago Paternain"
    ],
    "url": "http://arxiv.org/abs/2406.02964v1",
    "timestamp": 1717565966,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3529e1cd-476b-4488-b0e9-e4eed6a4b6ae": {
    "pk": "3529e1cd-476b-4488-b0e9-e4eed6a4b6ae",
    "title": "GraphAlign: Pretraining One Graph Neural Network on Multiple Graphs via Feature Alignment",
    "abstract": "Graph self-supervised learning (SSL) holds considerable promise for mining and learning with graph-structured data. Yet, a significant challenge in graph SSL lies in the feature discrepancy among graphs across different domains. In this work, we aim to pretrain one graph neural network (GNN) on a varied collection of graphs endowed with rich node features and subsequently apply the pretrained GNN to unseen graphs. We present a general GraphAlign method that can be seamlessly integrated into the existing graph SSL framework. To align feature distributions across disparate graphs, GraphAlign designs alignment strategies of feature encoding, normalization, alongside a mixture-of-feature-expert module. Extensive experiments show that GraphAlign empowers existing graph SSL frameworks to pretrain a unified and powerful GNN across multiple graphs, showcasing performance superiority on both in-domain and out-of-domain graphs.",
    "authors": [
      "Zhenyu Hou",
      "Haozhan Li",
      "Yukuo Cen",
      "Jie Tang",
      "Yuxiao Dong"
    ],
    "url": "http://arxiv.org/abs/2406.02953v1",
    "timestamp": 1717564952,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "332a1437-f3e4-4e40-9d2d-a412d5010a76": {
    "pk": "332a1437-f3e4-4e40-9d2d-a412d5010a76",
    "title": "Combinatorial Optimization with Automated Graph Neural Networks",
    "abstract": "In recent years, graph neural networks (GNNs) have become increasingly popular for solving NP-hard combinatorial optimization (CO) problems, such as maximum cut and maximum independent set. The core idea behind these methods is to represent a CO problem as a graph and then use GNNs to learn the node/graph embedding with combinatorial information. Although these methods have achieved promising results, given a specific CO problem, the design of GNN architectures still requires heavy manual work with domain knowledge. Existing automated GNNs are mostly focused on traditional graph learning problems, which is inapplicable to solving NP-hard CO problems. To this end, we present a new class of \\textbf{AUTO}mated \\textbf{G}NNs for solving \\textbf{NP}-hard problems, namely \\textbf{AutoGNP}. We represent CO problems by GNNs and focus on two specific problems, i.e., mixed integer linear programming and quadratic unconstrained binary optimization. The idea of AutoGNP is to use graph neural architecture search algorithms to automatically find the best GNNs for a given NP-hard combinatorial optimization problem. Compared with existing graph neural architecture search algorithms, AutoGNP utilizes two-hop operators in the architecture search space. Moreover, AutoGNP utilizes simulated annealing and a strict early stopping policy to avoid local optimal solutions. Empirical results on benchmark combinatorial problems demonstrate the superiority of our proposed model.",
    "authors": [
      "Yang Liu",
      "Peng Zhang",
      "Yang Gao",
      "Chuan Zhou",
      "Zhao Li",
      "Hongyang Chen"
    ],
    "url": "http://arxiv.org/abs/2406.02872v2",
    "timestamp": 1717555421,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "67f30b2d-17a4-4fc1-a7b7-ef996a6259a4": {
    "pk": "67f30b2d-17a4-4fc1-a7b7-ef996a6259a4",
    "title": "Temporal Graph Learning Recurrent Neural Network for Traffic Forecasting",
    "abstract": "Accurate traffic flow forecasting is a crucial research topic in transportation management. However, it is a challenging problem due to rapidly changing traffic conditions, high nonlinearity of traffic flow, and complex spatial and temporal correlations of road networks. Most existing studies either try to capture the spatial dependencies between roads using the same semantic graph over different time steps, or assume all sensors on the roads are equally likely to be connected regardless of the distance between them. However, we observe that the spatial dependencies between roads indeed change over time, and two distant roads are not likely to be helpful to each other when predicting the traffic flow, both of which limit the performance of existing studies. In this paper, we propose Temporal Graph Learning Recurrent Neural Network (TGLRN) to address these problems. More precisely, to effectively model the nature of time series, we leverage Recurrent Neural Networks (RNNs) to dynamically construct a graph at each time step, thereby capturing the time-evolving spatial dependencies between roads (i.e., microscopic view). Simultaneously, we provide the Adaptive Structure Information to the model, ensuring that close and consecutive sensors are considered to be more important for predicting the traffic flow (i.e., macroscopic view). Furthermore, to endow TGLRN with robustness, we introduce an edge sampling strategy when constructing the graph at each time step, which eventually leads to further improvements on the model performance. Experimental results on four commonly used real-world benchmark datasets show the effectiveness of TGLRN.",
    "authors": [
      "Sanghyun Lee",
      "Chanyoung Park"
    ],
    "url": "http://arxiv.org/abs/2406.02726v1",
    "timestamp": 1717528120,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "50f9d663-ba42-46e9-b9d3-9d1d33b9a8d2": {
    "pk": "50f9d663-ba42-46e9-b9d3-9d1d33b9a8d2",
    "title": "RoutePlacer: An End-to-End Routability-Aware Placer with Graph Neural Network",
    "abstract": "Placement is a critical and challenging step of modern chip design, with routability being an essential indicator of placement quality. Current routability-oriented placers typically apply an iterative two-stage approach, wherein the first stage generates a placement solution, and the second stage provides non-differentiable routing results to heuristically improve the solution quality. This method hinders jointly optimizing the routability aspect during placement. To address this problem, this work introduces RoutePlacer, an end-to-end routability-aware placement method. It trains RouteGNN, a customized graph neural network, to efficiently and accurately predict routability by capturing and fusing geometric and topological representations of placements. Well-trained RouteGNN then serves as a differentiable approximation of routability, enabling end-to-end gradient-based routability optimization. In addition, RouteGNN can improve two-stage placers as a plug-and-play alternative to external routers. Our experiments on DREAMPlace, an open-source AI4EDA platform, show that RoutePlacer can reduce Total Overflow by up to 16% while maintaining routed wirelength, compared to the state-of-the-art; integrating RouteGNN within two-stage placers leads to a 44% reduction in Total Overflow without compromising wirelength.",
    "authors": [
      "Yunbo Hou",
      "Haoran Ye",
      "Yingxue Zhang",
      "Siyuan Xu",
      "Guojie Song"
    ],
    "url": "http://arxiv.org/abs/2406.02651v1",
    "timestamp": 1717515581,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cc273163-7f8d-44ea-af86-3ba2f5768d2a": {
    "pk": "cc273163-7f8d-44ea-af86-3ba2f5768d2a",
    "title": "AMOSL: Adaptive Modality-wise Structure Learning in Multi-view Graph Neural Networks For Enhanced Unified Representation",
    "abstract": "While Multi-view Graph Neural Networks (MVGNNs) excel at leveraging diverse modalities for learning object representation, existing methods assume identical local topology structures across modalities that overlook real-world discrepancies. This leads MVGNNs straggles in modality fusion and representations denoising. To address these issues, we propose adaptive modality-wise structure learning (AMoSL). AMoSL captures node correspondences between modalities via optimal transport, and jointly learning with graph embedding. To enable efficient end-to-end training, we employ an efficient solution for the resulting complex bilevel optimization problem. Furthermore, AMoSL adapts to downstream tasks through unsupervised learning on inter-modality distances. The effectiveness of AMoSL is demonstrated by its ability to train more accurate graph classifiers on six benchmark datasets.",
    "authors": [
      "Peiyu Liang",
      "Hongchang Gao",
      "Xubin He"
    ],
    "url": "http://arxiv.org/abs/2406.02348v1",
    "timestamp": 1717511070,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d0b5e754-1fa6-41bd-8ce2-8c82f9908543": {
    "pk": "d0b5e754-1fa6-41bd-8ce2-8c82f9908543",
    "title": "Graph Neural Networks Do Not Always Oversmooth",
    "abstract": "Graph neural networks (GNNs) have emerged as powerful tools for processing relational data in applications. However, GNNs suffer from the problem of oversmoothing, the property that the features of all nodes exponentially converge to the same vector over layers, prohibiting the design of deep GNNs. In this work we study oversmoothing in graph convolutional networks (GCNs) by using their Gaussian process (GP) equivalence in the limit of infinitely many hidden features. By generalizing methods from conventional deep neural networks (DNNs), we can describe the distribution of features at the output layer of deep GCNs in terms of a GP: as expected, we find that typical parameter choices from the literature lead to oversmoothing. The theory, however, allows us to identify a new, nonoversmoothing phase: if the initial weights of the network have sufficiently large variance, GCNs do not oversmooth, and node features remain informative even at large depth. We demonstrate the validity of this prediction in finite-size GCNs by training a linear classifier on their output. Moreover, using the linearization of the GCN GP, we generalize the concept of propagation depth of information from DNNs to GCNs. This propagation depth diverges at the transition between the oversmoothing and non-oversmoothing phase. We test the predictions of our approach and find good agreement with finite-size GCNs. Initializing GCNs near the transition to the non-oversmoothing phase, we obtain networks which are both deep and expressive.",
    "authors": [
      "Bastian Epping",
      "Alexandre Ren\u00e9",
      "Moritz Helias",
      "Michael T. Schaub"
    ],
    "url": "http://arxiv.org/abs/2406.02269v1",
    "timestamp": 1717505233,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ML",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e619c26d-da4f-4cdc-9174-58f1ae92cfa1": {
    "pk": "e619c26d-da4f-4cdc-9174-58f1ae92cfa1",
    "title": "DFA-GNN: Forward Learning of Graph Neural Networks by Direct Feedback Alignment",
    "abstract": "Graph neural networks are recognized for their strong performance across various applications, with the backpropagation algorithm playing a central role in the development of most GNN models. However, despite its effectiveness, BP has limitations that challenge its biological plausibility and affect the efficiency, scalability and parallelism of training neural networks for graph-based tasks. While several non-BP training algorithms, such as the direct feedback alignment, have been successfully applied to fully-connected and convolutional network components for handling Euclidean data, directly adapting these non-BP frameworks to manage non-Euclidean graph data in GNN models presents significant challenges. These challenges primarily arise from the violation of the i.i.d. assumption in graph data and the difficulty in accessing prediction errors for all samples (nodes) within the graph. To overcome these obstacles, in this paper we propose DFA-GNN, a novel forward learning framework tailored for GNNs with a case study of semi-supervised learning. The proposed method breaks the limitations of BP by using a dedicated forward training mechanism. Specifically, DFA-GNN extends the principles of DFA to adapt to graph data and unique architecture of GNNs, which incorporates the information of graph topology into the feedback links to accommodate the non-Euclidean characteristics of graph data. Additionally, for semi-supervised graph learning tasks, we developed a pseudo error generator that spreads residual errors from training data to create a pseudo error for each unlabeled node. These pseudo errors are then utilized to train GNNs using DFA. Extensive experiments on 10 public benchmarks reveal that our learning framework outperforms not only previous non-BP methods but also the standard BP methods, and it exhibits excellent robustness against various types of noise and attacks.",
    "authors": [
      "Gongpei Zhao",
      "Tao Wang",
      "Congyan Lang",
      "Yi Jin",
      "Yidong Li",
      "Haibin Ling"
    ],
    "url": "http://arxiv.org/abs/2406.02040v1",
    "timestamp": 1717485891,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b88fab04-6291-494b-99fd-ec9c870ec78a": {
    "pk": "b88fab04-6291-494b-99fd-ec9c870ec78a",
    "title": "Bayesian Mesh Optimization for Graph Neural Networks to Enhance Engineering Performance Prediction",
    "abstract": "In engineering design, surrogate models are widely employed to replace computationally expensive simulations by leveraging design variables and geometric parameters from computer-aided design (CAD) models. However, these models often lose critical information when simplified to lower dimensions and face challenges in parameter definition, especially with the complex 3D shapes commonly found in industrial datasets. To address these limitations, we propose a Bayesian graph neural network (GNN) framework for a 3D deep-learning-based surrogate model that predicts engineering performance by directly learning geometric features from CAD using mesh representation. Our framework determines the optimal size of mesh elements through Bayesian optimization, resulting in a high-accuracy surrogate model. Additionally, it effectively handles the irregular and complex structures of 3D CADs, which differ significantly from the regular and uniform pixel structures of 2D images typically used in deep learning. Experimental results demonstrate that the quality of the mesh significantly impacts the prediction accuracy of the surrogate model, with an optimally sized mesh achieving superior performance. We compare the performance of models based on various 3D representations such as voxel, point cloud, and graph, and evaluate the computational costs of Monte Carlo simulation and Bayesian optimization methods to find the optimal mesh size. We anticipate that our proposed framework has the potential to be applied to mesh-based simulations across various engineering fields, leveraging physics-based information commonly used in computer-aided engineering.",
    "authors": [
      "Jangseop Park",
      "Namwoo Kang"
    ],
    "url": "http://arxiv.org/abs/2406.01996v1",
    "timestamp": 1717482468,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "81c34562-b0cf-4da4-93e3-3dbfae109f3f": {
    "pk": "81c34562-b0cf-4da4-93e3-3dbfae109f3f",
    "title": "Graph Neural Network Enhanced Retrieval for Question Answering of LLMs",
    "abstract": "Retrieval augmented generation has revolutionized large language model (LLM) outputs by providing factual supports. Nevertheless, it struggles to capture all the necessary knowledge for complex reasoning questions. Existing retrieval methods typically divide reference documents into passages, treating them in isolation. These passages, however, are often interrelated, such as passages that are contiguous or share the same keywords. Therefore, recognizing the relatedness is crucial for enhancing the retrieval process. In this paper, we propose a novel retrieval method, called GNN-Ret, which leverages graph neural networks (GNNs) to enhance retrieval by considering the relatedness between passages. Specifically, we first construct a graph of passages by connecting passages that are structure-related and keyword-related. A graph neural network (GNN) is then leveraged to exploit the relationships between passages and improve the retrieval of supporting passages. Furthermore, we extend our method to handle multi-hop reasoning questions using a recurrent graph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates the graphs of passages from previous steps, thereby enhancing the retrieval of supporting passages. Extensive experiments on benchmark datasets demonstrate that GNN-Ret achieves higher accuracy for question answering with a single query of LLMs than strong baselines that require multiple queries, and RGNN-Ret further improves accuracy and achieves state-of-the-art performance, with up to 10.4% accuracy improvement on the 2WikiMQA dataset.",
    "authors": [
      "Zijian Li",
      "Qingyan Guo",
      "Jiawei Shao",
      "Lei Song",
      "Jiang Bian",
      "Jun Zhang",
      "Rui Wang"
    ],
    "url": "http://arxiv.org/abs/2406.06572v1",
    "timestamp": 1717434466,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f975e09f-a3f6-46b2-ade5-5d46ce18c319": {
    "pk": "f975e09f-a3f6-46b2-ade5-5d46ce18c319",
    "title": "The Intelligible and Effective Graph Neural Additive Networks",
    "abstract": "Graph Neural Networks (GNNs) have emerged as the predominant approach for learning over graph-structured data. However, most GNNs operate as black-box models and require post-hoc explanations, which may not suffice in high-stakes scenarios where transparency is crucial. In this paper, we present a GNN that is interpretable by design. Our model, Graph Neural Additive Network (GNAN), is a novel extension of the interpretable class of Generalized Additive Models, and can be visualized and fully understood by humans. GNAN is designed to be fully interpretable, allowing both global and local explanations at the feature and graph levels through direct visualization of the model. These visualizations describe the exact way the model uses the relationships between the target variable, the features, and the graph. We demonstrate the intelligibility of GNANs in a series of examples on different tasks and datasets. In addition, we show that the accuracy of GNAN is on par with black-box GNNs, making it suitable for critical applications where transparency is essential, alongside high accuracy.",
    "authors": [
      "Maya Bechler-Speicher",
      "Amir Globerson",
      "Ran Gilad-Bachrach"
    ],
    "url": "http://arxiv.org/abs/2406.01317v1",
    "timestamp": 1717421376,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3e2f787a-256a-4d08-898a-27a5497e7248": {
    "pk": "3e2f787a-256a-4d08-898a-27a5497e7248",
    "title": "A hybrid numerical methodology coupling Reduced Order Modeling and Graph Neural Networks for non-parametric geometries: applications to structural dynamics problems",
    "abstract": "This work introduces a new approach for accelerating the numerical analysis of time-domain partial differential equations (PDEs) governing complex physical systems. The methodology is based on a combination of a classical reduced-order modeling (ROM) framework and recently-introduced Graph Neural Networks (GNNs), where the latter is trained on highly heterogeneous databases of varying numerical discretization sizes. The proposed techniques are shown to be particularly suitable for non-parametric geometries, ultimately enabling the treatment of a diverse range of geometries and topologies. Performance studies are presented in an application context related to the design of aircraft seats and their corresponding mechanical responses to shocks, where the main motivation is to reduce the computational burden and enable the rapid design iteration for such problems that entail non-parametric geometries. The methods proposed here are straightforwardly applicable to other scientific or engineering problems requiring a large number of finite element-based numerical simulations, with the potential to significantly enhance efficiency while maintaining reasonable accuracy.",
    "authors": [
      "Victor Matray",
      "Faisal Amlani",
      "Fr\u00e9d\u00e9ric Feyel",
      "David N\u00e9ron"
    ],
    "url": "http://arxiv.org/abs/2406.02615v1",
    "timestamp": 1717404685,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7ae333f1-766c-4bc3-87cf-095cd8be906b": {
    "pk": "7ae333f1-766c-4bc3-87cf-095cd8be906b",
    "title": "Explore Internal and External Similarity for Single Image Deraining with Graph Neural Networks",
    "abstract": "Patch-level non-local self-similarity is an important property of natural images. However, most existing methods do not consider this property into neural networks for image deraining, thus affecting recovery performance. Motivated by this property, we find that there exists significant patch recurrence property of a rainy image, that is, similar patches tend to recur many times in one image and its multi-scale images and external images. To better model this property for image detaining, we develop a multi-scale graph network with exemplars, called MSGNN, that contains two branches: 1) internal data-based supervised branch is used to model the internal relations of similar patches from the rainy image itself and its multi-scale images and 2) external data-participated unsupervised branch is used to model the external relations of the similar patches in the rainy image and exemplar. Specifically, we construct a graph model by searching the k-nearest neighboring patches from both the rainy images in a multi-scale framework and the exemplar. After obtaining the corresponding k neighboring patches from the multi-scale images and exemplar, we build a graph and aggregate them in an attentional manner so that the graph can provide more information from similar patches for image deraining. We embed the proposed graph in a deep neural network and train it in an end-to-end manner. Extensive experiments demonstrate that the proposed algorithm performs favorably against eight state-of-the-art methods on five public synthetic datasets and one real-world dataset. The source codes will be available at https://github.com/supersupercong/MSGNN.",
    "authors": [
      "Cong Wang",
      "Wei Wang",
      "Chengjin Yu",
      "Jie Mu"
    ],
    "url": "http://arxiv.org/abs/2406.00721v1",
    "timestamp": 1717330096,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b0669b56-44c9-4762-a725-119135a0e7ba": {
    "pk": "b0669b56-44c9-4762-a725-119135a0e7ba",
    "title": "A Multi-Graph Convolutional Neural Network Model for Short-Term Prediction of Turning Movements at Signalized Intersections",
    "abstract": "Traffic flow forecasting is a crucial first step in intelligent and proactive traffic management. Traffic flow parameters are volatile and uncertain, making traffic flow forecasting a difficult task if the appropriate forecasting model is not used. Additionally, the non-Euclidean data structure of traffic flow parameters is challenging to analyze from both spatial and temporal perspectives. State-of-the-art deep learning approaches use pure convolution, recurrent neural networks, and hybrid methods to achieve this objective efficiently. However, many of the approaches in the literature rely on complex architectures that can be difficult to train. This complexity also adds to the black-box nature of deep learning. This study introduces a novel deep learning architecture, referred to as the multigraph convolution neural network (MGCNN), for turning movement prediction at intersections. The proposed architecture combines a multigraph structure, built to model temporal variations in traffic data, with a spectral convolution operation to support modeling the spatial variations in traffic data over the graphs. The proposed model was tested using twenty days of flow and traffic control data collected from an arterial in downtown Chattanooga, TN, with ten signalized intersections. The model's ability to perform short-term predictions over 1, 2, 3, 4, and 5 minutes into the future was evaluated against four baseline state-of-the-art models. The results showed that our proposed model is superior to the other baseline models in predicting turning movements with a mean squared error (MSE) of 0.9",
    "authors": [
      "Jewel Rana Palit",
      "Osama A Osman"
    ],
    "url": "http://arxiv.org/abs/2406.00619v1",
    "timestamp": 1717306885,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a81d7d8d-827c-4aa9-802b-39849bd374a5": {
    "pk": "a81d7d8d-827c-4aa9-802b-39849bd374a5",
    "title": "Graph Neural Network Training Systems: A Performance Comparison of Full-Graph and Mini-Batch",
    "abstract": "Graph Neural Networks (GNNs) have gained significant attention in recent years due to their ability to learn representations of graph structured data. Two common methods for training GNNs are mini-batch training and full-graph training. Since these two methods require different training pipelines and systems optimizations, two separate categories of GNN training systems emerged, each tailored for one method. Works that introduce systems belonging to a particular category predominantly compare them with other systems within the same category, offering limited or no comparison with systems from the other category. Some prior work also justifies its focus on one specific training method by arguing that it achieves higher accuracy than the alternative. The literature, however, has incomplete and contradictory evidence in this regard. In this paper, we provide a comprehensive empirical comparison of full-graph and mini-batch GNN training systems to get a clearer picture of the state of the art in the field. We find that the mini-batch training systems we consider consistently converge faster than the full-graph training ones across multiple datasets, GNN models, and system configurations, with speedups between 2.4x - 15.2x. We also find that both training techniques converge to similar accuracy values, so comparing systems across the two categories in terms of time-to-accuracy is a sound approach.",
    "authors": [
      "Saurabh Bajaj",
      "Hui Guan",
      "Marco Serafini"
    ],
    "url": "http://arxiv.org/abs/2406.00552v2",
    "timestamp": 1717276044,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "29930e53-b4b8-464c-b9ed-9d9b4be93035": {
    "pk": "29930e53-b4b8-464c-b9ed-9d9b4be93035",
    "title": "Graph Neural Networks for Brain Graph Learning: A Survey",
    "abstract": "Exploring the complex structure of the human brain is crucial for understanding its functionality and diagnosing brain disorders. Thanks to advancements in neuroimaging technology, a novel approach has emerged that involves modeling the human brain as a graph-structured pattern, with different brain regions represented as nodes and the functional relationships among these regions as edges. Moreover, graph neural networks (GNNs) have demonstrated a significant advantage in mining graph-structured data. Developing GNNs to learn brain graph representations for brain disorder analysis has recently gained increasing attention. However, there is a lack of systematic survey work summarizing current research methods in this domain. In this paper, we aim to bridge this gap by reviewing brain graph learning works that utilize GNNs. We first introduce the process of brain graph modeling based on common neuroimaging data. Subsequently, we systematically categorize current works based on the type of brain graph generated and the targeted research problems. To make this research accessible to a broader range of interested researchers, we provide an overview of representative methods and commonly used datasets, along with their implementation sources. Finally, we present our insights on future research directions. The repository of this survey is available at \\url{https://github.com/XuexiongLuoMQ/Awesome-Brain-Graph-Learning-with-GNNs}.",
    "authors": [
      "Xuexiong Luo",
      "Jia Wu",
      "Jian Yang",
      "Shan Xue",
      "Amin Beheshti",
      "Quan Z. Sheng",
      "David McAlpine",
      "Paul Sowman",
      "Alexis Giral",
      "Philip S. Yu"
    ],
    "url": "http://arxiv.org/abs/2406.02594v1",
    "timestamp": 1717210059,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5cb36c02-3aa8-4c73-9e41-116dcecdd4e7": {
    "pk": "5cb36c02-3aa8-4c73-9e41-116dcecdd4e7",
    "title": "SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation",
    "abstract": "Sequential recommendation effectively addresses information overload by modeling users' temporal and sequential interaction patterns. To overcome the limitations of supervision signals, recent approaches have adopted self-supervised learning techniques in recommender systems. However, there are still two critical challenges that remain unsolved. Firstly, existing sequential models primarily focus on long-term modeling of individual interaction sequences, overlooking the valuable short-term collaborative relationships among the behaviors of different users. Secondly, real-world data often contain noise, particularly in users' short-term behaviors, which can arise from temporary intents or misclicks. Such noise negatively impacts the accuracy of both graph and sequence models, further complicating the modeling process. To address these challenges, we propose a novel framework called Self-Supervised Graph Neural Network (SelfGNN) for sequential recommendation. The SelfGNN framework encodes short-term graphs based on time intervals and utilizes Graph Neural Networks (GNNs) to learn short-term collaborative relationships. It captures long-term user and item representations at multiple granularity levels through interval fusion and dynamic behavior modeling. Importantly, our personalized self-augmented learning structure enhances model robustness by mitigating noise in short-term graphs based on long-term user interests and personal stability. Extensive experiments conducted on four real-world datasets demonstrate that SelfGNN outperforms various state-of-the-art baselines. Our model implementation codes are available at https://github.com/HKUDS/SelfGNN.",
    "authors": [
      "Yuxi Liu",
      "Lianghao Xia",
      "Chao Huang"
    ],
    "url": "http://arxiv.org/abs/2405.20878v1",
    "timestamp": 1717167192,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "93915d66-dcb7-45f5-a808-2701ba2b9c28": {
    "pk": "93915d66-dcb7-45f5-a808-2701ba2b9c28",
    "title": "Heterophilous Distribution Propagation for Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in various graph mining tasks by aggregating information from neighborhoods for representation learning. The success relies on the homophily assumption that nearby nodes exhibit similar behaviors, while it may be violated in many real-world graphs. Recently, heterophilous graph neural networks (HeterGNNs) have attracted increasing attention by modifying the neural message passing schema for heterophilous neighborhoods. However, they suffer from insufficient neighborhood partition and heterophily modeling, both of which are critical but challenging to break through. To tackle these challenges, in this paper, we propose heterophilous distribution propagation (HDP) for graph neural networks. Instead of aggregating information from all neighborhoods, HDP adaptively separates the neighbors into homophilous and heterphilous parts based on the pseudo assignments during training. The heterophilous neighborhood distribution is learned with orthogonality-oriented constraint via a trusted prototype contrastive learning paradigm. Both the homophilous and heterophilous patterns are propagated with a novel semantic-aware message passing mechanism. We conduct extensive experiments on 9 benchmark datasets with different levels of homophily. Experimental results show that our method outperforms representative baselines on heterophilous datasets.",
    "authors": [
      "Zhuonan Zheng",
      "Sheng Zhou",
      "Hongjia Xu",
      "Ming Gu",
      "Yilun Xu",
      "Ao Li",
      "Yuhong Li",
      "Jingjun Gu",
      "Jiajun Bu"
    ],
    "url": "http://arxiv.org/abs/2405.20640v1",
    "timestamp": 1717137656,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c5d03684-ee53-47df-8ffe-68ae037252bd": {
    "pk": "c5d03684-ee53-47df-8ffe-68ae037252bd",
    "title": "Flexible SE(2) graph neural networks with applications to PDE surrogates",
    "abstract": "This paper presents a novel approach for constructing graph neural networks equivariant to 2D rotations and translations and leveraging them as PDE surrogates on non-gridded domains. We show that aligning the representations with the principal axis allows us to sidestep many constraints while preserving SE(2) equivariance. By applying our model as a surrogate for fluid flow simulations and conducting thorough benchmarks against non-equivariant models, we demonstrate significant gains in terms of both data efficiency and accuracy.",
    "authors": [
      "Maria B\u00e5nkestad",
      "Olof Mogren",
      "Aleksis Pirinen"
    ],
    "url": "http://arxiv.org/abs/2405.20287v1",
    "timestamp": 1717090755,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "171f02ba-4320-42ff-93ea-9c20fb5fca89": {
    "pk": "171f02ba-4320-42ff-93ea-9c20fb5fca89",
    "title": "Combining physics-informed graph neural network and finite difference for solving forward and inverse spatiotemporal PDEs",
    "abstract": "The great success of Physics-Informed Neural Networks (PINN) in solving partial differential equations (PDEs) has significantly advanced our simulation and understanding of complex physical systems in science and engineering. However, many PINN-like methods are poorly scalable and are limited to in-sample scenarios. To address these challenges, this work proposes a novel discrete approach termed Physics-Informed Graph Neural Network (PIGNN) to solve forward and inverse nonlinear PDEs. In particular, our approach seamlessly integrates the strength of graph neural networks (GNN), physical equations and finite difference to approximate solutions of physical systems. Our approach is compared with the PINN baseline on three well-known nonlinear PDEs (heat, Burgers and FitzHugh-Nagumo). We demonstrate the excellent performance of the proposed method to work with irregular meshes, longer time steps, arbitrary spatial resolutions, varying initial conditions (ICs) and boundary conditions (BCs) by conducting extensive numerical experiments. Numerical results also illustrate the superiority of our approach in terms of accuracy, time extrapolability, generalizability and scalability. The main advantage of our approach is that models trained in small domains with simple settings have excellent fitting capabilities and can be directly applied to more complex situations in large domains.",
    "authors": [
      "Hao Zhang",
      "Longxiang Jiang",
      "Xinkun Chu",
      "Yong Wen",
      "Luxiong Li",
      "Yonghao Xiao",
      "Liyuan Wang"
    ],
    "url": "http://arxiv.org/abs/2405.20000v1",
    "timestamp": 1717072778,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.NA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9552e5f4-bfcf-4706-851b-770de13bfcda": {
    "pk": "9552e5f4-bfcf-4706-851b-770de13bfcda",
    "title": "Spatio-Spectral Graph Neural Networks",
    "abstract": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their \"receptive field\" is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.",
    "authors": [
      "Simon Geisler",
      "Arthur Kosmala",
      "Daniel Herbst",
      "Stephan G\u00fcnnemann"
    ],
    "url": "http://arxiv.org/abs/2405.19121v2",
    "timestamp": 1716992888,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "666bc6ff-0c75-41e6-8588-bad6aea0ec03": {
    "pk": "666bc6ff-0c75-41e6-8588-bad6aea0ec03",
    "title": "SIG: Efficient Self-Interpretable Graph Neural Network for Continuous-time Dynamic Graphs",
    "abstract": "While dynamic graph neural networks have shown promise in various applications, explaining their predictions on continuous-time dynamic graphs (CTDGs) is difficult. This paper investigates a new research task: self-interpretable GNNs for CTDGs. We aim to predict future links within the dynamic graph while simultaneously providing causal explanations for these predictions. There are two key challenges: (1) capturing the underlying structural and temporal information that remains consistent across both independent and identically distributed (IID) and out-of-distribution (OOD) data, and (2) efficiently generating high-quality link prediction results and explanations. To tackle these challenges, we propose a novel causal inference model, namely the Independent and Confounded Causal Model (ICCM). ICCM is then integrated into a deep learning architecture that considers both effectiveness and efficiency. Extensive experiments demonstrate that our proposed model significantly outperforms existing methods across link prediction accuracy, explanation quality, and robustness to shortcut features. Our code and datasets are anonymously released at https://github.com/2024SIG/SIG.",
    "authors": [
      "Lanting Fang",
      "Yulian Yang",
      "Kai Wang",
      "Shanshan Feng",
      "Kaiyu Feng",
      "Jie Gui",
      "Shuliang Wang",
      "Yew-Soon Ong"
    ],
    "url": "http://arxiv.org/abs/2405.19062v1",
    "timestamp": 1716988173,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "09868550-bbb7-4fff-a757-acef370f8681": {
    "pk": "09868550-bbb7-4fff-a757-acef370f8681",
    "title": "LSPI: Heterogeneous Graph Neural Network Classification Aggregation Algorithm Based on Size Neighbor Path Identification",
    "abstract": "Existing heterogeneous graph neural network algorithms (HGNNs) mostly rely on meta-paths to capture the rich semantic information contained in heterogeneous graphs (also known as heterogeneous information networks (HINs)), but most of these HGNNs focus on different ways of feature aggre gation and ignore the properties of the meta-paths themselves. This paper studies meta-paths in three commonly used data sets and finds that there are huge differences in the number of neighbors connected by different meta paths. At the same time, the noise information contained in large neigh bor paths will have an adverse impact on model performance. Therefore, this paper proposes a Heterogeneous Graph Neural Network Classification and Aggregation Algorithm Based on Large and Small Neighbor Path Iden tification(LSPI). LSPI firstly divides the meta-paths into large and small neighbor paths through the path discriminator , and in order to reduce the noise interference problem in large neighbor paths, LSPI selects neighbor nodes with higher similarity from both topology and feature perspectives, and passes small neighbor paths and filtered large neighbor paths through different graph convolution components. Aggregation is performed to obtain feature information under different subgraphs, and then LSPI uses subgraph level attention to fuse the feature information under different subgraphs to generate the final node embedding. Finally this paper verifies the superiority of the method through extensive experiments and also gives suggestions on the number of nodes to be retained in large neighbor paths through exper iments. The complete reproducible code adn data has been published at: https://github.com/liuhua811/LSPIA.",
    "authors": [
      "Yufei Zhao",
      "Shiduo Wang",
      "Hua Duan"
    ],
    "url": "http://arxiv.org/abs/2405.18933v2",
    "timestamp": 1716975443,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ce1bf0a0-7b9b-46f5-99e3-42fc2756dba2": {
    "pk": "ce1bf0a0-7b9b-46f5-99e3-42fc2756dba2",
    "title": "Inverse Design of Promising Alloys for Electrocatalytic CO$_2$ Reduction via Generative Graph Neural Networks Combined with Bird Swarm Algorithm",
    "abstract": "Directly generating material structures with optimal properties is a long-standing goal in material design. One of the fundamental challenges lies in how to overcome the limitation of traditional generative models to efficiently explore the global chemical space rather than a small localized space. Herein, we develop a framework named MAGECS to address this dilemma, by integrating the bird swarm algorithm and supervised graph neural network to effectively navigate the generative model in the immense chemical space towards materials with target properties. As a demonstration, MAGECS is applied to design compelling alloy electrocatalysts for CO$_2$ reduction reaction (CO$_2$RR) and works extremely well. Specifically, the chemical space of CO$_2$RR is effectively explored, where over 250,000 promising structures with high activity have been generated and notably, the proportion of desired structures is 2.5-fold increased. Moreover, five predicted alloys, i.e., CuAl, AlPd, Sn$_2$Pd$_5$, Sn$_9$Pd$_7$, and CuAlSe$_2$ are successfully synthesized and characterized experimentally, two of which exhibit about 90% Faraday efficiency of CO$_2$RR, and CuAl achieved 76% efficiency for C$_2$ products. This pioneering application of inverse design in CO$_2$RR catalysis showcases the potential of MAGECS to dramatically accelerate the development of functional materials, paving the way for fully automated, artificial intelligence-driven material design.",
    "authors": [
      "Zhilong Song",
      "Linfeng Fan",
      "Shuaihua Lu",
      "Qionghua Zhou",
      "Chongyi Ling",
      "Jinlan Wang"
    ],
    "url": "http://arxiv.org/abs/2405.18891v1",
    "timestamp": 1716972473,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cond-mat.mtrl-sci",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8e687cdf-f014-4b23-ae1c-107ccdad0830": {
    "pk": "8e687cdf-f014-4b23-ae1c-107ccdad0830",
    "title": "Spatiotemporal Forecasting Meets Efficiency: Causal Graph Process Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have advanced spatiotemporal forecasting by leveraging relational inductive biases among sensors (or any other measuring scheme) represented as nodes in a graph. However, current methods often rely on Recurrent Neural Networks (RNNs), leading to increased runtimes and memory use. Moreover, these methods typically operate within 1-hop neighborhoods, exacerbating the reduction of the receptive field. Causal Graph Processes (CGPs) offer an alternative, using graph filters instead of MLP layers to reduce parameters and minimize memory consumption. This paper introduces the Causal Graph Process Neural Network (CGProNet), a non-linear model combining CGPs and GNNs for spatiotemporal forecasting. CGProNet employs higher-order graph filters, optimizing the model with fewer parameters, reducing memory usage, and improving runtime efficiency. We present a comprehensive theoretical and experimental stability analysis, highlighting key aspects of CGProNet. Experiments on synthetic and real data demonstrate CGProNet's superior efficiency, minimizing memory and time requirements while maintaining competitive forecasting performance.",
    "authors": [
      "Aref Einizade",
      "Fragkiskos D. Malliaros",
      "Jhony H. Giraldo"
    ],
    "url": "http://arxiv.org/abs/2405.18879v1",
    "timestamp": 1716971868,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8c47bb08-b005-4028-8e07-88aa5f9ce15f": {
    "pk": "8c47bb08-b005-4028-8e07-88aa5f9ce15f",
    "title": "Continuous Product Graph Neural Networks",
    "abstract": "Processing multidomain data defined on multiple graphs holds significant potential in various practical applications in computer science. However, current methods are mostly limited to discrete graph filtering operations. Tensorial partial differential equations on graphs (TPDEGs) provide a principled framework for modeling structured data across multiple interacting graphs, addressing the limitations of the existing discrete methodologies. In this paper, we introduce Continuous Product Graph Neural Networks (CITRUS) that emerge as a natural solution to the TPDEG. CITRUS leverages the separability of continuous heat kernels from Cartesian graph products to efficiently implement graph spectral decomposition. We conduct thorough theoretical analyses of the stability and over-smoothing properties of CITRUS in response to domain-specific graph perturbations and graph spectra effects on the performance. We evaluate CITRUS on well-known traffic and weather spatiotemporal forecasting datasets, demonstrating superior performance over existing approaches.",
    "authors": [
      "Aref Einizade",
      "Fragkiskos D. Malliaros",
      "Jhony H. Giraldo"
    ],
    "url": "http://arxiv.org/abs/2405.18877v1",
    "timestamp": 1716971769,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4bfda434-223a-4fcb-8d66-fd3a81ddd93b": {
    "pk": "4bfda434-223a-4fcb-8d66-fd3a81ddd93b",
    "title": "Node Injection Attack Based on Label Propagation Against Graph Neural Network",
    "abstract": "Graph Neural Network (GNN) has achieved remarkable success in various graph learning tasks, such as node classification, link prediction and graph classification. The key to the success of GNN lies in its effective structure information representation through neighboring aggregation. However, the attacker can easily perturb the aggregation process through injecting fake nodes, which reveals that GNN is vulnerable to the graph injection attack. Existing graph injection attack methods primarily focus on damaging the classical feature aggregation process while overlooking the neighborhood aggregation process via label propagation. To bridge this gap, we propose the label-propagation-based global injection attack (LPGIA) which conducts the graph injection attack on the node classification task. Specifically, we analyze the aggregation process from the perspective of label propagation and transform the graph injection attack problem into a global injection label specificity attack problem. To solve this problem, LPGIA utilizes a label propagation-based strategy to optimize the combinations of the nodes connected to the injected node. Then, LPGIA leverages the feature mapping to generate malicious features for injected nodes. In extensive experiments against representative GNNs, LPGIA outperforms the previous best-performing injection attack method in various datasets, demonstrating its superiority and transferability.",
    "authors": [
      "Peican Zhu",
      "Zechen Pan",
      "Keke Tang",
      "Xiaodong Cui",
      "Jinhuan Wang",
      "Qi Xuan"
    ],
    "url": "http://arxiv.org/abs/2405.18824v1",
    "timestamp": 1716966556,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "62e16672-b106-4514-ba92-315a9d8efea4": {
    "pk": "62e16672-b106-4514-ba92-315a9d8efea4",
    "title": "DeepHGNN: Study of Graph Neural Network based Forecasting Methods for Hierarchically Related Multivariate Time Series",
    "abstract": "Graph Neural Networks (GNN) have gained significant traction in the forecasting domain, especially for their capacity to simultaneously account for intra-series temporal correlations and inter-series relationships. This paper introduces a novel Hierarchical GNN (DeepHGNN) framework, explicitly designed for forecasting in complex hierarchical structures. The uniqueness of DeepHGNN lies in its innovative graph-based hierarchical interpolation and an end-to-end reconciliation mechanism. This approach ensures forecast accuracy and coherence across various hierarchical levels while sharing signals across them, addressing a key challenge in hierarchical forecasting. A critical insight in hierarchical time series is the variance in forecastability across levels, with upper levels typically presenting more predictable components. DeepHGNN capitalizes on this insight by pooling and leveraging knowledge from all hierarchy levels, thereby enhancing the overall forecast accuracy. Our comprehensive evaluation set against several state-of-the-art models confirm the superior performance of DeepHGNN. This research not only demonstrates DeepHGNN's effectiveness in achieving significantly improved forecast accuracy but also contributes to the understanding of graph-based methods in hierarchical time series forecasting.",
    "authors": [
      "Abishek Sriramulu",
      "Nicolas Fourrier",
      "Christoph Bergmeir"
    ],
    "url": "http://arxiv.org/abs/2405.18693v1",
    "timestamp": 1716948377,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0ca2bf12-2565-4be6-b83e-051eead247fd": {
    "pk": "0ca2bf12-2565-4be6-b83e-051eead247fd",
    "title": "Injecting Hierarchical Biological Priors into Graph Neural Networks for Flow Cytometry Prediction",
    "abstract": "In the complex landscape of hematologic samples such as peripheral blood or bone marrow derived from flow cytometry (FC) data, cell-level prediction presents profound challenges. This work explores injecting hierarchical prior knowledge into graph neural networks (GNNs) for single-cell multi-class classification of tabular cellular data. By representing the data as graphs and encoding hierarchical relationships between classes, we propose our hierarchical plug-in method to be applied to several GNN models, namely, FCHC-GNN, and effectively designed to capture neighborhood information crucial for single-cell FC domain. Extensive experiments on our cohort of 19 distinct patients, demonstrate that incorporating hierarchical biological constraints boosts performance significantly across multiple metrics compared to baseline GNNs without such priors. The proposed approach highlights the importance of structured inductive biases for gaining improved generalization in complex biological prediction tasks.",
    "authors": [
      "Fatemeh Nassajian Mojarrad",
      "Lorenzo Bini",
      "Thomas Matthes",
      "St\u00e9phane Marchand-Maillet"
    ],
    "url": "http://arxiv.org/abs/2405.18507v1",
    "timestamp": 1716920656,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "feee148f-37bb-4459-a5bd-5da71d9f4e7a": {
    "pk": "feee148f-37bb-4459-a5bd-5da71d9f4e7a",
    "title": "A Vlogger-augmented Graph Neural Network Model for Micro-video Recommendation",
    "abstract": "Existing micro-video recommendation models exploit the interactions between users and micro-videos and/or multi-modal information of micro-videos to predict the next micro-video a user will watch, ignoring the information related to vloggers, i.e., the producers of micro-videos. However, in micro-video scenarios, vloggers play a significant role in user-video interactions, since vloggers generally focus on specific topics and users tend to follow the vloggers they are interested in. Therefore, in the paper, we propose a vlogger-augmented graph neural network model VA-GNN, which takes the effect of vloggers into consideration. Specifically, we construct a tripartite graph with users, micro-videos, and vloggers as nodes, capturing user preferences from different views, i.e., the video-view and the vlogger-view. Moreover, we conduct cross-view contrastive learning to keep the consistency between node embeddings from the two different views. Besides, when predicting the next user-video interaction, we adaptively combine the user preferences for a video itself and its vlogger. We conduct extensive experiments on two real-world datasets. The experimental results show that VA-GNN outperforms multiple existing GNN-based recommendation models.",
    "authors": [
      "Weijiang Lai",
      "Beihong Jin",
      "Beibei Li",
      "Yiyuan Zheng",
      "Rui Zhao"
    ],
    "url": "http://arxiv.org/abs/2405.18260v1",
    "timestamp": 1716909209,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5cedcba5-cb18-4558-bbd3-03e5fd94fea9": {
    "pk": "5cedcba5-cb18-4558-bbd3-03e5fd94fea9",
    "title": "ForecastGrapher: Redefining Multivariate Time Series Forecasting with Graph Neural Networks",
    "abstract": "The challenge of effectively learning inter-series correlations for multivariate time series forecasting remains a substantial and unresolved problem. Traditional deep learning models, which are largely dependent on the Transformer paradigm for modeling long sequences, often fail to integrate information from multiple time series into a coherent and universally applicable model. To bridge this gap, our paper presents ForecastGrapher, a framework reconceptualizes multivariate time series forecasting as a node regression task, providing a unique avenue for capturing the intricate temporal dynamics and inter-series correlations. Our approach is underpinned by three pivotal steps: firstly, generating custom node embeddings to reflect the temporal variations within each series; secondly, constructing an adaptive adjacency matrix to encode the inter-series correlations; and thirdly, augmenting the GNNs' expressive power by diversifying the node feature distribution. To enhance this expressive power, we introduce the Group Feature Convolution GNN (GFC-GNN). This model employs a learnable scaler to segment node features into multiple groups and applies one-dimensional convolutions with different kernel lengths to each group prior to the aggregation phase. Consequently, the GFC-GNN method enriches the diversity of node feature distribution in a fully end-to-end fashion. Through extensive experiments and ablation studies, we show that ForecastGrapher surpasses strong baselines and leading published techniques in the domain of multivariate time series forecasting.",
    "authors": [
      "Wanlin Cai",
      "Kun Wang",
      "Hao Wu",
      "Xiaoxu Chen",
      "Yuankai Wu"
    ],
    "url": "http://arxiv.org/abs/2405.18036v1",
    "timestamp": 1716892820,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8dc56196-9f5a-4831-a6c0-7a57c4d90831": {
    "pk": "8dc56196-9f5a-4831-a6c0-7a57c4d90831",
    "title": "Revisiting the Message Passing in Heterophilous Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors. However, in many real-world graphs, connected nodes may display contrasting behaviors, termed as heterophilous patterns, which has attracted increased interest in heterophilous GNNs (HTGNNs). Although the message-passing mechanism seems unsuitable for heterophilous graphs due to the propagation of class-irrelevant information, it is still widely used in many existing HTGNNs and consistently achieves notable success. This raises the question: why does message passing remain effective on heterophilous graphs? To answer this question, in this paper, we revisit the message-passing mechanisms in heterophilous graph neural networks and reformulate them into a unified heterophilious message-passing (HTMP) mechanism. Based on HTMP and empirical analysis, we reveal that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the compatibility matrix among classes. Moreover, we argue that the full potential of the compatibility matrix is not completely achieved due to the existence of incomplete and noisy semantic neighborhoods in real-world heterophilous graphs. To bridge this gap, we introduce a new approach named CMGNN, which operates within the HTMP mechanism to explicitly leverage and improve the compatibility matrix. A thorough evaluation involving 10 benchmark datasets and comparative analysis against 13 well-established baselines highlights the superior performance of the HTMP mechanism and CMGNN method.",
    "authors": [
      "Zhuonan Zheng",
      "Yuanchen Bei",
      "Sheng Zhou",
      "Yao Ma",
      "Ming Gu",
      "HongJia XU",
      "Chengyu Lai",
      "Jiawei Chen",
      "Jiajun Bu"
    ],
    "url": "http://arxiv.org/abs/2405.17768v1",
    "timestamp": 1716864473,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "46a68f16-012c-4fbd-839b-0f312b392eae": {
    "pk": "46a68f16-012c-4fbd-839b-0f312b392eae",
    "title": "Spectral Greedy Coresets for Graph Neural Networks",
    "abstract": "The ubiquity of large-scale graphs in node-classification tasks significantly hinders the real-world applications of Graph Neural Networks (GNNs). Node sampling, graph coarsening, and dataset condensation are effective strategies for enhancing data efficiency. However, owing to the interdependence of graph nodes, coreset selection, which selects subsets of the data examples, has not been successfully applied to speed up GNN training on large graphs, warranting special treatment. This paper studies graph coresets for GNNs and avoids the interdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs around a node) based on their spectral embeddings. We decompose the coreset selection problem for GNNs into two phases: a coarse selection of widely spread ego graphs and a refined selection to diversify their topologies. We design a greedy algorithm that approximately optimizes both objectives. Our spectral greedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates the need for model pre-training, and applies to low-homophily graphs. Extensive experiments on ten datasets demonstrate that SGGC outperforms other coreset methods by a wide margin, generalizes well across GNN architectures, and is much faster than graph condensation.",
    "authors": [
      "Mucong Ding",
      "Yinhan He",
      "Jundong Li",
      "Furong Huang"
    ],
    "url": "http://arxiv.org/abs/2405.17404v1",
    "timestamp": 1716832332,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a4853697-5abc-4a20-beda-09d801549052": {
    "pk": "a4853697-5abc-4a20-beda-09d801549052",
    "title": "Survey of Graph Neural Network for Internet of Things and NextG Networks",
    "abstract": "The exponential increase in Internet of Things (IoT) devices coupled with 6G pushing towards higher data rates and connected devices has sparked a surge in data. Consequently, harnessing the full potential of data-driven machine learning has become one of the important thrusts. In addition to the advancement in wireless technology, it is important to efficiently use the resources available and meet the users' requirements. Graph Neural Networks (GNNs) have emerged as a promising paradigm for effectively modeling and extracting insights which inherently exhibit complex network structures due to its high performance and accuracy, scalability, adaptability, and resource efficiency. There is a lack of a comprehensive survey that focuses on the applications and advances GNN has made in the context of IoT and Next Generation (NextG) networks. To bridge that gap, this survey starts by providing a detailed description of GNN's terminologies, architecture, and the different types of GNNs. Then we provide a comprehensive survey of the advancements in applying GNNs for IoT from the perspective of data fusion and intrusion detection. Thereafter, we survey the impact GNN has made in improving spectrum awareness. Next, we provide a detailed account of how GNN has been leveraged for networking and tactical systems. Through this survey, we aim to provide a comprehensive resource for researchers to learn more about GNN in the context of wireless networks, and understand its state-of-the-art use cases while contrasting to other machine learning approaches. Finally, we also discussed the challenges and wide range of future research directions to further motivate the use of GNN for IoT and NextG Networks.",
    "authors": [
      "Sabarish Krishna Moorthy",
      "Jithin Jagannath"
    ],
    "url": "http://arxiv.org/abs/2405.17309v1",
    "timestamp": 1716826249,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "29a9dbf2-8c9d-4f54-85c6-314dc2f33bc2": {
    "pk": "29a9dbf2-8c9d-4f54-85c6-314dc2f33bc2",
    "title": "Graph Neural Networks on Quantum Computers",
    "abstract": "Graph Neural Networks (GNNs) are powerful machine learning models that excel at analyzing structured data represented as graphs, demonstrating remarkable performance in applications like social network analysis and recommendation systems. However, classical GNNs face scalability challenges when dealing with large-scale graphs. This paper proposes frameworks for implementing GNNs on quantum computers to potentially address the challenges. We devise quantum algorithms corresponding to the three fundamental types of classical GNNs: Graph Convolutional Networks, Graph Attention Networks, and Message-Passing GNNs. A complexity analysis of our quantum implementation of the Simplified Graph Convolutional (SGC) Network shows potential quantum advantages over its classical counterpart, with significant improvements in time and space complexities. Our complexities can have trade-offs between the two: when optimizing for minimal circuit depth, our quantum SGC achieves logarithmic time complexity in the input sizes (albeit at the cost of linear space complexity). When optimizing for minimal qubit usage, the quantum SGC exhibits space complexity logarithmic in the input sizes, offering an exponential reduction compared to classical SGCs, while still maintaining better time complexity. These results suggest our Quantum GNN frameworks could efficiently process large-scale graphs. This work paves the way for implementing more advanced Graph Neural Network models on quantum computers, opening new possibilities in quantum machine learning for analyzing graph-structured data.",
    "authors": [
      "Yidong Liao",
      "Xiao-Ming Zhang",
      "Chris Ferrie"
    ],
    "url": "http://arxiv.org/abs/2405.17060v1",
    "timestamp": 1716809468,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0bfd3cf7-512d-41b4-b5f1-30cd02be65cc": {
    "pk": "0bfd3cf7-512d-41b4-b5f1-30cd02be65cc",
    "title": "FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks",
    "abstract": "Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectrum. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.",
    "authors": [
      "Renqiang Luo",
      "Huafei Huang",
      "Shuo Yu",
      "Zhuoyang Han",
      "Estrid He",
      "Xiuzhen Zhang",
      "Feng Xia"
    ],
    "url": "http://arxiv.org/abs/2405.17034v1",
    "timestamp": 1716806421,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "046ef318-7379-41cc-a655-da4b78415d1e": {
    "pk": "046ef318-7379-41cc-a655-da4b78415d1e",
    "title": "Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning",
    "abstract": "Spiking neural networks (SNNs) are investigated as biologically inspired models of neural computation, distinguished by their computational capability and energy efficiency due to precise spiking times and sparse spikes with event-driven computation. A significant question is how SNNs can emulate human-like graph-based reasoning of concepts and relations, especially leveraging the temporal domain optimally. This paper reveals that SNNs, when amalgamated with synaptic delay and temporal coding, are proficient in executing (knowledge) graph reasoning. It is elucidated that spiking time can function as an additional dimension to encode relation properties via a neural-generalized path formulation. Empirical results highlight the efficacy of temporal delay in relation processing and showcase exemplary performance in diverse graph reasoning tasks. The spiking model is theoretically estimated to achieve $20\\times$ energy savings compared to non-spiking counterparts, deepening insights into the capabilities and potential of biologically inspired SNNs for efficient reasoning. The code is available at https://github.com/pkuxmq/GRSNN.",
    "authors": [
      "Mingqing Xiao",
      "Yixin Zhu",
      "Di He",
      "Zhouchen Lin"
    ],
    "url": "http://arxiv.org/abs/2405.16851v1",
    "timestamp": 1716789210,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.NE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a95339d7-39c9-477f-ad3b-c406df5fdf01": {
    "pk": "a95339d7-39c9-477f-ad3b-c406df5fdf01",
    "title": "Graph neural networks with configuration cross-attention for tensor compilers",
    "abstract": "With the recent popularity of neural networks comes the need for efficient serving of inference workloads. A neural network inference workload can be represented as a computational graph with nodes as operators transforming multidimensional tensors. The tensors can be transposed and/or tiled in a combinatorially large number of ways, some configurations leading to accelerated inference. We propose TGraph, a neural graph architecture that allows screening for fast configurations of the target computational graph, thus representing an artificial intelligence (AI) tensor compiler in contrast to the traditional heuristics-based compilers. The proposed solution improves mean Kendall's $\\tau$ across layout collections of TpuGraphs from 29.8% of the reliable baseline to 67.4% of TGraph. We estimate the potential CO$_2$ emission reduction associated with our work to be equivalent to over 50% of the total household emissions in the areas hosting AI-oriented data centers.",
    "authors": [
      "Dmitrii Khizbullin",
      "Eduardo Rocha de Andrade",
      "Thanh Hau Nguyen",
      "Matheus Pedroza Ferreira",
      "David R. Pugh"
    ],
    "url": "http://arxiv.org/abs/2405.16623v1",
    "timestamp": 1716741559,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "09c24e48-1e1d-4baa-aa3c-16df41774cdb": {
    "pk": "09c24e48-1e1d-4baa-aa3c-16df41774cdb",
    "title": "Differentiable Cluster Graph Neural Network",
    "abstract": "Graph Neural Networks often struggle with long-range information propagation and in the presence of heterophilous neighborhoods. We address both challenges with a unified framework that incorporates a clustering inductive bias into the message passing mechanism, using additional cluster-nodes. Central to our approach is the formulation of an optimal transport based implicit clustering objective function. However, the algorithm for solving the implicit objective function needs to be differentiable to enable end-to-end learning of the GNN. To facilitate this, we adopt an entropy regularized objective function and propose an iterative optimization process, alternating between solving for the cluster assignments and updating the node/cluster-node embeddings. Notably, our derived closed-form optimization steps are themselves simple yet elegant message passing steps operating seamlessly on a bipartite graph of nodes and cluster-nodes. Our clustering-based approach can effectively capture both local and global information, demonstrated by extensive experiments on both heterophilous and homophilous datasets.",
    "authors": [
      "Yanfei Dong",
      "Mohammed Haroon Dupty",
      "Lambert Deng",
      "Zhuanghua Liu",
      "Yong Liang Goh",
      "Wee Sun Lee"
    ],
    "url": "http://arxiv.org/abs/2405.16185v1",
    "timestamp": 1716636219,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "df7baf2e-a05b-4c44-aa06-6336f8c14a72": {
    "pk": "df7baf2e-a05b-4c44-aa06-6336f8c14a72",
    "title": "Learning from Linear Algebra: A Graph Neural Network Approach to Preconditioner Design for Conjugate Gradient Solvers",
    "abstract": "Large linear systems are ubiquitous in modern computational science. The main recipe for solving them is iterative solvers with well-designed preconditioners. Deep learning models may be used to precondition residuals during iteration of such linear solvers as the conjugate gradient (CG) method. Neural network models require an enormous number of parameters to approximate well in this setup. Another approach is to take advantage of small graph neural networks (GNNs) to construct preconditioners of the predefined sparsity pattern. In our work, we recall well-established preconditioners from linear algebra and use them as a starting point for training the GNN. Numerical experiments demonstrate that our approach outperforms both classical methods and neural network-based preconditioning. We also provide a heuristic justification for the loss function used and validate our approach on complex datasets.",
    "authors": [
      "Vladislav Trifonov",
      "Alexander Rudikov",
      "Oleg Iliev",
      "Ivan Oseledets",
      "Ekaterina Muravleva"
    ],
    "url": "http://arxiv.org/abs/2405.15557v1",
    "timestamp": 1716558270,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "01a9b17e-08ba-4983-bab1-a72373c90706": {
    "pk": "01a9b17e-08ba-4983-bab1-a72373c90706",
    "title": "Bundle Neural Networks for message diffusion on graphs",
    "abstract": "The dominant paradigm for learning on graph-structured data is message passing. Despite being a strong inductive bias, the local message passing mechanism suffers from pathological issues such as over-smoothing, over-squashing, and limited node-level expressivity. To address these limitations we propose Bundle Neural Networks (BuNN), a new type of GNN that operates via message diffusion over flat vector bundles - structures analogous to connections on Riemannian manifolds that augment the graph by assigning to each node a vector space and an orthogonal map. A BuNN layer evolves the features according to a diffusion-type partial differential equation. When discretized, BuNNs are a special case of Sheaf Neural Networks (SNNs), a recently proposed MPNN capable of mitigating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate on larger scales of the graph and, therefore, to mitigate over-squashing. Finally, we prove that BuNN can approximate any feature transformation over nodes on any (potentially infinite) family of graphs given injective positional encodings, resulting in universal node-level expressivity. We support our theory via synthetic experiments and showcase the strong empirical performance of BuNNs over a range of real-world tasks, achieving state-of-the-art results on several standard benchmarks in transductive and inductive settings.",
    "authors": [
      "Jacob Bamberger",
      "Federico Barbero",
      "Xiaowen Dong",
      "Michael Bronstein"
    ],
    "url": "http://arxiv.org/abs/2405.15540v1",
    "timestamp": 1716557328,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "17db010e-0a23-46a0-93ee-9c14e6a10129": {
    "pk": "17db010e-0a23-46a0-93ee-9c14e6a10129",
    "title": "DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback",
    "abstract": "The graph-based recommendation has achieved great success in recent years. However, most existing graph-based recommendations focus on capturing user preference based on positive edges/feedback, while ignoring negative edges/feedback (e.g., dislike, low rating) that widely exist in real-world recommender systems. How to utilize negative feedback in graph-based recommendations still remains underexplored. In this study, we first conducted a comprehensive experimental analysis and found that (1) existing graph neural networks are not well-suited for modeling negative feedback, which acts as a high-frequency signal in a user-item graph. (2) The graph-based recommendation suffers from the representation degeneration problem. Based on the two observations, we propose a novel model that models positive and negative feedback from a frequency filter perspective called Dual-frequency Graph Neural Network for Sign-aware Recommendation (DFGNN). Specifically, in DFGNN, the designed dual-frequency graph filter (DGF) captures both low-frequency and high-frequency signals that contain positive and negative feedback. Furthermore, the proposed signed graph regularization is applied to maintain the user/item embedding uniform in the embedding space to alleviate the representation degeneration problem. Additionally, we conduct extensive experiments on real-world datasets and demonstrate the effectiveness of the proposed model. Codes of our model will be released upon acceptance.",
    "authors": [
      "Yiqing Wu",
      "Ruobing Xie",
      "Zhao Zhang",
      "Xu Zhang",
      "Fuzhen Zhuang",
      "Leyu Lin",
      "Zhanhui Kang",
      "Yongjun Xu"
    ],
    "url": "http://arxiv.org/abs/2405.15280v1",
    "timestamp": 1716534461,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "613c9957-148f-465a-8777-b8c5e043c85d": {
    "pk": "613c9957-148f-465a-8777-b8c5e043c85d",
    "title": "AGS-GNN: Attribute-guided Sampling for Graph Neural Networks",
    "abstract": "We propose AGS-GNN, a novel attribute-guided sampling algorithm for Graph Neural Networks (GNNs) that exploits node features and connectivity structure of a graph while simultaneously adapting for both homophily and heterophily in graphs. (In homophilic graphs vertices of the same class are more likely to be connected, and vertices of different classes tend to be linked in heterophilic graphs.) While GNNs have been successfully applied to homophilic graphs, their application to heterophilic graphs remains challenging. The best-performing GNNs for heterophilic graphs do not fit the sampling paradigm, suffer high computational costs, and are not inductive. We employ samplers based on feature-similarity and feature-diversity to select subsets of neighbors for a node, and adaptively capture information from homophilic and heterophilic neighborhoods using dual channels. Currently, AGS-GNN is the only algorithm that we know of that explicitly controls homophily in the sampled subgraph through similar and diverse neighborhood samples. For diverse neighborhood sampling, we employ submodularity, which was not used in this context prior to our work. The sampling distribution is pre-computed and highly parallel, achieving the desired scalability. Using an extensive dataset consisting of 35 small ($\\le$ 100K nodes) and large (>100K nodes) homophilic and heterophilic graphs, we demonstrate the superiority of AGS-GNN compare to the current approaches in the literature. AGS-GNN achieves comparable test accuracy to the best-performing heterophilic GNNs, even outperforming methods using the entire graph for node classification. AGS-GNN also converges faster compared to methods that sample neighborhoods randomly, and can be incorporated into existing GNN models that employ node or graph sampling.",
    "authors": [
      "Siddhartha Shankar Das",
      "S M Ferdous",
      "Mahantesh M Halappanavar",
      "Edoardo Serra",
      "Alex Pothen"
    ],
    "url": "http://arxiv.org/abs/2405.15218v1",
    "timestamp": 1716527746,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4482ccdc-8c3a-4bef-a330-6caa8086639e": {
    "pk": "4482ccdc-8c3a-4bef-a330-6caa8086639e",
    "title": "Message-Passing Monte Carlo: Generating low-discrepancy point sets via Graph Neural Networks",
    "abstract": "Discrepancy is a well-known measure for the irregularity of the distribution of a point set. Point sets with small discrepancy are called low-discrepancy and are known to efficiently fill the space in a uniform manner. Low-discrepancy points play a central role in many problems in science and engineering, including numerical integration, computer vision, machine perception, computer graphics, machine learning, and simulation. In this work, we present the first machine learning approach to generate a new class of low-discrepancy point sets named Message-Passing Monte Carlo (MPMC) points. Motivated by the geometric nature of generating low-discrepancy point sets, we leverage tools from Geometric Deep Learning and base our model on Graph Neural Networks. We further provide an extension of our framework to higher dimensions, which flexibly allows the generation of custom-made points that emphasize the uniformity in specific dimensions that are primarily important for the particular problem at hand. Finally, we demonstrate that our proposed model achieves state-of-the-art performance superior to previous methods by a significant margin. In fact, MPMC points are empirically shown to be either optimal or near-optimal with respect to the discrepancy for every dimension and the number of points for which the optimal discrepancy can be determined.",
    "authors": [
      "T. Konstantin Rusch",
      "Nathan Kirk",
      "Michael M. Bronstein",
      "Christiane Lemieux",
      "Daniela Rus"
    ],
    "url": "http://arxiv.org/abs/2405.15059v1",
    "timestamp": 1716499040,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "553afec7-8864-4d59-b0b2-75513a15d475": {
    "pk": "553afec7-8864-4d59-b0b2-75513a15d475",
    "title": "Analysis of Atom-level pretraining with Quantum Mechanics (QM) data for Graph Neural Networks Molecular property models",
    "abstract": "Despite the rapid and significant advancements in deep learning for Quantitative Structure-Activity Relationship (QSAR) models, the challenge of learning robust molecular representations that effectively generalize in real-world scenarios to novel compounds remains an elusive and unresolved task. This study examines how atom-level pretraining with quantum mechanics (QM) data can mitigate violations of assumptions regarding the distributional similarity between training and test data and therefore improve performance and generalization in downstream tasks. In the public dataset Therapeutics Data Commons (TDC), we show how pretraining on atom-level QM improves performance overall and makes the activation of the features distributes more Gaussian-like which results in a representation that is more robust to distribution shifts. To the best of our knowledge, this is the first time that hidden state molecular representations are analyzed to compare the effects of molecule-level and atom-level pretraining on QM data.",
    "authors": [
      "Jose Arjona-Medina",
      "Ramil Nugmanov"
    ],
    "url": "http://arxiv.org/abs/2405.14837v2",
    "timestamp": 1716486665,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fe29adcf-6303-4d7f-be99-7caf4badc198": {
    "pk": "fe29adcf-6303-4d7f-be99-7caf4badc198",
    "title": "Logical Characterizations of Recurrent Graph Neural Networks with Reals and Floats",
    "abstract": "In pioneering work from 2019, Barcel\\'o and coauthors identified logics that precisely match the expressive power of constant iteration-depth graph neural networks (GNNs) relative to properties definable in first-order logic. In this article, we give exact logical characterizations of recurrent GNNs in two scenarios: (1) in the setting with floating-point numbers and (2) with reals. For floats, the formalism matching recurrent GNNs is a rule-based modal logic with counting, while for reals we use a suitable infinitary modal logic, also with counting. These results give exact matches between logics and GNNs in the recurrent setting without relativising to a background logic in either case, but using some natural assumptions about floating-point arithmetic. Applying our characterizations, we also prove that, relative to graph properties definable in monadic second-order logic (MSO), our infinitary and rule-based logics are equally expressive. This implies that recurrent GNNs with reals and floats have the same expressive power over MSO-definable properties and shows that, for such properties, also recurrent GNNs with reals are characterized by a (finitary!) rule-based modal logic. In the general case, in contrast, the expressive power with floats is weaker than with reals. In addition to logic-oriented results, we also characterize recurrent GNNs, with both reals and floats, via distributed automata, drawing links to distributed computing models.",
    "authors": [
      "Veeti Ahvonen",
      "Damian Heiman",
      "Antti Kuusisto",
      "Carsten Lutz"
    ],
    "url": "http://arxiv.org/abs/2405.14606v1",
    "timestamp": 1716473961,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5da6053c-ad92-4a2d-96cd-d256d78fc7f3": {
    "pk": "5da6053c-ad92-4a2d-96cd-d256d78fc7f3",
    "title": "Gradient Transformation: Towards Efficient and Model-Agnostic Unlearning for Dynamic Graph Neural Networks",
    "abstract": "Graph unlearning has emerged as an essential tool for safeguarding user privacy and mitigating the negative impacts of undesirable data. Meanwhile, the advent of dynamic graph neural networks (DGNNs) marks a significant advancement due to their superior capability in learning from dynamic graphs, which encapsulate spatial-temporal variations in diverse real-world applications (e.g., traffic forecasting). With the increasing prevalence of DGNNs, it becomes imperative to investigate the implementation of dynamic graph unlearning. However, current graph unlearning methodologies are designed for GNNs operating on static graphs and exhibit limitations including their serving in a pre-processing manner and impractical resource demands. Furthermore, the adaptation of these methods to DGNNs presents non-trivial challenges, owing to the distinctive nature of dynamic graphs. To this end, we propose an effective, efficient, model-agnostic, and post-processing method to implement DGNN unlearning. Specifically, we first define the unlearning requests and formulate dynamic graph unlearning in the context of continuous-time dynamic graphs. After conducting a role analysis on the unlearning data, the remaining data, and the target DGNN model, we propose a method called Gradient Transformation and a loss function to map the unlearning request to the desired parameter update. Evaluations on six real-world datasets and state-of-the-art DGNN backbones demonstrate its effectiveness (e.g., limited performance drop even obvious improvement) and efficiency (e.g., at most 7.23$\\times$ speed-up) outperformance, and potential advantages in handling future unlearning requests (e.g., at most 32.59$\\times$ speed-up).",
    "authors": [
      "He Zhang",
      "Bang Wu",
      "Xiangwen Yang",
      "Xingliang Yuan",
      "Chengqi Zhang",
      "Shirui Pan"
    ],
    "url": "http://arxiv.org/abs/2405.14407v1",
    "timestamp": 1716459978,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4ea17154-c7c6-40b7-bbf5-ee391e651480": {
    "pk": "4ea17154-c7c6-40b7-bbf5-ee391e651480",
    "title": "Explaining Graph Neural Networks via Structure-aware Interaction Index",
    "abstract": "The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.",
    "authors": [
      "Ngoc Bui",
      "Hieu Trung Nguyen",
      "Viet Anh Nguyen",
      "Rex Ying"
    ],
    "url": "http://arxiv.org/abs/2405.14352v1",
    "timestamp": 1716456273,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "630a33b8-ee64-4891-92a2-6c2ef5adc387": {
    "pk": "630a33b8-ee64-4891-92a2-6c2ef5adc387",
    "title": "Similarity-Navigated Conformal Prediction for Graph Neural Networks",
    "abstract": "Graph Neural Networks have achieved remarkable accuracy in semi-supervised node classification tasks. However, these results lack reliable uncertainty estimates. Conformal prediction methods provide a theoretical guarantee for node classification tasks, ensuring that the conformal prediction set contains the ground-truth label with a desired probability (e.g., 95%). In this paper, we empirically show that for each node, aggregating the non-conformity scores of nodes with the same label can improve the efficiency of conformal prediction sets. This observation motivates us to propose a novel algorithm named Similarity-Navigated Adaptive Prediction Sets (SNAPS), which aggregates the non-conformity scores based on feature similarity and structural neighborhood. The key idea behind SNAPS is that nodes with high feature similarity or direct connections tend to have the same label. By incorporating adaptive similar nodes information, SNAPS can generate compact prediction sets and increase the singleton hit ratio (correct prediction sets of size one). Moreover, we theoretically provide a finite-sample coverage guarantee of SNAPS. Extensive experiments demonstrate the superiority of SNAPS, improving the efficiency of prediction sets and singleton hit ratio while maintaining valid coverage.",
    "authors": [
      "Jianqing Song",
      "Jianguo Huang",
      "Wenyu Jiang",
      "Baoming Zhang",
      "Shuangjie Li",
      "Chongjun Wang"
    ],
    "url": "http://arxiv.org/abs/2405.14303v1",
    "timestamp": 1716452602,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c0eda4bd-edcf-4ec0-92a9-5e211ceefb27": {
    "pk": "c0eda4bd-edcf-4ec0-92a9-5e211ceefb27",
    "title": "Graphcode: Learning from multiparameter persistent homology using graph neural networks",
    "abstract": "We introduce graphcodes, a novel multi-scale summary of the topological properties of a dataset that is based on the well-established theory of persistent homology. Graphcodes handle datasets that are filtered along two real-valued scale parameters. Such multi-parameter topological summaries are usually based on complicated theoretical foundations and difficult to compute; in contrast, graphcodes yield an informative and interpretable summary and can be computed as efficient as one-parameter summaries. Moreover, a graphcode is simply an embedded graph and can therefore be readily integrated in machine learning pipelines using graph neural networks. We describe such a pipeline and demonstrate that graphcodes achieve better classification accuracy than state-of-the-art approaches on various datasets.",
    "authors": [
      "Michael Kerber",
      "Florian Russold"
    ],
    "url": "http://arxiv.org/abs/2405.14302v1",
    "timestamp": 1716452520,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.AT",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6069a7d5-050d-4050-b3ef-c0d879ac4c8d": {
    "pk": "6069a7d5-050d-4050-b3ef-c0d879ac4c8d",
    "title": "Investigation of Customized Medical Decision Algorithms Utilizing Graph Neural Networks",
    "abstract": "Aiming at the limitations of traditional medical decision system in processing large-scale heterogeneous medical data and realizing highly personalized recommendation, this paper introduces a personalized medical decision algorithm utilizing graph neural network (GNN). This research innovatively integrates graph neural network technology into the medical and health field, aiming to build a high-precision representation model of patient health status by mining the complex association between patients' clinical characteristics, genetic information, living habits. In this study, medical data is preprocessed to transform it into a graph structure, where nodes represent different data entities (such as patients, diseases, genes, etc.) and edges represent interactions or relationships between entities. The core of the algorithm is to design a novel multi-scale fusion mechanism, combining the historical medical records, physiological indicators and genetic characteristics of patients, to dynamically adjust the attention allocation strategy of the graph neural network, so as to achieve highly customized analysis of individual cases. In the experimental part, this study selected several publicly available medical data sets for validation, and the results showed that compared with traditional machine learning methods and a single graph neural network model, the proposed personalized medical decision algorithm showed significantly superior performance in terms of disease prediction accuracy, treatment effect evaluation and patient risk stratification.",
    "authors": [
      "Yafeng Yan",
      "Shuyao He",
      "Zhou Yu",
      "Jiajie Yuan",
      "Ziang Liu",
      "Yan Chen"
    ],
    "url": "http://arxiv.org/abs/2405.17460v1",
    "timestamp": 1716438641,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7162a650-0fd0-4b6f-99a4-3af1b1a9c55f": {
    "pk": "7162a650-0fd0-4b6f-99a4-3af1b1a9c55f",
    "title": "LOGIN: A Large Language Model Consulted Graph Neural Network Training Framework",
    "abstract": "Recent prevailing works on graph machine learning typically follow a similar methodology that involves designing advanced variants of graph neural networks (GNNs) to maintain the superior performance of GNNs on different graphs. In this paper, we aim to streamline the GNN design process and leverage the advantages of Large Language Models (LLMs) to improve the performance of GNNs on downstream tasks. We formulate a new paradigm, coined \"LLMs-as-Consultants,\" which integrates LLMs with GNNs in an interactive manner. A framework named LOGIN (LLM Consulted GNN training) is instantiated, empowering the interactive utilization of LLMs within the GNN training process. First, we attentively craft concise prompts for spotted nodes, carrying comprehensive semantic and topological information, and serving as input to LLMs. Second, we refine GNNs by devising a complementary coping mechanism that utilizes the responses from LLMs, depending on their correctness. We empirically evaluate the effectiveness of LOGIN on node classification tasks across both homophilic and heterophilic graphs. The results illustrate that even basic GNN architectures, when employed within the proposed LLMs-as-Consultants paradigm, can achieve comparable performance to advanced GNNs with intricate designs. Our codes are available at https://github.com/QiaoYRan/LOGIN.",
    "authors": [
      "Yiran Qiao",
      "Xiang Ao",
      "Yang Liu",
      "Jiarong Xu",
      "Xiaoqian Sun",
      "Qing He"
    ],
    "url": "http://arxiv.org/abs/2405.13902v2",
    "timestamp": 1716401840,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7e4d1567-14de-428b-8508-6fd064cfad72": {
    "pk": "7e4d1567-14de-428b-8508-6fd064cfad72",
    "title": "Bond Graphs for multi-physics informed Neural Networks for multi-variate time series",
    "abstract": "In the trend of hybrid Artificial Intelligence (AI) techniques, Physic Informed Machine Learning has seen a growing interest. It operates mainly by imposing a data, learning or inductive bias with simulation data, Partial Differential Equations or equivariance and invariance properties. While these models have shown great success on tasks involving one physical domain such as fluid dynamics, existing methods still struggle on tasks with complex multi-physical and multi-domain phenomena. To address this challenge, we propose to leverage Bond Graphs, a multi-physics modeling approach together with Graph Neural Network. We thus propose Neural Bond Graph Encoder (NBgE), a model agnostic physical-informed encoder tailored for multi-physics systems. It provides an unified framework for any multi-physics informed AI with a graph encoder readable for any deep learning model. Our experiments on two challenging multi-domain physical systems - a Direct Current Motor and the Respiratory system - demonstrate the effectiveness of our approach on a multi-variate time series forecasting task.",
    "authors": [
      "Alexis-Raja Brachet",
      "Pierre-Yves Richard",
      "C\u00e9line Hudelot"
    ],
    "url": "http://arxiv.org/abs/2405.13586v1",
    "timestamp": 1716381025,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9aa778c3-23d2-4e44-acb1-a310f4ea9159": {
    "pk": "9aa778c3-23d2-4e44-acb1-a310f4ea9159",
    "title": "Quantum Positional Encodings for Graph Neural Networks",
    "abstract": "In this work, we propose novel families of positional encodings tailored to graph neural networks obtained with quantum computers. These encodings leverage the long-range correlations inherent in quantum systems that arise from mapping the topology of a graph onto interactions between qubits in a quantum computer. Our inspiration stems from the recent advancements in quantum processing units, which offer computational capabilities beyond the reach of classical hardware. We prove that some of these quantum features are theoretically more expressive for certain graphs than the commonly used relative random walk probabilities. Empirically, we show that the performance of state-of-the-art models can be improved on standard benchmarks and large-scale datasets by computing tractable versions of quantum features. Our findings highlight the potential of leveraging quantum computing capabilities to enhance the performance of transformers in handling graph data.",
    "authors": [
      "Slimane Thabet",
      "Mehdi Djellabi",
      "Igor Sokolov",
      "Sachin Kasture",
      "Louis-Paul Henry",
      "Lo\u00efc Henriet"
    ],
    "url": "http://arxiv.org/abs/2406.06547v1",
    "timestamp": 1716314193,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ebf641f0-406c-4005-9c64-6f32ef0d14fb": {
    "pk": "ebf641f0-406c-4005-9c64-6f32ef0d14fb",
    "title": "Graph neural networks informed locally by thermodynamics",
    "abstract": "Thermodynamics-informed neural networks employ inductive biases for the enforcement of the first and second principles of thermodynamics. To construct these biases, a metriplectic evolution of the system is assumed. This provides excellent results, when compared to uninformed, black box networks. While the degree of accuracy can be increased in one or two orders of magnitude, in the case of graph networks, this requires assembling global Poisson and dissipation matrices, which breaks the local structure of such networks. In order to avoid this drawback, a local version of the metriplectic biases has been developed in this work, which avoids the aforementioned matrix assembly, thus preserving the node-by-node structure of the graph networks. We apply this framework for examples in the fields of solid and fluid mechanics. Our approach demonstrates significant computational efficiency and strong generalization capabilities, accurately making inferences on examples significantly different from those encountered during training.",
    "authors": [
      "Alicia Tierz",
      "Iciar Alfaro",
      "David Gonz\u00e1lez",
      "Francisco Chinesta",
      "El\u00edas Cueto"
    ],
    "url": "http://arxiv.org/abs/2405.13093v1",
    "timestamp": 1716296230,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "da7ba5ff-4ad5-43f3-82a7-8ccc25220293": {
    "pk": "da7ba5ff-4ad5-43f3-82a7-8ccc25220293",
    "title": "Utilizing Description Logics for Global Explanations of Heterogeneous Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) are effective for node classification in graph-structured data, but they lack explainability, especially at the global level. Current research mainly utilizes subgraphs of the input as local explanations or generates new graphs as global explanations. However, these graph-based methods are limited in their ability to explain classes with multiple sufficient explanations. To provide more expressive explanations, we propose utilizing class expressions (CEs) from the field of description logic (DL). Our approach explains heterogeneous graphs with different types of nodes using CEs in the EL description logic. To identify the best explanation among multiple candidate explanations, we employ and compare two different scoring functions: (1) For a given CE, we construct multiple graphs, have the GNN make a prediction for each graph, and aggregate the predicted scores. (2) We score the CE in terms of fidelity, i.e., we compare the predictions of the GNN to the predictions by the CE on a separate validation set. Instead of subgraph-based explanations, we offer CE-based explanations.",
    "authors": [
      "Dominik K\u00f6hler",
      "Stefan Heindorf"
    ],
    "url": "http://arxiv.org/abs/2405.12654v1",
    "timestamp": 1716286049,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4c9629d2-3d6b-427f-b2fc-e7e764118103": {
    "pk": "4c9629d2-3d6b-427f-b2fc-e7e764118103",
    "title": "Unleash Graph Neural Networks from Heavy Tuning",
    "abstract": "Graph Neural Networks (GNNs) are deep-learning architectures designed for graph-type data, where understanding relationships among individual observations is crucial. However, achieving promising GNN performance, especially on unseen data, requires comprehensive hyperparameter tuning and meticulous training. Unfortunately, these processes come with high computational costs and significant human effort. Additionally, conventional searching algorithms such as grid search may result in overfitting on validation data, diminishing generalization accuracy. To tackle these challenges, we propose a graph conditional latent diffusion framework (GNN-Diff) to generate high-performing GNNs directly by learning from checkpoints saved during a light-tuning coarse search. Our method: (1) unleashes GNN training from heavy tuning and complex search space design; (2) produces GNN parameters that outperform those obtained through comprehensive grid search; and (3) establishes higher-quality generation for GNNs compared to diffusion frameworks designed for general neural networks.",
    "authors": [
      "Lequan Lin",
      "Dai Shi",
      "Andi Han",
      "Zhiyong Wang",
      "Junbin Gao"
    ],
    "url": "http://arxiv.org/abs/2405.12521v1",
    "timestamp": 1716272627,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2268e9f9-0c79-49e2-83e7-582298fc57f7": {
    "pk": "2268e9f9-0c79-49e2-83e7-582298fc57f7",
    "title": "MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation",
    "abstract": "Graph Neural Networks (GNNs) have shown remarkable success in molecular tasks, yet their interpretability remains challenging. Traditional model-level explanation methods like XGNN and GNNInterpreter often fail to identify valid substructures like rings, leading to questionable interpretability. This limitation stems from XGNN's atom-by-atom approach and GNNInterpreter's reliance on average graph embeddings, which overlook the essential structural elements crucial for molecules. To address these gaps, we introduce an innovative \\textbf{M}otif-b\\textbf{A}sed \\textbf{G}NN \\textbf{E}xplainer (MAGE) that uses motifs as fundamental units for generating explanations. Our approach begins with extracting potential motifs through a motif decomposition technique. Then, we utilize an attention-based learning method to identify class-specific motifs. Finally, we employ a motif-based graph generator for each class to create molecular graph explanations based on these class-specific motifs. This novel method not only incorporates critical substructures into the explanations but also guarantees their validity, yielding results that are human-understandable. Our proposed method's effectiveness is demonstrated through quantitative and qualitative assessments conducted on six real-world molecular datasets.",
    "authors": [
      "Zhaoning Yu",
      "Hongyang Gao"
    ],
    "url": "http://arxiv.org/abs/2405.12519v1",
    "timestamp": 1716271944,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0b736c61-28ff-4b45-99ce-32cc100a8421": {
    "pk": "0b736c61-28ff-4b45-99ce-32cc100a8421",
    "title": "How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing",
    "abstract": "Spectral Graph Neural Networks (GNNs), alternatively known as graph filters, have gained increasing prevalence for heterophily graphs. Optimal graph filters rely on Laplacian eigendecomposition for Fourier transform. In an attempt to avert prohibitive computations, numerous polynomial filters have been proposed. However, polynomials in the majority of these filters are predefined and remain fixed across different graphs, failing to accommodate the varying degrees of heterophily. Addressing this gap, we demystify the intrinsic correlation between the spectral property of desired polynomial bases and the heterophily degrees via thorough theoretical analyses. Subsequently, we develop a novel adaptive heterophily basis wherein the basis vectors mutually form angles reflecting the heterophily degree of the graph. We integrate this heterophily basis with the homophily basis to construct a universal polynomial basis UniBasis, which devises a polynomial filter based graph neural network - UniFilter. It optimizes the convolution and propagation in GNN, thus effectively limiting over-smoothing and alleviating over-squashing. Our extensive experiments, conducted on a diverse range of real-world and synthetic datasets with varying degrees of heterophily, support the superiority of UniFilter. These results not only demonstrate the universality of UniBasis but also highlight its proficiency in graph explanation.",
    "authors": [
      "Keke Huang",
      "Yu Guang Wang",
      "Ming Li",
      "and Pietro Li\u00f2"
    ],
    "url": "http://arxiv.org/abs/2405.12474v1",
    "timestamp": 1716262125,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9d88b067-d0c8-42ee-b003-5895cb97ee2b": {
    "pk": "9d88b067-d0c8-42ee-b003-5895cb97ee2b",
    "title": "Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) are recognized as potent tools for processing real-world data organized in graph structures. Especially inductive GNNs, which enable the processing of graph-structured data without relying on predefined graph structures, are gaining importance in an increasingly wide variety of applications. As these networks demonstrate proficiency across a range of tasks, they become lucrative targets for model-stealing attacks where an adversary seeks to replicate the functionality of the targeted network. A large effort has been made to develop model-stealing attacks that focus on models trained with images and texts. However, little attention has been paid to GNNs trained on graph data. This paper introduces a novel method for unsupervised model-stealing attacks against inductive GNNs, based on graph contrasting learning and spectral graph augmentations to efficiently extract information from the target model. The proposed attack is thoroughly evaluated on six datasets. The results show that this approach demonstrates a higher level of efficiency compared to existing stealing attacks. More concretely, our attack outperforms the baseline on all benchmarks achieving higher fidelity and downstream accuracy of the stolen model while requiring fewer queries sent to the target model.",
    "authors": [
      "Marcin Podhajski",
      "Jan Dubi\u0144ski",
      "Franziska Boenisch",
      "Adam Dziedzic",
      "Agnieszka Pregowska",
      "Tomasz Michalak"
    ],
    "url": "http://arxiv.org/abs/2405.12295v2",
    "timestamp": 1716228075,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c6fd7a59-5968-43d5-8260-49dfee682b64": {
    "pk": "c6fd7a59-5968-43d5-8260-49dfee682b64",
    "title": "Conditional Shift-Robust Conformal Prediction for Graph Neural Network",
    "abstract": "Graph Neural Networks (GNNs) have emerged as potent tools for predicting outcomes in graph-structured data. Despite their efficacy, a significant drawback of GNNs lies in their limited ability to provide robust uncertainty estimates, posing challenges to their reliability in contexts where errors carry significant consequences. Moreover, GNNs typically excel in in-distribution settings, assuming that training and test data follow identical distributions a condition often unmet in real world graph data scenarios. In this article, we leverage conformal prediction, a widely recognized statistical technique for quantifying uncertainty by transforming predictive model outputs into prediction sets, to address uncertainty quantification in GNN predictions amidst conditional shift\\footnote{Representing the change in conditional probability distribution \\(P(label|input)\\) from source domain to target domain.} in graph-based semi-supervised learning (SSL). Additionally, we propose a novel loss function aimed at refining model predictions by minimizing conditional shift in latent stages. Termed Conditional Shift Robust (CondSR) conformal prediction for GNNs, our approach CondSR is model-agnostic and adaptable to various classification models. We validate the effectiveness of our method on standard graph benchmark datasets, integrating it with state-of-the-art GNNs in node classification tasks. Comprehensive evaluations demonstrate that our approach consistently achieves any predefined target marginal coverage, enhances the accuracy of state of the art GNN models by up to 12\\% under conditional shift, and reduces the prediction set size by up to 48\\%. The code implementation is publicly available for further exploration and experimentation.",
    "authors": [
      "S. Akansha"
    ],
    "url": "http://arxiv.org/abs/2405.11968v2",
    "timestamp": 1716205651,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b808ade3-f970-4e69-8aa7-9e7dddcb1aba": {
    "pk": "b808ade3-f970-4e69-8aa7-9e7dddcb1aba",
    "title": "LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering",
    "abstract": "Graph clustering is a fundamental problem in machine learning. Deep learning methods achieve the state-of-the-art results in recent years, but they still cannot work without predefined cluster numbers. Such limitation motivates us to pose a more challenging problem of graph clustering with unknown cluster number. We propose to address this problem from a fresh perspective of graph information theory (i.e., structural information). In the literature, structural information has not yet been introduced to deep clustering, and its classic definition falls short of discrete formulation and modeling node features. In this work, we first formulate a differentiable structural information (DSI) in the continuous realm, accompanied by several theoretical results. By minimizing DSI, we construct the optimal partitioning tree where densely connected nodes in the graph tend to have the same assignment, revealing the cluster structure. DSI is also theoretically presented as a new graph clustering objective, not requiring the predefined cluster number. Furthermore, we design a neural LSEnet in the Lorentz model of hyperbolic space, where we integrate node features to structural information via manifold-valued graph convolution. Extensive empirical results on real graphs show the superiority of our approach.",
    "authors": [
      "Li Sun",
      "Zhenhao Huang",
      "Hao Peng",
      "Yujie Wang",
      "Chunyang Liu",
      "Philip S. Yu"
    ],
    "url": "http://arxiv.org/abs/2405.11801v1",
    "timestamp": 1716184001,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "49acf336-0449-4586-8215-7bf980e1165b": {
    "pk": "49acf336-0449-4586-8215-7bf980e1165b",
    "title": "CTGNN: Crystal Transformer Graph Neural Network for Crystal Material Property Prediction",
    "abstract": "The combination of deep learning algorithm and materials science has made significant progress in predicting novel materials and understanding various behaviours of materials. Here, we introduced a new model called as the Crystal Transformer Graph Neural Network (CTGNN), which combines the advantages of Transformer model and graph neural networks to address the complexity of structure-properties relation of material data. Compared to the state-of-the-art models, CTGNN incorporates the graph network structure for capturing local atomic interactions and the dual-Transformer structures to model intra-crystal and inter-atomic relationships comprehensively. The benchmark carried on by the proposed CTGNN indicates that CTGNN significantly outperforms existing models like CGCNN and MEGNET in the prediction of formation energy and bandgap properties. Our work highlights the potential of CTGNN to enhance the performance of properties prediction and accelerates the discovery of new materials, particularly for perovskite materials.",
    "authors": [
      "Zijian Du",
      "Luozhijie Jin",
      "Le Shu",
      "Yan Cen",
      "Yuanfeng Xu",
      "Yongfeng Mei",
      "Hao Zhang"
    ],
    "url": "http://arxiv.org/abs/2405.11502v1",
    "timestamp": 1716112806,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cond-mat.mtrl-sci",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "314f90f3-d4a3-46c6-81af-f6fa60562181": {
    "pk": "314f90f3-d4a3-46c6-81af-f6fa60562181",
    "title": "Detecting Complex Multi-step Attacks with Explainable Graph Neural Network",
    "abstract": "Complex multi-step attacks have caused significant damage to numerous critical infrastructures. To detect such attacks, graph neural network based methods have shown promising results by modeling the system's events as a graph. However, existing methods still face several challenges when deployed in practice. First, there is a lack of sufficient real attack data especially considering the large volume of normal data. Second, the modeling of event graphs is challenging due to their dynamic and heterogeneous nature. Third, the lack of explanation in learning models undermines the trustworthiness of such methods in production environments. To address the above challenges, in this paper, we propose an attack detection method, Trace2Vec. The approach first designs an erosion function to augment rare attack samples, and integrates them into the event graphs. Next, it models the event graphs via a continuous-time dynamic heterogeneous graph neural network. Finally, it employs the Monte Carlo tree search algorithm to identify events with greater contributions to the attack, thus enhancing the explainability of the detection result. We have implemented a prototype for Trace2Vec, and the experimental evaluations demonstrate its superior detection and explanation performance compared to existing methods.",
    "authors": [
      "Wei Liu",
      "Peng Gao",
      "Haotian Zhang",
      "Ke Li",
      "Weiyong Yang",
      "Xingshen Wei",
      "Shuji Wu"
    ],
    "url": "http://arxiv.org/abs/2405.11335v1",
    "timestamp": 1716050841,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5fdbc5f0-0859-4d55-8269-70fc92f2f33e": {
    "pk": "5fdbc5f0-0859-4d55-8269-70fc92f2f33e",
    "title": "GraSS: Combining Graph Neural Networks with Expert Knowledge for SAT Solver Selection",
    "abstract": "Boolean satisfiability (SAT) problems are routinely solved by SAT solvers in real-life applications, yet solving time can vary drastically between solvers for the same instance. This has motivated research into machine learning models that can predict, for a given SAT instance, which solver to select among several options. Existing SAT solver selection methods all rely on some hand-picked instance features, which are costly to compute and ignore the structural information in SAT graphs. In this paper we present GraSS, a novel approach for automatic SAT solver selection based on tripartite graph representations of instances and a heterogeneous graph neural network (GNN) model. While GNNs have been previously adopted in other SAT-related tasks, they do not incorporate any domain-specific knowledge and ignore the runtime variation introduced by different clause orders. We enrich the graph representation with domain-specific decisions, such as novel node feature design, positional encodings for clauses in the graph, a GNN architecture tailored to our tripartite graphs and a runtime-sensitive loss function. Through extensive experiments, we demonstrate that this combination of raw representations and domain-specific choices leads to improvements in runtime for a pool of seven state-of-the-art solvers on both an industrial circuit design benchmark, and on instances from the 20-year Anniversary Track of the 2022 SAT Competition.",
    "authors": [
      "Zhanguang Zhang",
      "Didier Chetelat",
      "Joseph Cotnareanu",
      "Amur Ghose",
      "Wenyi Xiao",
      "Hui-Ling Zhen",
      "Yingxue Zhang",
      "Jianye Hao",
      "Mark Coates",
      "Mingxuan Yuan"
    ],
    "url": "http://arxiv.org/abs/2405.11024v1",
    "timestamp": 1715968850,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "37dc3ff0-556f-419f-8d23-686944881674": {
    "pk": "37dc3ff0-556f-419f-8d23-686944881674",
    "title": "Harnessing Collective Structure Knowledge in Data Augmentation for Graph Neural Networks",
    "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art performance in graph representation learning. Message passing neural networks, which learn representations through recursively aggregating information from each node and its neighbors, are among the most commonly-used GNNs. However, a wealth of structural information of individual nodes and full graphs is often ignored in such process, which restricts the expressive power of GNNs. Various graph data augmentation methods that enable the message passing with richer structure knowledge have been introduced as one main way to tackle this issue, but they are often focused on individual structure features and difficult to scale up with more structure features. In this work we propose a novel approach, namely collective structure knowledge-augmented graph neural network (CoS-GNN), in which a new message passing method is introduced to allow GNNs to harness a diverse set of node- and graph-level structure features, together with original node features/attributes, in augmented graphs. In doing so, our approach largely improves the structural knowledge modeling of GNNs in both node and graph levels, resulting in substantially improved graph representations. This is justified by extensive empirical results where CoS-GNN outperforms state-of-the-art models in various graph-level learning tasks, including graph classification, anomaly detection, and out-of-distribution generalization.",
    "authors": [
      "Rongrong Ma",
      "Guansong Pang",
      "Ling Chen"
    ],
    "url": "http://arxiv.org/abs/2405.10633v1",
    "timestamp": 1715935800,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bb61a9cb-c135-427d-b6c7-45ba03958d57": {
    "pk": "bb61a9cb-c135-427d-b6c7-45ba03958d57",
    "title": "Multi-Evidence based Fact Verification via A Confidential Graph Neural Network",
    "abstract": "Fact verification tasks aim to identify the integrity of textual contents according to the truthful corpus. Existing fact verification models usually build a fully connected reasoning graph, which regards claim-evidence pairs as nodes and connects them with edges. They employ the graph to propagate the semantics of the nodes. Nevertheless, the noisy nodes usually propagate their semantics via the edges of the reasoning graph, which misleads the semantic representations of other nodes and amplifies the noise signals. To mitigate the propagation of noisy semantic information, we introduce a Confidential Graph Attention Network (CO-GAT), which proposes a node masking mechanism for modeling the nodes. Specifically, CO-GAT calculates the node confidence score by estimating the relevance between the claim and evidence pieces. Then, the node masking mechanism uses the node confidence scores to control the noise information flow from the vanilla node to the other graph nodes. CO-GAT achieves a 73.59% FEVER score on the FEVER dataset and shows the generalization ability by broadening the effectiveness to the science-specific domain.",
    "authors": [
      "Yuqing Lan",
      "Zhenghao Liu",
      "Yu Gu",
      "Xiaoyuan Yi",
      "Xiaohua Li",
      "Liner Yang",
      "Ge Yu"
    ],
    "url": "http://arxiv.org/abs/2405.10481v1",
    "timestamp": 1715907723,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fa1b9e0c-214b-4f29-9ded-854b52eb73dd": {
    "pk": "fa1b9e0c-214b-4f29-9ded-854b52eb73dd",
    "title": "Physics-Informed Heterogeneous Graph Neural Networks for DC Blocker Placement",
    "abstract": "The threat of geomagnetic disturbances (GMDs) to the reliable operation of the bulk energy system has spurred the development of effective strategies for mitigating their impacts. One such approach involves placing transformer neutral blocking devices, which interrupt the path of geomagnetically induced currents (GICs) to limit their impact. The high cost of these devices and the sparsity of transformers that experience high GICs during GMD events, however, calls for a sparse placement strategy that involves high computational cost. To address this challenge, we developed a physics-informed heterogeneous graph neural network (PIHGNN) for solving the graph-based dc-blocker placement problem. Our approach combines a heterogeneous graph neural network (HGNN) with a physics-informed neural network (PINN) to capture the diverse types of nodes and edges in ac/dc networks and incorporates the physical laws of the power grid. We train the PIHGNN model using a surrogate power flow model and validate it using case studies. Results demonstrate that PIHGNN can effectively and efficiently support the deployment of GIC dc-current blockers, ensuring the continued supply of electricity to meet societal demands. Our approach has the potential to contribute to the development of more reliable and resilient power grids capable of withstanding the growing threat that GMDs pose.",
    "authors": [
      "Hongwei Jin",
      "Prasanna Balaprakash",
      "Allen Zou",
      "Pieter Ghysels",
      "Aditi S. Krishnapriyan",
      "Adam Mate",
      "Arthur Barnes",
      "Russell Bent"
    ],
    "url": "http://arxiv.org/abs/2405.10389v1",
    "timestamp": 1715884415,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4036efb9-a3f7-43b2-abbc-23b42f36398d": {
    "pk": "4036efb9-a3f7-43b2-abbc-23b42f36398d",
    "title": "Physics-incorporated Graph Neural Network for Multivariate Time Series Imputation",
    "abstract": "Exploring the missing values is an essential but challenging issue due to the complex latent spatio-temporal correlation and dynamic nature of time series. Owing to the outstanding performance in dealing with structure learning potentials, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) are often used to capture such complex spatio-temporal features in multivariate time series. However, these data-driven models often fail to capture the essential spatio-temporal relationships when significant signal corruption occurs. Additionally, calculating the high-order neighbor nodes in these models is of high computational complexity. To address these problems, we propose a novel higher-order spatio-temporal physics-incorporated GNN (HSPGNN). Firstly, the dynamic Laplacian matrix can be obtained by the spatial attention mechanism. Then, the generic inhomogeneous partial differential equation (PDE) of physical dynamic systems is used to construct the dynamic higher-order spatio-temporal GNN to obtain the missing time series values. Moreover, we estimate the missing impact by Normalizing Flows (NF) to evaluate the importance of each node in the graph for better explainability. Experimental results on four benchmark datasets demonstrate the effectiveness of HSPGNN and the superior performance when combining various order neighbor nodes. Also, graph-like optical flow, dynamic graphs, and missing impact can be obtained naturally by HSPGNN, which provides better dynamic analysis and explanation than traditional data-driven models. Our code is available at https://github.com/gorgen2020/HSPGNN.",
    "authors": [
      "Guojun Liang",
      "Prayag Tiwari",
      "Slawomir Nowaczyk",
      "Stefan Byttner"
    ],
    "url": "http://arxiv.org/abs/2405.10995v1",
    "timestamp": 1715877343,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "56d26af3-4255-433d-a52b-25486f96fa8f": {
    "pk": "56d26af3-4255-433d-a52b-25486f96fa8f",
    "title": "ENADPool: The Edge-Node Attention-based Differentiable Pooling for Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) are powerful tools for graph classification. One important operation for GNNs is the downsampling or pooling that can learn effective embeddings from the node representations. In this paper, we propose a new hierarchical pooling operation, namely the Edge-Node Attention-based Differentiable Pooling (ENADPool), for GNNs to learn effective graph representations. Unlike the classical hierarchical pooling operation that is based on the unclear node assignment and simply computes the averaged feature over the nodes of each cluster, the proposed ENADPool not only employs a hard clustering strategy to assign each node into an unique cluster, but also compress the node features as well as their edge connectivity strengths into the resulting hierarchical structure based on the attention mechanism after each pooling step. As a result, the proposed ENADPool simultaneously identifies the importance of different nodes within each separated cluster and edges between corresponding clusters, that significantly addresses the shortcomings of the uniform edge-node based structure information aggregation arising in the classical hierarchical pooling operation. Moreover, to mitigate the over-smoothing problem arising in existing GNNs, we propose a Multi-distance GNN (MD-GNN) model associated with the proposed ENADPool operation, allowing the nodes to actively and directly receive the feature information from neighbors at different random walk steps. Experiments demonstrate the effectiveness of the MD-GNN associated with the proposed ENADPool.",
    "authors": [
      "Zhehan Zhao",
      "Lu Bai",
      "Lixin Cui",
      "Ming Li",
      "Yue Wang",
      "Lixiang Xu",
      "Edwin R. Hancock"
    ],
    "url": "http://arxiv.org/abs/2405.10218v1",
    "timestamp": 1715875729,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7ea52784-c96b-4c62-a44a-4d80fad82364": {
    "pk": "7ea52784-c96b-4c62-a44a-4d80fad82364",
    "title": "Graph Neural Network based Handwritten Trajectories Recognition",
    "abstract": "The graph neural networks has been proved to be an efficient machine learning technique in real life applications. The handwritten recognition is one of the useful area in real life use where both offline and online handwriting recognition are required. The chain code as feature extraction technique has shown significant results in literature and we have been able to use chain codes with graph neural networks. To the best of our knowledge, this work presents first time a novel combination of handwritten trajectories features as chain codes and graph neural networks together. The handwritten trajectories for offline handwritten text has been evaluated using recovery of drawing order, whereas online handwritten trajectories are directly used with chain codes. Our results prove that present combination surpass previous results and minimize error rate in few epochs only.",
    "authors": [
      "Anuj Sharma",
      "Sukhdeep Singh",
      "S Ratna"
    ],
    "url": "http://arxiv.org/abs/2405.09247v1",
    "timestamp": 1715770842,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  }
}