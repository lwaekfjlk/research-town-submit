{
  "e6151e59-c621-4eb7-8bab-bb96b1ec6f02": {
    "pk": "e6151e59-c621-4eb7-8bab-bb96b1ec6f02",
    "title": "CDSA: Conservative Denoising Score-based Algorithm for Offline Reinforcement Learning",
    "abstract": "Distribution shift is a major obstacle in offline reinforcement learning, which necessitates minimizing the discrepancy between the learned policy and the behavior policy to avoid overestimating rare or unseen actions. Previous conservative offline RL algorithms struggle to generalize to unseen actions, despite their success in learning good in-distribution policy. In contrast, we propose to use the gradient fields of the dataset density generated from a pre-trained offline RL algorithm to adjust the original actions. We decouple the conservatism constraints from the policy, thus can benefit wide offline RL algorithms. As a consequence, we propose the Conservative Denoising Score-based Algorithm (CDSA) which utilizes the denoising score-based model to model the gradient of the dataset density, rather than the dataset density itself, and facilitates a more accurate and efficient method to adjust the action generated by the pre-trained policy in a deterministic and continuous MDP environment. In experiments, we show that our approach significantly improves the performance of baseline algorithms in D4RL datasets, and demonstrate the generalizability and plug-and-play capability of our model across different pre-trained offline RL policy in different tasks. We also validate that the agent exhibits greater risk aversion after employing our method while showcasing its ability to generalize effectively across diverse tasks.",
    "authors": [
      "Zeyuan Liu",
      "Kai Yang",
      "Xiu Li"
    ],
    "url": "http://arxiv.org/abs/2406.07541v1",
    "timestamp": 1718128769,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fddc2e19-1b18-4a3a-a54d-e3c386d0db77": {
    "pk": "fddc2e19-1b18-4a3a-a54d-e3c386d0db77",
    "title": "Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis",
    "abstract": "In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which resembles the result in classic RL, where $c_{\\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.",
    "authors": [
      "Qining Zhang",
      "Honghao Wei",
      "Lei Ying"
    ],
    "url": "http://arxiv.org/abs/2406.07455v1",
    "timestamp": 1718125301,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a972b03c-dd5b-44e5-b626-395532e048e5": {
    "pk": "a972b03c-dd5b-44e5-b626-395532e048e5",
    "title": "EdgeTimer: Adaptive Multi-Timescale Scheduling in Mobile Edge Computing with Deep Reinforcement Learning",
    "abstract": "In mobile edge computing (MEC), resource scheduling is crucial to task requests' performance and service providers' cost, involving multi-layer heterogeneous scheduling decisions. Existing schedulers typically adopt static timescales to regularly update scheduling decisions of each layer, without adaptive adjustment of timescales for different layers, resulting in potentially poor performance in practice.   We notice that the adaptive timescales would significantly improve the trade-off between the operation cost and delay performance. Based on this insight, we propose EdgeTimer, the first work to automatically generate adaptive timescales to update multi-layer scheduling decisions using deep reinforcement learning (DRL). First, EdgeTimer uses a three-layer hierarchical DRL framework to decouple the multi-layer decision-making task into a hierarchy of independent sub-tasks for improving learning efficiency. Second, to cope with each sub-task, EdgeTimer adopts a safe multi-agent DRL algorithm for decentralized scheduling while ensuring system reliability. We apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns. Extensive trace-driven experiments demonstrate that EdgeTimer can learn adaptive timescales, irrespective of workload patterns and built-in scheduling rules. It obtains up to 9.1x more profit than existing approaches without sacrificing the delay performance.",
    "authors": [
      "Yijun Hao",
      "Shusen Yang",
      "Fang Li",
      "Yifan Zhang",
      "Shibo Wang",
      "Xuebin Ren"
    ],
    "url": "http://arxiv.org/abs/2406.07342v1",
    "timestamp": 1718118494,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.NI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "007c9fda-00ff-47e9-bc5e-4d0bdd41dd3a": {
    "pk": "007c9fda-00ff-47e9-bc5e-4d0bdd41dd3a",
    "title": "Beyond Training: Optimizing Reinforcement Learning Based Job Shop Scheduling Through Adaptive Action Sampling",
    "abstract": "Learned construction heuristics for scheduling problems have become increasingly competitive with established solvers and heuristics in recent years. In particular, significant improvements have been observed in solution approaches using deep reinforcement learning (DRL). While much attention has been paid to the design of network architectures and training algorithms to achieve state-of-the-art results, little research has investigated the optimal use of trained DRL agents during inference. Our work is based on the hypothesis that, similar to search algorithms, the utilization of trained DRL agents should be dependent on the acceptable computational budget. We propose a simple yet effective parameterization, called $\\delta$-sampling that manipulates the trained action vector to bias agent behavior towards exploration or exploitation during solution construction. By following this approach, we can achieve a more comprehensive coverage of the search space while still generating an acceptable number of solutions. In addition, we propose an algorithm for obtaining the optimal parameterization for such a given number of solutions and any given trained agent. Experiments extending existing training protocols for job shop scheduling problems with our inference method validate our hypothesis and result in the expected improvements of the generated solutions.",
    "authors": [
      "Constantin Waubert de Puiseau",
      "Christian D\u00f6rpelkus",
      "Jannik Peters",
      "Hasan Tercan",
      "Tobias Meisen"
    ],
    "url": "http://arxiv.org/abs/2406.07325v1",
    "timestamp": 1718117958,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c9a6cb6d-a647-48c0-bd3f-0cdc88cc6b21": {
    "pk": "c9a6cb6d-a647-48c0-bd3f-0cdc88cc6b21",
    "title": "Multi-objective Reinforcement learning from AI Feedback",
    "abstract": "This paper presents Multi-Objective Reinforcement Learning from AI Feedback (MORLAIF), a novel approach to improving the alignment and performance of language models trained using reinforcement learning from AI feedback (RLAIF). In contrast to standard approaches that train a single preference model to represent all human preferences, MORLAIF decomposes this task into multiple simpler principles, such as toxicity, factuality, and sycophancy. Separate preference models are trained for each principle using feedback from GPT-3.5-Turbo. These preference model scores are then combined using different scalarization functions to provide a reward signal for Proximal Policy Optimization (PPO) training of the target language model. Our experiments indicate that MORLAIF outperforms the standard RLAIF baselines and that MORLAIF can be used to align larger language models using smaller ones. Surprisingly, the choice of scalarization function does not appear to significantly impact the results.",
    "authors": [
      "Marcus Williams"
    ],
    "url": "http://arxiv.org/abs/2406.07295v1",
    "timestamp": 1718115840,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "69bd177a-9215-4132-9111-ae91bec2b2c9": {
    "pk": "69bd177a-9215-4132-9111-ae91bec2b2c9",
    "title": "Hybrid Reinforcement Learning from Offline Observation Alone",
    "abstract": "We consider the hybrid reinforcement learning setting where the agent has access to both offline data and online interactive access. While Reinforcement Learning (RL) research typically assumes offline data contains complete action, reward and transition information, datasets with only state information (also known as observation-only datasets) are more general, abundant and practical. This motivates our study of the hybrid RL with observation-only offline dataset framework. While the task of competing with the best policy \"covered\" by the offline data can be solved if a reset model of the environment is provided (i.e., one that can be reset to any state), we show evidence of hardness when only given the weaker trace model (i.e., one can only reset to the initial states and must produce full traces through the environment), without further assumption of admissibility of the offline data. Under the admissibility assumptions -- that the offline data could actually be produced by the policy class we consider -- we propose the first algorithm in the trace model setting that provably matches the performance of algorithms that leverage a reset model. We also perform proof-of-concept experiments that suggest the effectiveness of our algorithm in practice.",
    "authors": [
      "Yuda Song",
      "J. Andrew Bagnell",
      "Aarti Singh"
    ],
    "url": "http://arxiv.org/abs/2406.07253v1",
    "timestamp": 1718112845,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8e9d8386-b1ca-43a5-8a19-cbfa9eb19026": {
    "pk": "8e9d8386-b1ca-43a5-8a19-cbfa9eb19026",
    "title": "A generic and robust quantum agent inspired by deep meta-reinforcement learning",
    "abstract": "Deep reinforcement learning (deep RL) has enabled human- or superhuman- performances in various applications. Recently, deep RL has also been adopted to improve the performance of quantum control. However, a large volume of data is typically required to train the neural network in deep RL, making it inefficient compared with the traditional optimal quantum control method. Here, we thus develop a new training algorithm inspired by the deep meta-reinforcement learning (deep meta-RL), which requires significantly less training data. The trained neural network is adaptive and robust. In addition, the algorithm proposed by us has been applied to design the Hadamard gate and show that for a wide range of parameters the infidelity of the obtained gate can be made of the order 0.0001. Our algorithm can also automatically adjust the number of pulses required to generate the target gate, which is different from the traditional optimal quantum control method which typically fixes the number of pulses a-priory. The results of this paper can pave the way towards constructing a universally robust quantum agent catering to the different demands in quantum technologies.",
    "authors": [
      "Zibo Miao",
      "Shihui Zhang",
      "Yu Pan",
      "Sibo Tao",
      "Yu Chen"
    ],
    "url": "http://arxiv.org/abs/2406.07225v1",
    "timestamp": 1718111070,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d7363152-efce-4bb4-9808-4ad199d0daee": {
    "pk": "d7363152-efce-4bb4-9808-4ad199d0daee",
    "title": "Semantic-Aware Spectrum Sharing in Internet of Vehicles Based on Deep Reinforcement Learning",
    "abstract": "This work aims to investigate semantic communication in high-speed mobile Internet of vehicles (IoV) environments, with a focus on the spectrum sharing between vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications. We specifically address spectrum scarcity and network traffic and then propose a semantic-aware spectrum sharing algorithm (SSS) based on the deep reinforcement learning (DRL) soft actor-critic (SAC) approach. Firstly, we delve into the extraction of semantic information. Secondly, we redefine metrics for semantic information in V2V and V2I spectrum sharing in IoV environments, introducing high-speed semantic spectrum efficiency (HSSE) and semantic transmission rate (HSR). Finally, we employ the SAC algorithm for decision optimization in V2V and V2I spectrum sharing based on semantic information. This optimization encompasses the optimal link of V2V and V2I sharing strategies, the transmission power for vehicles sending semantic information and the length of transmitted semantic symbols, aiming at maximizing HSSE of V2I and enhancing success rate of effective semantic information transmission (SRS) of V2V. Experimental results demonstrate that the SSS algorithm outperforms other baseline algorithms, including other traditional-communication-based spectrum sharing algorithms and spectrum sharing algorithm using other reinforcement learning approaches. The SSS algorithm exhibits a 15% increase in HSSE and approximately a 7% increase in SRS.",
    "authors": [
      "Zhiyu Shao",
      "Qiong Wu",
      "Pingyi Fan",
      "Nan Cheng",
      "Wen Chen",
      "Jiangzhou Wang",
      "Khaled B. Letaief"
    ],
    "url": "http://arxiv.org/abs/2406.07213v1",
    "timestamp": 1718109761,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "dfbe6763-11d5-4c95-a2e2-34c412d02d01": {
    "pk": "dfbe6763-11d5-4c95-a2e2-34c412d02d01",
    "title": "CHARME: A chain-based reinforcement learning approach for the minor embedding problem",
    "abstract": "Quantum Annealing (QA) holds great potential for solving combinatorial optimization problems efficiently. However, the effectiveness of QA algorithms heavily relies on the embedding of problem instances, represented as logical graphs, into the quantum unit processing (QPU) whose topology is in form of a limited connectivity graph, known as the minor embedding Problem. Existing methods for the minor embedding problem suffer from scalability issues when confronted with larger problem sizes. In this paper, we propose a novel approach utilizing Reinforcement Learning (RL) techniques to address the minor embedding problem, named CHARME. CHARME includes three key components: a Graph Neural Network (GNN) architecture for policy modeling, a state transition algorithm ensuring solution validity, and an order exploration strategy for effective training. Through comprehensive experiments on synthetic and real-world instances, we demonstrate that the efficiency of our proposed order exploration strategy as well as our proposed RL framework, CHARME. In details, CHARME yields superior solutions compared to fast embedding methods such as Minorminer and ATOM. Moreover, our method surpasses the OCT-based approach, known for its slower runtime but high-quality solutions, in several cases. In addition, our proposed exploration enhances the efficiency of the training of the CHARME framework by providing better solutions compared to the greedy strategy.",
    "authors": [
      "Hoang M. Ngo",
      "Nguyen H K. Do",
      "Minh N. Vu",
      "Tamer Kahveci",
      "My T. Thai"
    ],
    "url": "http://arxiv.org/abs/2406.07124v1",
    "timestamp": 1718100730,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cb74e37a-a928-460b-a293-f970108b407e": {
    "pk": "cb74e37a-a928-460b-a293-f970108b407e",
    "title": "Optimal Gait Control for a Tendon-driven Soft Quadruped Robot by Model-based Reinforcement Learning",
    "abstract": "This study presents an innovative approach to optimal gait control for a soft quadruped robot enabled by four Compressible Tendon-driven Soft Actuators (CTSAs). Improving our previous studies of using model-free reinforcement learning for gait control, we employ model-based reinforcement learning (MBRL) to further enhance the performance of the gait controller. Compared to rigid robots, the proposed soft quadruped robot has better safety, less weight, and a simpler mechanism for fabrication and control. However, the primary challenge lies in developing sophisticated control algorithms to attain optimal gait control for fast and stable locomotion. The research employs a multi-stage methodology, including state space restriction, data-driven model training, and reinforcement learning algorithm development. Compared to benchmark methods, the proposed MBRL algorithm, combined with post-training, significantly improves the efficiency and performance of gait control policies. The developed policy is both robust and adaptable to the robot's deformable morphology. The study concludes by highlighting the practical applicability of these findings in real-world scenarios.",
    "authors": [
      "Xuezhi Niu",
      "Kaige Tan",
      "Lei Feng"
    ],
    "url": "http://arxiv.org/abs/2406.07069v1",
    "timestamp": 1718096168,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ab6759b2-2369-4c2f-bc36-f696051d8680": {
    "pk": "ab6759b2-2369-4c2f-bc36-f696051d8680",
    "title": "DNN Partitioning, Task Offloading, and Resource Allocation in Dynamic Vehicular Networks: A Lyapunov-Guided Diffusion-Based Reinforcement Learning Approach",
    "abstract": "The rapid advancement of Artificial Intelligence (AI) has introduced Deep Neural Network (DNN)-based tasks to the ecosystem of vehicular networks. These tasks are often computation-intensive, requiring substantial computation resources, which are beyond the capability of a single vehicle. To address this challenge, Vehicular Edge Computing (VEC) has emerged as a solution, offering computing services for DNN-based tasks through resource pooling via Vehicle-to-Vehicle/Infrastructure (V2V/V2I) communications. In this paper, we formulate the problem of joint DNN partitioning, task offloading, and resource allocation in VEC as a dynamic long-term optimization. Our objective is to minimize the DNN-based task completion time while guaranteeing the system stability over time. To this end, we first leverage a Lyapunov optimization technique to decouple the original long-term optimization with stability constraints into a per-slot deterministic problem. Afterwards, we propose a Multi-Agent Diffusion-based Deep Reinforcement Learning (MAD2RL) algorithm, incorporating the innovative use of diffusion models to determine the optimal DNN partitioning and task offloading decisions. Furthermore, we integrate convex optimization techniques into MAD2RL as a subroutine to allocate computation resources, enhancing the learning efficiency. Through simulations under real-world movement traces of vehicles, we demonstrate the superior performance of our proposed algorithm compared to existing benchmark solutions.",
    "authors": [
      "Zhang Liu",
      "Hongyang Du",
      "Junzhe Lin",
      "Zhibin Gao",
      "Lianfen Huang",
      "Seyyedali Hosseinalipour",
      "Dusit Niyato"
    ],
    "url": "http://arxiv.org/abs/2406.06986v1",
    "timestamp": 1718087463,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c2f32278-5c52-44c0-bc82-42a473e2302c": {
    "pk": "c2f32278-5c52-44c0-bc82-42a473e2302c",
    "title": "Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning",
    "abstract": "In this paper, we study the non-asymptotic sample complexity for the pure exploration problem in contextual bandits and tabular reinforcement learning (RL): identifying an epsilon-optimal policy from a set of policies with high probability. Existing work in bandits has shown that it is possible to identify the best policy by estimating only the difference between the behaviors of individual policies, which can be substantially cheaper than estimating the behavior of each policy directly. However, the best-known complexities in RL fail to take advantage of this and instead estimate the behavior of each policy directly. Does it suffice to estimate only the differences in the behaviors of policies in RL? We answer this question positively for contextual bandits but in the negative for tabular RL, showing a separation between contextual bandits and RL. However, inspired by this, we show that it almost suffices to estimate only the differences in RL: if we can estimate the behavior of a single reference policy, it suffices to only estimate how any other policy deviates from this reference policy. We develop an algorithm which instantiates this principle and obtains, to the best of our knowledge, the tightest known bound on the sample complexity of tabular RL.",
    "authors": [
      "Adhyyan Narang",
      "Andrew Wagenmaker",
      "Lillian Ratliff",
      "Kevin Jamieson"
    ],
    "url": "http://arxiv.org/abs/2406.06856v1",
    "timestamp": 1718064139,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e1cae35e-967a-4ca4-a4c4-cb04a675a177": {
    "pk": "e1cae35e-967a-4ca4-a4c4-cb04a675a177",
    "title": "Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation",
    "abstract": "Adaptive brain stimulation can treat neurological conditions such as Parkinson's disease and post-stroke motor deficits by influencing abnormal neural activity. Because of patient heterogeneity, each patient requires a unique stimulation policy to achieve optimal neural responses. Model-free reinforcement learning (MFRL) holds promise in learning effective policies for a variety of similar control tasks, but is limited in domains like brain stimulation by a need for numerous costly environment interactions. In this work we introduce Coprocessor Actor Critic, a novel, model-based reinforcement learning (MBRL) approach for learning neural coprocessor policies for brain stimulation. Our key insight is that coprocessor policy learning is a combination of learning how to act optimally in the world and learning how to induce optimal actions in the world through stimulation of an injured brain. We show that our approach overcomes the limitations of traditional MFRL methods in terms of sample efficiency and task success and outperforms baseline MBRL approaches in a neurologically realistic model of an injured brain.",
    "authors": [
      "Michelle Pan",
      "Mariah Schrum",
      "Vivek Myers",
      "Erdem B\u0131y\u0131k",
      "Anca Dragan"
    ],
    "url": "http://arxiv.org/abs/2406.06714v1",
    "timestamp": 1718043783,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b8659a0f-fd47-4caf-b7a4-d7dc24a5eacf": {
    "pk": "b8659a0f-fd47-4caf-b7a4-d7dc24a5eacf",
    "title": "Verification-Guided Shielding for Deep Reinforcement Learning",
    "abstract": "In recent years, Deep Reinforcement Learning (DRL) has emerged as an effective approach to solving real-world tasks. However, despite their successes, DRL-based policies suffer from poor reliability, which limits their deployment in safety-critical domains. As a result, various methods have been put forth to address this issue by providing formal safety guarantees. Two main approaches include shielding and verification. While shielding ensures the safe behavior of the policy by employing an external online component (i.e., a ``shield'') that overruns potentially dangerous actions, this approach has a significant computational cost as the shield must be invoked at runtime to validate every decision. On the other hand, verification is an offline process that can identify policies that are unsafe, prior to their deployment, yet, without providing alternative actions when such a policy is deemed unsafe. In this work, we present verification-guided shielding -- a novel approach that bridges the DRL reliability gap by integrating these two methods. Our approach combines both formal and probabilistic verification tools to partition the input domain into safe and unsafe regions. In addition, we employ clustering and symbolic representation procedures that compress the unsafe regions into a compact representation. This, in turn, allows to temporarily activate the shield solely in (potentially) unsafe regions, in an efficient manner. Our novel approach allows to significantly reduce runtime overhead while still preserving formal safety guarantees. We extensively evaluate our approach on two benchmarks from the robotic navigation domain, as well as provide an in-depth analysis of its scalability and completeness.",
    "authors": [
      "Davide Corsi",
      "Guy Amir",
      "Andoni Rodriguez",
      "Cesar Sanchez",
      "Guy Katz",
      "Roy Fox"
    ],
    "url": "http://arxiv.org/abs/2406.06507v1",
    "timestamp": 1718041499,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "187b7cc9-1a32-4b16-8f43-fa714c55f793": {
    "pk": "187b7cc9-1a32-4b16-8f43-fa714c55f793",
    "title": "Boosting Robustness in Preference-Based Reinforcement Learning with Dynamic Sparsity",
    "abstract": "For autonomous agents to successfully integrate into human-centered environments, agents should be able to learn from and adapt to humans in their native settings. Preference-based reinforcement learning (PbRL) is a promising approach that learns reward functions from human preferences. This enables RL agents to adapt their behavior based on human desires. However, humans live in a world full of diverse information, most of which is not relevant to completing a particular task. It becomes essential that agents learn to focus on the subset of task-relevant environment features. Unfortunately, prior work has largely ignored this aspect; primarily focusing on improving PbRL algorithms in standard RL environments that are carefully constructed to contain only task-relevant features. This can result in algorithms that may not effectively transfer to a more noisy real-world setting. To that end, this work proposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages principles of dynamic sparse training to learn robust reward models that can focus on task-relevant features. We study the effectiveness of R2N in the Extremely Noisy Environment setting, an RL problem setting where up to 95% of the state features are irrelevant distractions. In experiments with a simulated teacher, we demonstrate that R2N can adapt the sparse connectivity of its neural networks to focus on task-relevant features, enabling R2N to significantly outperform several state-of-the-art PbRL algorithms in multiple locomotion and control environments.",
    "authors": [
      "Calarina Muslimani",
      "Bram Grooten",
      "Deepak Ranganatha Sastry Mamillapalli",
      "Mykola Pechenizkiy",
      "Decebal Constantin Mocanu",
      "Matthew E. Taylor"
    ],
    "url": "http://arxiv.org/abs/2406.06495v1",
    "timestamp": 1718040667,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d6b792d7-c5c9-4100-ad48-60c51120f89d": {
    "pk": "d6b792d7-c5c9-4100-ad48-60c51120f89d",
    "title": "Towards Real-World Efficiency: Domain Randomization in Reinforcement Learning for Pre-Capture of Free-Floating Moving Targets by Autonomous Robots",
    "abstract": "In this research, we introduce a deep reinforcement learning-based control approach to address the intricate challenge of the robotic pre-grasping phase under microgravity conditions. Leveraging reinforcement learning eliminates the necessity for manual feature design, therefore simplifying the problem and empowering the robot to learn pre-grasping policies through trial and error. Our methodology incorporates an off-policy reinforcement learning framework, employing the soft actor-critic technique to enable the gripper to proficiently approach a free-floating moving object, ensuring optimal pre-grasp success. For effective learning of the pre-grasping approach task, we developed a reward function that offers the agent clear and insightful feedback. Our case study examines a pre-grasping task where a Robotiq 3F gripper is required to navigate towards a free-floating moving target, pursue it, and subsequently position itself at the desired pre-grasp location. We assessed our approach through a series of experiments in both simulated and real-world environments. The source code, along with recordings of real-world robot grasping, is available at Fanuc_Robotiq_Grasp.",
    "authors": [
      "Bahador Beigomi",
      "Zheng H. Zhu"
    ],
    "url": "http://arxiv.org/abs/2406.06460v1",
    "timestamp": 1718038491,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ed50c87a-948d-48ae-a8c1-ca9c50657c5c": {
    "pk": "ed50c87a-948d-48ae-a8c1-ca9c50657c5c",
    "title": "Is Value Functions Estimation with Classification Plug-and-play for Offline Reinforcement Learning?",
    "abstract": "In deep Reinforcement Learning (RL), value functions are typically approximated using deep neural networks and trained via mean squared error regression objectives to fit the true value functions. Recent research has proposed an alternative approach, utilizing the cross-entropy classification objective, which has demonstrated improved performance and scalability of RL algorithms. However, existing study have not extensively benchmarked the effects of this replacement across various domains, as the primary objective was to demonstrate the efficacy of the concept across a broad spectrum of tasks, without delving into in-depth analysis. Our work seeks to empirically investigate the impact of such a replacement in an offline RL setup and analyze the effects of different aspects on performance. Through large-scale experiments conducted across a diverse range of tasks using different algorithms, we aim to gain deeper insights into the implications of this approach. Our results reveal that incorporating this change can lead to superior performance over state-of-the-art solutions for some algorithms in certain tasks, while maintaining comparable performance levels in other tasks, however for other algorithms this modification might lead to the dramatic performance drop. This findings are crucial for further application of classification approach in research and practical tasks.",
    "authors": [
      "Denis Tarasov",
      "Kirill Brilliantov",
      "Dmitrii Kharlapenko"
    ],
    "url": "http://arxiv.org/abs/2406.06309v1",
    "timestamp": 1718029511,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c39cd953-c72c-4de3-a466-011cb248e0d1": {
    "pk": "c39cd953-c72c-4de3-a466-011cb248e0d1",
    "title": "Deep Multi-Objective Reinforcement Learning for Utility-Based Infrastructural Maintenance Optimization",
    "abstract": "In this paper, we introduce Multi-Objective Deep Centralized Multi-Agent Actor-Critic (MO- DCMAC), a multi-objective reinforcement learning (MORL) method for infrastructural maintenance optimization, an area traditionally dominated by single-objective reinforcement learning (RL) approaches. Previous single-objective RL methods combine multiple objectives, such as probability of collapse and cost, into a singular reward signal through reward-shaping. In contrast, MO-DCMAC can optimize a policy for multiple objectives directly, even when the utility function is non-linear. We evaluated MO-DCMAC using two utility functions, which use probability of collapse and cost as input. The first utility function is the Threshold utility, in which MO-DCMAC should minimize cost so that the probability of collapse is never above the threshold. The second is based on the Failure Mode, Effects, and Criticality Analysis (FMECA) methodology used by asset managers to asses maintenance plans. We evaluated MO-DCMAC, with both utility functions, in multiple maintenance environments, including ones based on a case study of the historical quay walls of Amsterdam. The performance of MO-DCMAC was compared against multiple rule-based policies based on heuristics currently used for constructing maintenance plans. Our results demonstrate that MO-DCMAC outperforms traditional rule-based policies across various environments and utility functions.",
    "authors": [
      "Jesse van Remmerden",
      "Maurice Kenter",
      "Diederik M. Roijers",
      "Charalampos Andriotis",
      "Yingqian Zhang",
      "Zaharah Bukhsh"
    ],
    "url": "http://arxiv.org/abs/2406.06184v1",
    "timestamp": 1718018905,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f2e99450-1a33-4966-9b45-277732c31f67": {
    "pk": "f2e99450-1a33-4966-9b45-277732c31f67",
    "title": "Sim-To-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery",
    "abstract": "Automation holds the potential to assist surgeons in robotic interventions, shifting their mental work load from visuomotor control to high level decision making. Reinforcement learning has shown promising results in learning complex visuomotor policies, especially in simulation environments where many samples can be collected at low cost. A core challenge is learning policies in simulation that can be deployed in the real world, thereby overcoming the sim-to-real gap.   In this work, we bridge the visual sim-to-real gap with an image-based reinforcement learning pipeline based on pixel-level domain adaptation and demonstrate its effectiveness on an image-based task in deformable object manipulation. We choose a tissue retraction task because of its importance in clinical reality of precise cancer surgery. After training in simulation on domain-translated images, our policy requires no retraining to perform tissue retraction with a 50% success rate on the real robotic system using raw RGB images. Furthermore, our sim-to-real transfer method makes no assumptions on the task itself and requires no paired images. This work introduces the first successful application of visual sim-to-real transfer for robotic manipulation of deformable objects in the surgical field, which represents a notable step towards the clinical translation of cognitive surgical robotics.",
    "authors": [
      "Paul Maria Scheikl",
      "Eleonora Tagliabue",
      "Bal\u00e1zs Gyenes",
      "Martin Wagner",
      "Diego Dall'Alba",
      "Paolo Fiorini",
      "Franziska Mathis-Ullrich"
    ],
    "url": "http://arxiv.org/abs/2406.06092v1",
    "timestamp": 1718007796,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "280b959c-130f-48f6-a4d1-410c34615638": {
    "pk": "280b959c-130f-48f6-a4d1-410c34615638",
    "title": "LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical Reinforcement Learning",
    "abstract": "Intent-based network automation is a promising tool to enable easier network management however certain challenges need to be effectively addressed. These are: 1) processing intents, i.e., identification of logic and necessary parameters to fulfill an intent, 2) validating an intent to align it with current network status, and 3) satisfying intents via network optimizing functions like xApps and rApps in O-RAN. This paper addresses these points via a three-fold strategy to introduce intent-based automation for O-RAN. First, intents are processed via a lightweight Large Language Model (LLM). Secondly, once an intent is processed, it is validated against future incoming traffic volume profiles (high or low). Finally, a series of network optimization applications (rApps and xApps) have been developed. With their machine learning-based functionalities, they can improve certain key performance indicators such as throughput, delay, and energy efficiency. In this final stage, using an attention-based hierarchical reinforcement learning algorithm, these applications are optimally initiated to satisfy the intent of an operator. Our simulations show that the proposed method can achieve at least 12% increase in throughput, 17.1% increase in energy efficiency, and 26.5% decrease in network delay compared to the baseline algorithms.",
    "authors": [
      "Md Arafat Habib",
      "Pedro Enrique Iturria Rivera",
      "Yigit Ozcan",
      "Medhat Elsayed",
      "Majid Bavand",
      "Raimundus Gaigalas",
      "Melike Erol-Kantarci"
    ],
    "url": "http://arxiv.org/abs/2406.06059v1",
    "timestamp": 1718003634,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.NI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "84646c95-bb14-4565-ab20-a4a2e77b7694": {
    "pk": "84646c95-bb14-4565-ab20-a4a2e77b7694",
    "title": "Risk Sensitivity in Markov Games and Multi-Agent Reinforcement Learning: A Systematic Review",
    "abstract": "Markov games (MGs) and multi-agent reinforcement learning (MARL) are studied to model decision making in multi-agent systems. Traditionally, the objective in MG and MARL has been risk-neutral, i.e., agents are assumed to optimize a performance metric such as expected return, without taking into account subjective or cognitive preferences of themselves or of other agents. However, ignoring such preferences leads to inaccurate models of decision making in many real-world scenarios in finance, operations research, and behavioral economics. Therefore, when these preferences are present, it is necessary to incorporate a suitable measure of risk into the optimization objective of agents, which opens the door to risk-sensitive MG and MARL. In this paper, we systemically review the literature on risk sensitivity in MG and MARL that has been growing in recent years alongside other areas of reinforcement learning and game theory. We define and mathematically describe different risk measures used in MG and MARL and individually for each measure, discuss articles that incorporate it. Finally, we identify recent trends in theoretical and applied works in the field and discuss possible directions of future research.",
    "authors": [
      "Hafez Ghaemi",
      "Shirin Jamshidi",
      "Mohammad Mashreghi",
      "Majid Nili Ahmadabadi",
      "Hamed Kebriaei"
    ],
    "url": "http://arxiv.org/abs/2406.06041v1",
    "timestamp": 1718000373,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.GT",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9d8e1742-1683-45a4-8235-7df06675676f": {
    "pk": "9d8e1742-1683-45a4-8235-7df06675676f",
    "title": "Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning",
    "abstract": "Recently, various pre-training methods have been introduced in vision-based Reinforcement Learning (RL). However, their generalization ability remains unclear due to evaluations being limited to in-distribution environments and non-unified experimental setups. To address this, we introduce the Atari Pre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10 million transitions from 50 Atari games and evaluates it across diverse environment distributions. Our experiments show that pre-training objectives focused on learning task-agnostic features (e.g., identifying objects and understanding temporal dynamics) enhance generalization across different environments. In contrast, objectives focused on learning task-specific knowledge (e.g., identifying agents and fitting reward functions) improve performance in environments similar to the pre-training dataset but not in varied ones. We publicize our codes, datasets, and model checkpoints at https://github.com/dojeon-ai/Atari-PB.",
    "authors": [
      "Donghu Kim",
      "Hojoon Lee",
      "Kyungmin Lee",
      "Dongyoon Hwang",
      "Jaegul Choo"
    ],
    "url": "http://arxiv.org/abs/2406.06037v1",
    "timestamp": 1717999598,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "80605a40-139c-49e3-a0f6-3b6a03c3683a": {
    "pk": "80605a40-139c-49e3-a0f6-3b6a03c3683a",
    "title": "Discovering Multiple Solutions from a Single Task in Offline Reinforcement Learning",
    "abstract": "Recent studies on online reinforcement learning (RL) have demonstrated the advantages of learning multiple behaviors from a single task, as in the case of few-shot adaptation to a new environment. Although this approach is expected to yield similar benefits in offline RL, appropriate methods for learning multiple solutions have not been fully investigated in previous studies. In this study, we therefore addressed the problem of finding multiple solutions from a single task in offline RL. We propose algorithms that can learn multiple solutions in offline RL, and empirically investigate their performance. Our experimental results show that the proposed algorithm learns multiple qualitatively and quantitatively distinctive solutions in offline RL.",
    "authors": [
      "Takayuki Osa",
      "Tatsuya Harada"
    ],
    "url": "http://arxiv.org/abs/2406.05993v1",
    "timestamp": 1717989949,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0e8bd3e5-6950-4bcf-98e0-648f98980ca7": {
    "pk": "0e8bd3e5-6950-4bcf-98e0-648f98980ca7",
    "title": "LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning",
    "abstract": "Developing interactive systems that leverage natural language instructions to solve complex robotic control tasks has been a long-desired goal in the robotics community. Large Language Models (LLMs) have demonstrated exceptional abilities in handling complex tasks, including logical reasoning, in-context learning, and code generation. However, predicting low-level robotic actions using LLMs poses significant challenges. Additionally, the complexity of such tasks usually demands the acquisition of policies to execute diverse subtasks and combine them to attain the ultimate objective. Hierarchical Reinforcement Learning (HRL) is an elegant approach for solving such tasks, which provides the intuitive benefits of temporal abstraction and improved exploration. However, HRL faces the recurring issue of non-stationarity due to unstable lower primitive behaviour. In this work, we propose LGR2, a novel HRL framework that leverages language instructions to generate a stationary reward function for the higher-level policy. Since the language-guided reward is unaffected by the lower primitive behaviour, LGR2 mitigates non-stationarity and is thus an elegant method for leveraging language instructions to solve robotic control tasks. To analyze the efficacy of our approach, we perform empirical analysis and demonstrate that LGR2 effectively alleviates non-stationarity in HRL. Our approach attains success rates exceeding 70$\\%$ in challenging, sparse-reward robotic navigation and manipulation environments where the baselines fail to achieve any significant progress. Additionally, we conduct real-world robotic manipulation experiments and demonstrate that CRISP shows impressive generalization in real-world scenarios.",
    "authors": [
      "Utsav Singh",
      "Pramit Bhattacharyya",
      "Vinay P. Namboodiri"
    ],
    "url": "http://arxiv.org/abs/2406.05881v1",
    "timestamp": 1717958424,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a47ab862-dfef-43dd-a4a2-8d5cf3222bc2": {
    "pk": "a47ab862-dfef-43dd-a4a2-8d5cf3222bc2",
    "title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
    "abstract": "Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents. Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills. In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment. These games let the agent hone their skills on a predefined set of tasks. We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT-3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision. Experimental results based on both the human participants and baseline text-based RL agents reveal that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can. These results enforce STARLING's potential to serve as a sandbox environment for further research in self-supervised text-based RL.",
    "authors": [
      "Shreyas Basavatia",
      "Keerthiram Murugesan",
      "Shivam Ratnakar"
    ],
    "url": "http://arxiv.org/abs/2406.05872v1",
    "timestamp": 1717956467,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "75363bf9-ef03-4c0b-b78d-130be9cc3f8e": {
    "pk": "75363bf9-ef03-4c0b-b78d-130be9cc3f8e",
    "title": "Enhanced Flight Envelope Protection: A Novel Reinforcement Learning Approach",
    "abstract": "This paper introduces a flight envelope protection algorithm on a longitudinal axis that leverages reinforcement learning (RL). By considering limits on variables such as angle of attack, load factor, and pitch rate, the algorithm counteracts excessive pilot or control commands with restoring actions. Unlike traditional methods requiring manual tuning, RL facilitates the approximation of complex functions within the trained model, streamlining the design process. This study demonstrates the promising results of RL in enhancing flight envelope protection, offering a novel and easy-to-scale method for safety-ensured flight.",
    "authors": [
      "Akin Catak",
      "Ege C. Altunkaya",
      "Mustafa Demir",
      "Emre Koyuncu",
      "Ibrahim Ozkol"
    ],
    "url": "http://arxiv.org/abs/2406.05586v1",
    "timestamp": 1717884677,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "db9e6345-7673-4145-9767-982cdafc522c": {
    "pk": "db9e6345-7673-4145-9767-982cdafc522c",
    "title": "Joint Cooperative Clustering and Power Control for Energy-Efficient Cell-Free XL-MIMO with Multi-Agent Reinforcement Learning",
    "abstract": "In this paper, we investigate the amalgamation of cell-free (CF) and extremely large-scale multiple-input multiple-output (XL-MIMO) technologies, referred to as a CF XL-MIMO, as a promising advancement for enabling future mobile networks. To address the computational complexity and communication power consumption associated with conventional centralized optimization, we focus on user-centric dynamic networks in which each user is served by an adaptive subset of access points (AP) rather than all of them. We begin our research by analyzing a joint resource allocation problem for energy-efficient CF XL-MIMO systems, encompassing cooperative clustering and power control design, where all clusters are adaptively adjustable. Then, we propose an innovative double-layer multi-agent reinforcement learning (MARL)-based scheme, which offers an effective strategy to tackle the challenges of high-dimensional signal processing. In the section of numerical results, we compare various algorithms with different network architectures. These comparisons reveal that the proposed MARL-based cooperative architecture can effectively strike a balance between system performance and communication overhead, thereby improving energy efficiency performance. It is important to note that increasing the number of user equipments participating in information sharing can effectively enhance SE performance, which also leads to an increase in power consumption, resulting in a non-trivial trade-off between the number of participants and EE performance.",
    "authors": [
      "Ziheng Liu",
      "Jiayi Zhang",
      "Zhilong Li",
      "Derrick Wing Kwan Ng",
      "Bo Ai"
    ],
    "url": "http://arxiv.org/abs/2406.05481v1",
    "timestamp": 1717855754,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IT",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2465f506-3bf6-494b-b751-0158e4da7f82": {
    "pk": "2465f506-3bf6-494b-b751-0158e4da7f82",
    "title": "Diffusion-based Reinforcement Learning for Dynamic UAV-assisted Vehicle Twins Migration in Vehicular Metaverses",
    "abstract": "Air-ground integrated networks can relieve communication pressure on ground transportation networks and provide 6G-enabled vehicular Metaverses services offloading in remote areas with sparse RoadSide Units (RSUs) coverage and downtown areas where users have a high demand for vehicular services. Vehicle Twins (VTs) are the digital twins of physical vehicles to enable more immersive and realistic vehicular services, which can be offloaded and updated on RSU, to manage and provide vehicular Metaverses services to passengers and drivers. The high mobility of vehicles and the limited coverage of RSU signals necessitate VT migration to ensure service continuity when vehicles leave the signal coverage of RSUs. However, uneven VT task migration might overload some RSUs, which might result in increased service latency, and thus impactive immersive experiences for users. In this paper, we propose a dynamic Unmanned Aerial Vehicle (UAV)-assisted VT migration framework in air-ground integrated networks, where UAVs act as aerial edge servers to assist ground RSUs during VT task offloading. In this framework, we propose a diffusion-based Reinforcement Learning (RL) algorithm, which can efficiently make immersive VT migration decisions in UAV-assisted vehicular networks. To balance the workload of RSUs and improve VT migration quality, we design a novel dynamic path planning algorithm based on a heuristic search strategy for UAVs. Simulation results show that the diffusion-based RL algorithm with UAV-assisted performs better than other baseline schemes.",
    "authors": [
      "Yongju Tong",
      "Jiawen Kang",
      "Junlong Chen",
      "Minrui Xu",
      "Gaolei Li",
      "Weiting Zhang",
      "Xincheng Yan"
    ],
    "url": "http://arxiv.org/abs/2406.05422v1",
    "timestamp": 1717840436,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6a98e0aa-e446-48b2-b122-fab0c6255743": {
    "pk": "6a98e0aa-e446-48b2-b122-fab0c6255743",
    "title": "Reinforcement Learning for Intensity Control: An Application to Choice-Based Network Revenue Management",
    "abstract": "Intensity control is a type of continuous-time dynamic optimization problems with many important applications in Operations Research including queueing and revenue management. In this study, we adapt the reinforcement learning framework to intensity control using choice-based network revenue management as a case study, which is a classical problem in revenue management that features a large state space, a large action space and a continuous time horizon. We show that by utilizing the inherent discretization of the sample paths created by the jump points, a unique and defining feature of intensity control, one does not need to discretize the time horizon in advance, which was believed to be necessary because most reinforcement learning algorithms are designed for discrete-time problems. As a result, the computation can be facilitated and the discretization error is significantly reduced. We lay the theoretical foundation for the Monte Carlo and temporal difference learning algorithms for policy evaluation and develop policy gradient based actor critic algorithms for intensity control. Via a comprehensive numerical study, we demonstrate the benefit of our approach versus other state-of-the-art benchmarks.",
    "authors": [
      "Huiling Meng",
      "Ningyuan Chen",
      "Xuefeng Gao"
    ],
    "url": "http://arxiv.org/abs/2406.05358v1",
    "timestamp": 1717824421,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cd1472fa-305a-47fc-b605-9c15522c2b3c": {
    "pk": "cd1472fa-305a-47fc-b605-9c15522c2b3c",
    "title": "Optimizing Automatic Differentiation with Deep Reinforcement Learning",
    "abstract": "Computing Jacobians with automatic differentiation is ubiquitous in many scientific domains such as machine learning, computational fluid dynamics, robotics and finance. Even small savings in the number of computations or memory usage in Jacobian computations can already incur massive savings in energy consumption and runtime. While there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian. In this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian. Cross-country elimination is a framework for automatic differentiation that phrases Jacobian accumulation as ordered elimination of all vertices on the computational graph where every elimination incurs a certain computational cost. We formulate the search for the optimal elimination order that minimizes the number of necessary multiplications as a single player game which is played by an RL agent. We demonstrate that this method achieves up to 33% improvements over state-of-the-art methods on several relevant tasks taken from diverse domains. Furthermore, we show that these theoretical gains translate into actual runtime improvements by providing a cross-country elimination interpreter in JAX that can efficiently execute the obtained elimination orders.",
    "authors": [
      "Jamie Lohoff",
      "Emre Neftci"
    ],
    "url": "http://arxiv.org/abs/2406.05027v1",
    "timestamp": 1717775073,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b0c3a31f-c096-4eab-8a9d-882f5ae83256": {
    "pk": "b0c3a31f-c096-4eab-8a9d-882f5ae83256",
    "title": "Sim-to-real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning",
    "abstract": "Sim-to-real transfer presents a difficult challenge, where models trained in simulation are to be deployed in the real world. The distribution shift between the two settings leads to biased representations of the perceived real-world environment, and thus to suboptimal predictions. In this work, we tackle the challenge of sim-to-real transfer of reinforcement learning (RL) agents for coverage path planning (CPP). In CPP, the task is for a robot to find a path that visits every point of a confined area. Specifically, we consider the case where the environment is unknown, and the agent needs to plan the path online while mapping the environment. We bridge the sim-to-real gap through a semi-virtual environment with a simulated sensor and obstacles, while including real robot kinematics and real-time aspects. We investigate what level of fine-tuning is needed for adapting to a realistic setting, comparing to an agent trained solely in simulation. We find that a high model inference frequency is sufficient for reducing the sim-to-real gap, while fine-tuning degrades performance initially. By training the model in simulation and deploying it at a high inference frequency, we transfer state-of-the-art results from simulation to the real domain, where direct learning would take in the order of weeks with manual interaction, i.e., would be completely infeasible.",
    "authors": [
      "Arvi Jonnarth",
      "Ola Johansson",
      "Michael Felsberg"
    ],
    "url": "http://arxiv.org/abs/2406.04920v1",
    "timestamp": 1717766659,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "79d4e1f8-5afb-49b1-9e41-e66fba4f2219": {
    "pk": "79d4e1f8-5afb-49b1-9e41-e66fba4f2219",
    "title": "Skill-aware Mutual Information Optimisation for Generalisation in Reinforcement Learning",
    "abstract": "Meta-Reinforcement Learning (Meta-RL) agents can struggle to operate across tasks with varying environmental features that require different optimal skills (i.e., different modes of behaviours). Using context encoders based on contrastive learning to enhance the generalisability of Meta-RL agents is now widely studied but faces challenges such as the requirement for a large sample size, also referred to as the $\\log$-$K$ curse. To improve RL generalisation to different tasks, we first introduce Skill-aware Mutual Information (SaMI), an optimisation objective that aids in distinguishing context embeddings according to skills, thereby equipping RL agents with the ability to identify and execute different skills across tasks. We then propose Skill-aware Noise Contrastive Estimation (SaNCE), a $K$-sample estimator used to optimise the SaMI objective. We provide a framework for equipping an RL agent with SaNCE in practice and conduct experimental validation on modified MuJoCo and Panda-gym benchmarks. We empirically find that RL agents that learn by maximising SaMI achieve substantially improved zero-shot generalisation to unseen tasks. Additionally, the context encoder equipped with SaNCE demonstrates greater robustness to reductions in the number of available samples, thus possessing the potential to overcome the $\\log$-$K$ curse.",
    "authors": [
      "Xuehui Yu",
      "Mhairi Dunion",
      "Xin Li",
      "Stefano V. Albrecht"
    ],
    "url": "http://arxiv.org/abs/2406.04815v1",
    "timestamp": 1717756529,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "172fbc14-52d8-4d36-8d19-d2706b77074f": {
    "pk": "172fbc14-52d8-4d36-8d19-d2706b77074f",
    "title": "Reinforcement Learning and Regret Bounds for Admission Control",
    "abstract": "The expected regret of any reinforcement learning algorithm is lower bounded by $\\Omega\\left(\\sqrt{DXAT}\\right)$ for undiscounted returns, where $D$ is the diameter of the Markov decision process, $X$ the size of the state space, $A$ the size of the action space and $T$ the number of time steps. However, this lower bound is general. A smaller regret can be obtained by taking into account some specific knowledge of the problem structure. In this article, we consider an admission control problem to an $M/M/c/S$ queue with $m$ job classes and class-dependent rewards and holding costs. Queuing systems often have a diameter that is exponential in the buffer size $S$, making the previous lower bound prohibitive for any practical use. We propose an algorithm inspired by UCRL2, and use the structure of the problem to upper bound the expected total regret by $O(S\\log T + \\sqrt{mT \\log T})$ in the finite server case. In the infinite server case, we prove that the dependence of the regret on $S$ disappears.",
    "authors": [
      "Lucas Weber",
      "Ana Bu\u0161i\u0107",
      "Jiamin Zhu"
    ],
    "url": "http://arxiv.org/abs/2406.04766v1",
    "timestamp": 1717751354,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cf62308a-b39c-4b0f-af8b-edea28786719": {
    "pk": "cf62308a-b39c-4b0f-af8b-edea28786719",
    "title": "Probabilistic Perspectives on Error Minimization in Adversarial Reinforcement Learning",
    "abstract": "Deep Reinforcement Learning (DRL) policies are critically vulnerable to adversarial noise in observations, posing severe risks in safety-critical scenarios. For example, a self-driving car receiving manipulated sensory inputs about traffic signs could lead to catastrophic outcomes. Existing strategies to fortify RL algorithms against such adversarial perturbations generally fall into two categories: (a) using regularization methods that enhance robustness by incorporating adversarial loss terms into the value objectives, and (b) adopting \"maximin\" principles, which focus on maximizing the minimum value to ensure robustness. While regularization methods reduce the likelihood of successful attacks, their effectiveness drops significantly if an attack does succeed. On the other hand, maximin objectives, although robust, tend to be overly conservative. To address this challenge, we introduce a novel objective called Adversarial Counterfactual Error (ACoE), which naturally balances optimizing value and robustness against adversarial attacks. To optimize ACoE in a scalable manner in model-free settings, we propose a theoretically justified surrogate objective known as Cumulative-ACoE (C-ACoE). The core idea of optimizing C-ACoE is utilizing the belief about the underlying true state given the adversarially perturbed observation. Our empirical evaluations demonstrate that our method outperforms current state-of-the-art approaches for addressing adversarial RL problems across all established benchmarks (MuJoCo, Atari, and Highway) used in the literature.",
    "authors": [
      "Roman Belaire",
      "Arunesh Sinha",
      "Pradeep Varakantham"
    ],
    "url": "http://arxiv.org/abs/2406.04724v1",
    "timestamp": 1717748064,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "07bc3c4b-003b-44d4-89bf-91dd281aac76": {
    "pk": "07bc3c4b-003b-44d4-89bf-91dd281aac76",
    "title": "Optimization of geological carbon storage operations with multimodal latent dynamic model and deep reinforcement learning",
    "abstract": "Maximizing storage performance in geological carbon storage (GCS) is crucial for commercial deployment, but traditional optimization demands resource-intensive simulations, posing computational challenges. This study introduces the multimodal latent dynamic (MLD) model, a deep learning framework for fast flow prediction and well control optimization in GCS. The MLD model includes a representation module for compressed latent representations, a transition module for system state evolution, and a prediction module for flow responses. A novel training strategy combining regression loss and joint-embedding consistency loss enhances temporal consistency and multi-step prediction accuracy. Unlike existing models, the MLD supports diverse input modalities, allowing comprehensive data interactions. The MLD model, resembling a Markov decision process (MDP), can train deep reinforcement learning agents, specifically using the soft actor-critic (SAC) algorithm, to maximize net present value (NPV) through continuous interactions. The approach outperforms traditional methods, achieving the highest NPV while reducing computational resources by over 60%. It also demonstrates strong generalization performance, providing improved decisions for new scenarios based on knowledge from previous ones.",
    "authors": [
      "Zhongzheng Wang",
      "Yuntian Chen",
      "Guodong Chen",
      "Dongxiao Zhang"
    ],
    "url": "http://arxiv.org/abs/2406.04575v1",
    "timestamp": 1717723821,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "46d47cb5-1437-4885-9f77-18f7ae51b89f": {
    "pk": "46d47cb5-1437-4885-9f77-18f7ae51b89f",
    "title": "Entanglement engineering of optomechanical systems by reinforcement learning",
    "abstract": "Entanglement is fundamental to quantum information science and technology, yet controlling and manipulating entanglement -- so-called entanglement engineering -- for arbitrary quantum systems remains a formidable challenge. There are two difficulties: the fragility of quantum entanglement and its experimental characterization. We develop a deep reinforcement-learning (RL) approach to entanglement engineering, in which feedback control together with weak continuous measurement and partial state observation is exploited to generate and maintain desired entanglement. We employ quantum optomechanical systems with linear or nonlinear photon-phonon interactions to demonstrate the workings of our machine-learning-based entanglement engineering protocol. In particular, the RL agent sequentially interacts with one or multiple parallel quantum optomechanical environments, collects trajectories, and updates the policy to maximize the accumulated reward to create and stabilize quantum entanglement over an arbitrary amount of time. The machine-learning-based control principle is applicable to entanglement engineering of experimental quantum systems in general.",
    "authors": [
      "Li-Li Ye",
      "Christian Arenz",
      "Kanu Sinha",
      "Joseph M. Lukens",
      "Ying-Cheng Lai"
    ],
    "url": "http://arxiv.org/abs/2406.04550v1",
    "timestamp": 1717716336,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "44dec933-5fe6-4408-9cc8-15b0b531ac6e": {
    "pk": "44dec933-5fe6-4408-9cc8-15b0b531ac6e",
    "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories",
    "abstract": "Training autonomous agents with sparse rewards is a long-standing problem in online reinforcement learning (RL), due to low data efficiency. Prior work overcomes this challenge by extracting useful knowledge from offline data, often accomplished through the learning of action distribution from offline data and utilizing the learned distribution to facilitate online RL. However, since the offline data are given and fixed, the extracted knowledge is inherently limited, making it difficult to generalize to new tasks. We propose a novel approach that leverages offline data to learn a generative diffusion model, coined as Adaptive Trajectory Diffuser (ATraDiff). This model generates synthetic trajectories, serving as a form of data augmentation and consequently enhancing the performance of online RL methods. The key strength of our diffuser lies in its adaptability, allowing it to effectively handle varying trajectory lengths and mitigate distribution shifts between online and offline data. Because of its simplicity, ATraDiff seamlessly integrates with a wide spectrum of RL methods. Empirical evaluation shows that ATraDiff consistently achieves state-of-the-art performance across a variety of environments, with particularly pronounced improvements in complicated settings. Our code and demo video are available at https://atradiff.github.io .",
    "authors": [
      "Qianlan Yang",
      "Yu-Xiong Wang"
    ],
    "url": "http://arxiv.org/abs/2406.04323v1",
    "timestamp": 1717696695,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "da128df6-9004-4ef4-9dd8-30639d0c7444": {
    "pk": "da128df6-9004-4ef4-9dd8-30639d0c7444",
    "title": "MARLander: A Local Path Planning for Drone Swarms using Multiagent Deep Reinforcement Learning",
    "abstract": "Achieving safe and precise landings for a swarm of drones poses a significant challenge, primarily attributed to conventional control and planning methods. This paper presents the implementation of multi-agent deep reinforcement learning (MADRL) techniques for the precise landing of a drone swarm at relocated target locations. The system is trained in a realistic simulated environment with a maximum velocity of 3 m/s in training spaces of 4 x 4 x 4 m and deployed utilizing Crazyflie drones with a Vicon indoor localization system. The experimental results revealed that the proposed approach achieved a landing accuracy of 2.26 cm on stationary and 3.93 cm on moving platforms surpassing a baseline method used with a Proportional-integral-derivative (PID) controller with an Artificial Potential Field (APF). This research highlights drone landing technologies that eliminate the need for analytical centralized systems, potentially offering scalability and revolutionizing applications in logistics, safety, and rescue missions.",
    "authors": [
      "Demetros Aschu",
      "Robinroy Peter",
      "Sausar Karaf",
      "Aleksey Fedoseev",
      "Dzmitry Tsetserukou"
    ],
    "url": "http://arxiv.org/abs/2406.04159v1",
    "timestamp": 1717687155,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d33487b3-596e-4c53-b0ab-97f0899c3bc6": {
    "pk": "d33487b3-596e-4c53-b0ab-97f0899c3bc6",
    "title": "Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning",
    "abstract": "Current approaches to model-based offline Reinforcement Learning (RL) often incorporate uncertainty-based reward penalization to address the distributional shift problem. While these approaches have achieved some success, we argue that this penalization introduces excessive conservatism, potentially resulting in suboptimal policies through underestimation. We identify as an important cause of over-penalization the lack of a reliable uncertainty estimator capable of propagating uncertainties in the Bellman operator. The common approach to calculating the penalty term relies on sampling-based uncertainty estimation, resulting in high variance. To address this challenge, we propose a novel method termed Moment Matching Offline Model-Based Policy Optimization (MOMBO). MOMBO learns a Q-function using moment matching, which allows us to deterministically propagate uncertainties through the Q-function. We evaluate MOMBO's performance across various environments and demonstrate empirically that MOMBO is a more stable and sample-efficient approach.",
    "authors": [
      "Abdullah Akg\u00fcl",
      "Manuel Hau\u00dfmann",
      "Melih Kandemir"
    ],
    "url": "http://arxiv.org/abs/2406.04088v1",
    "timestamp": 1717682321,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7d7f0115-deba-4f91-980a-4685555044e3": {
    "pk": "7d7f0115-deba-4f91-980a-4685555044e3",
    "title": "Bootstrapping Expectiles in Reinforcement Learning",
    "abstract": "Many classic Reinforcement Learning (RL) algorithms rely on a Bellman operator, which involves an expectation over the next states, leading to the concept of bootstrapping. To introduce a form of pessimism, we propose to replace this expectation with an expectile. In practice, this can be very simply done by replacing the $L_2$ loss with a more general expectile loss for the critic. Introducing pessimism in RL is desirable for various reasons, such as tackling the overestimation problem (for which classic solutions are double Q-learning or the twin-critic approach of TD3) or robust RL (where transitions are adversarial). We study empirically these two cases. For the overestimation problem, we show that the proposed approach, ExpectRL, provides better results than a classic twin-critic. On robust RL benchmarks, involving changes of the environment, we show that our approach is more robust than classic RL algorithms. We also introduce a variation of ExpectRL combined with domain randomization which is competitive with state-of-the-art robust RL agents. Eventually, we also extend \\ExpectRL with a mechanism for choosing automatically the expectile value, that is the degree of pessimism",
    "authors": [
      "Pierre Clavier",
      "Emmanuel Rachelson",
      "Erwan Le Pennec",
      "Matthieu Geist"
    ],
    "url": "http://arxiv.org/abs/2406.04081v1",
    "timestamp": 1717681899,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "649e9fb8-9b05-45ab-b4b1-2d6fa3632d9d": {
    "pk": "649e9fb8-9b05-45ab-b4b1-2d6fa3632d9d",
    "title": "Spatio-temporal Early Prediction based on Multi-objective Reinforcement Learning",
    "abstract": "Accuracy and timeliness are indeed often conflicting goals in prediction tasks. Premature predictions may yield a higher rate of false alarms, whereas delaying predictions to gather more information can render them too late to be useful. In applications such as wildfires, crimes, and traffic jams, timely predictions are vital for safeguarding human life and property. Consequently, finding a balance between accuracy and timeliness is crucial. In this paper, we propose a spatio-temporal early prediction model based on Multi-Objective reinforcement learning that can either implement an optimal policy given a preference or infer the preference based on a small number of samples. The model addresses two primary challenges: 1) enhancing the accuracy of early predictions and 2) providing the optimal policy for determining the most suitable prediction time for each area. Our method demonstrates superior performance on three large-scale real-world datasets, surpassing existing methods in early spatio-temporal prediction tasks.",
    "authors": [
      "Wei Shao",
      "Yufan Kang",
      "Ziyan Peng",
      "Xiao Xiao",
      "Lei Wang",
      "Yuhui Yang",
      "Flora D Salim"
    ],
    "url": "http://arxiv.org/abs/2406.04035v2",
    "timestamp": 1717679031,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d58fa659-7391-4080-b916-aedcd095e4f9": {
    "pk": "d58fa659-7391-4080-b916-aedcd095e4f9",
    "title": "HackAtari: Atari Learning Environments for Robust and Continual Reinforcement Learning",
    "abstract": "Artificial agents' adaptability to novelty and alignment with intended behavior is crucial for their effective deployment. Reinforcement learning (RL) leverages novelty as a means of exploration, yet agents often struggle to handle novel situations, hindering generalization. To address these issues, we propose HackAtari, a framework introducing controlled novelty to the most common RL benchmark, the Atari Learning Environment. HackAtari allows us to create novel game scenarios (including simplification for curriculum learning), to swap the game elements' colors, as well as to introduce different reward signals for the agent. We demonstrate that current agents trained on the original environments include robustness failures, and evaluate HackAtari's efficacy in enhancing RL agents' robustness and aligning behavior through experiments using C51 and PPO. Overall, HackAtari can be used to improve the robustness of current and future RL algorithms, allowing Neuro-Symbolic RL, curriculum RL, causal RL, as well as LLM-driven RL. Our work underscores the significance of developing interpretable in RL agents.",
    "authors": [
      "Quentin Delfosse",
      "Jannis Bl\u00fcml",
      "Bjarne Gregori",
      "Kristian Kersting"
    ],
    "url": "http://arxiv.org/abs/2406.03997v1",
    "timestamp": 1717676225,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6bfd774e-207f-4f76-b22e-1444a850a428": {
    "pk": "6bfd774e-207f-4f76-b22e-1444a850a428",
    "title": "AC4MPC: Actor-Critic Reinforcement Learning for Nonlinear Model Predictive Control",
    "abstract": "\\Ac{MPC} and \\ac{RL} are two powerful control strategies with, arguably, complementary advantages. In this work, we show how actor-critic \\ac{RL} techniques can be leveraged to improve the performance of \\ac{MPC}. The \\ac{RL} critic is used as an approximation of the optimal value function, and an actor roll-out provides an initial guess for primal variables of the \\ac{MPC}. A parallel control architecture is proposed where each \\ac{MPC} instance is solved twice for different initial guesses. Besides the actor roll-out initialization, a shifted initialization from the previous solution is used. Thereafter, the actor and the critic are again used to approximately evaluate the infinite horizon cost of these trajectories. The control actions from the lowest-cost trajectory are applied to the system at each time step. We establish that the proposed algorithm is guaranteed to outperform the original \\ac{RL} policy plus an error term that depends on the accuracy of the critic and decays with the horizon length of the \\ac{MPC} formulation. Moreover, we do not require globally optimal solutions for these guarantees to hold. The approach is demonstrated on an illustrative toy example and an \\ac{AD} overtaking scenario.",
    "authors": [
      "Rudolf Reiter",
      "Andrea Ghezzi",
      "Katrin Baumg\u00e4rtner",
      "Jasper Hoffmann",
      "Robert D. McAllister",
      "Moritz Diehl"
    ],
    "url": "http://arxiv.org/abs/2406.03995v1",
    "timestamp": 1717676151,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "96daaae2-b7c3-4670-9667-b57a90798694": {
    "pk": "96daaae2-b7c3-4670-9667-b57a90798694",
    "title": "Mini Honor of Kings: A Lightweight Environment for Multi-Agent Reinforcement Learning",
    "abstract": "Games are widely used as research environments for multi-agent reinforcement learning (MARL), but they pose three significant challenges: limited customization, high computational demands, and oversimplification. To address these issues, we introduce the first publicly available map editor for the popular mobile game Honor of Kings and design a lightweight environment, Mini Honor of Kings (Mini HoK), for researchers to conduct experiments. Mini HoK is highly efficient, allowing experiments to be run on personal PCs or laptops while still presenting sufficient challenges for existing MARL algorithms. We have tested our environment on common MARL algorithms and demonstrated that these algorithms have yet to find optimal solutions within this environment. This facilitates the dissemination and advancement of MARL methods within the research community. Additionally, we hope that more researchers will leverage the Honor of Kings map editor to develop innovative and scientifically valuable new maps. Our code and user manual are available at: https://github.com/tencent-ailab/mini-hok.",
    "authors": [
      "Lin Liu",
      "Jian Zhao",
      "Cheng Hu",
      "Zhengtao Cao",
      "Youpeng Zhao",
      "Zhenbin Ye",
      "Meng Meng",
      "Wenjun Wang",
      "Zhaofeng He",
      "Houqiang Li",
      "Xia Lin",
      "Lanxiao Huang"
    ],
    "url": "http://arxiv.org/abs/2406.03978v1",
    "timestamp": 1717674153,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.MA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5247b411-619e-4981-b47b-162093dae64b": {
    "pk": "5247b411-619e-4981-b47b-162093dae64b",
    "title": "Breeding Programs Optimization with Reinforcement Learning",
    "abstract": "Crop breeding is crucial in improving agricultural productivity while potentially decreasing land usage, greenhouse gas emissions, and water consumption. However, breeding programs are challenging due to long turnover times, high-dimensional decision spaces, long-term objectives, and the need to adapt to rapid climate change. This paper introduces the use of Reinforcement Learning (RL) to optimize simulated crop breeding programs. RL agents are trained to make optimal crop selection and cross-breeding decisions based on genetic information. To benchmark RL-based breeding algorithms, we introduce a suite of Gym environments. The study demonstrates the superiority of RL techniques over standard practices in terms of genetic gain when simulated in silico using real-world genomic maize data.",
    "authors": [
      "Omar G. Younis",
      "Luca Corinzia",
      "Ioannis N. Athanasiadis",
      "Andreas Krause",
      "Joachim M. Buhmann",
      "Matteo Turchetta"
    ],
    "url": "http://arxiv.org/abs/2406.03932v1",
    "timestamp": 1717669071,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "58aa9f5c-4294-44a1-803d-246d0c27015f": {
    "pk": "58aa9f5c-4294-44a1-803d-246d0c27015f",
    "title": "GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning Algorithms Based on Reduced Order Markov Decision Process Model",
    "abstract": "Although deep reinforcement learning has demonstrated impressive achievements in controlling various autonomous systems, e.g., autonomous vehicles or humanoid robots, its inherent reliance on random exploration raises safety concerns in their real-world applications. To improve system safety during the learning process, a variety of Safe Reinforcement Learning (SRL) algorithms have been proposed, which usually incorporate safety constraints within the Constrained Markov Decision Process (CMDP) framework. However, the efficacy of these SRL algorithms often relies on accurate function approximations, a task that is notably challenging to accomplish in the early learning stages due to data insufficiency. To address this problem, we introduce a Genralizable Safety enhancer (GenSafe) in this work. Leveraging model order reduction techniques, we first construct a Reduced Order Markov Decision Process (ROMDP) as a low-dimensional proxy for the original cost function in CMDP. Then, by solving ROMDP-based constraints that are reformulated from the original cost constraints, the proposed GenSafe refines the actions taken by the agent to enhance the possibility of constraint satisfaction. Essentially, GenSafe acts as an additional safety layer for SRL algorithms, offering broad compatibility across diverse SRL approaches. The performance of GenSafe is examined on multiple SRL benchmark problems. The results show that, it is not only able to improve the safety performance, especially in the early learning phases, but also to maintain the task performance at a satisfactory level.",
    "authors": [
      "Zhehua Zhou",
      "Xuan Xie",
      "Jiayang Song",
      "Zhan Shu",
      "Lei Ma"
    ],
    "url": "http://arxiv.org/abs/2406.03912v1",
    "timestamp": 1717667490,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2a526c2a-6112-4807-b68a-89733d89154b": {
    "pk": "2a526c2a-6112-4807-b68a-89733d89154b",
    "title": "Exploring Pessimism and Optimism Dynamics in Deep Reinforcement Learning",
    "abstract": "Off-policy actor-critic algorithms have shown promise in deep reinforcement learning for continuous control tasks. Their success largely stems from leveraging pessimistic state-action value function updates, which effectively address function approximation errors and improve performance. However, such pessimism can lead to under-exploration, constraining the agent's ability to explore/refine its policies. Conversely, optimism can counteract under-exploration, but it also carries the risk of excessive risk-taking and poor convergence if not properly balanced. Based on these insights, we introduce Utility Soft Actor-Critic (USAC), a novel framework within the actor-critic paradigm that enables independent control over the degree of pessimism/optimism for both the actor and the critic via interpretable parameters. USAC adapts its exploration strategy based on the uncertainty of critics through a utility function that allows us to balance between pessimism and optimism separately. By going beyond binary choices of optimism and pessimism, USAC represents a significant step towards achieving balance within off-policy actor-critic algorithms. Our experiments across various continuous control problems show that the degree of pessimism or optimism depends on the nature of the task. Furthermore, we demonstrate that USAC can outperform state-of-the-art algorithms for appropriately configured pessimism/optimism parameters.",
    "authors": [
      "Bahareh Tasdighi",
      "Nicklas Werge",
      "Yi-Shan Wu",
      "Melih Kandemir"
    ],
    "url": "http://arxiv.org/abs/2406.03890v1",
    "timestamp": 1717665962,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "eff444e9-0a02-457f-9403-c1646c2ef080": {
    "pk": "eff444e9-0a02-457f-9403-c1646c2ef080",
    "title": "GOOSE: Goal-Conditioned Reinforcement Learning for Safety-Critical Scenario Generation",
    "abstract": "Scenario-based testing is considered state-of-the-art for verifying and validating Advanced Driver Assistance Systems (ADASs) and Automated Driving Systems (ADSs). However, the practical application of scenario-based testing requires an efficient method to generate or collect the scenarios that are needed for the safety assessment. In this paper, we propose Goal-conditioned Scenario Generation (GOOSE), a goal-conditioned reinforcement learning (RL) approach that automatically generates safety-critical scenarios to challenge ADASs or ADSs. In order to simultaneously set up and optimize scenarios, we propose to control vehicle trajectories at the scenario level. Each step in the RL framework corresponds to a scenario simulation. We use Non-Uniform Rational B-Splines (NURBS) for trajectory modeling. To guide the goal-conditioned agent, we formulate test-specific, constraint-based goals inspired by the OpenScenario Domain Specific Language(DSL). Through experiments conducted on multiple pre-crash scenarios derived from UN Regulation No. 157 for Active Lane Keeping Systems (ALKS), we demonstrate the effectiveness of GOOSE in generating scenarios that lead to safety-critical events.",
    "authors": [
      "Joshua Ransiek",
      "Johannes Plaum",
      "Jacob Langner",
      "Eric Sax"
    ],
    "url": "http://arxiv.org/abs/2406.03870v1",
    "timestamp": 1717664348,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "439095cf-5fec-44cc-af20-ee6dca7783fb": {
    "pk": "439095cf-5fec-44cc-af20-ee6dca7783fb",
    "title": "Behavior-Targeted Attack on Reinforcement Learning with Limited Access to Victim's Policy",
    "abstract": "This study considers the attack on reinforcement learning agents where the adversary aims to control the victim's behavior as specified by the adversary by adding adversarial modifications to the victim's state observation. While some attack methods reported success in manipulating the victim agent's behavior, these methods often rely on environment-specific heuristics. In addition, all existing attack methods require white-box access to the victim's policy. In this study, we propose a novel method for manipulating the victim agent in the black-box (i.e., the adversary is allowed to observe the victim's state and action only) and no-box (i.e., the adversary is allowed to observe the victim's state only) setting without requiring environment-specific heuristics. Our attack method is formulated as a bi-level optimization problem that is reduced to a distribution matching problem and can be solved by an existing imitation learning algorithm in the black-box and no-box settings. Empirical evaluations on several reinforcement learning benchmarks show that our proposed method has superior attack performance to baselines.",
    "authors": [
      "Shojiro Yamabe",
      "Kazuto Fukuchi",
      "Ryoma Senda",
      "Jun Sakuma"
    ],
    "url": "http://arxiv.org/abs/2406.03862v1",
    "timestamp": 1717663791,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2b3416b9-49cb-4ee5-b423-1d14f8d0a1a2": {
    "pk": "2b3416b9-49cb-4ee5-b423-1d14f8d0a1a2",
    "title": "Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking",
    "abstract": "Continuous action spaces in reinforcement learning (RL) are commonly defined as interval sets. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using Proximal Policy Optimization (PPO), we evaluate our methods on three control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.",
    "authors": [
      "Roland Stolz",
      "Hanna Krasowski",
      "Jakob Thumm",
      "Michael Eichelbeck",
      "Philipp Gassert",
      "Matthias Althoff"
    ],
    "url": "http://arxiv.org/abs/2406.03704v1",
    "timestamp": 1717642516,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9ef8caa2-536d-4174-999b-3795b8ce88bc": {
    "pk": "9ef8caa2-536d-4174-999b-3795b8ce88bc",
    "title": "BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning",
    "abstract": "Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment. In this paper, we present a novel generative model, BindGPT which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site. Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step. We pretrain BindGPT on a large-scale dataset and fine-tune it with reinforcement learning using scores from external simulation software. We demonstrate how a single pretrained language model can serve at the same time as a 3D molecular generative model, conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator. Notably, the model does not make any representational equivariance assumptions about the domain of generation. We show how such simple conceptual approach combined with pretraining and scaling can perform on par or better than the current best specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.",
    "authors": [
      "Artem Zholus",
      "Maksim Kuznetsov",
      "Roman Schutski",
      "Rim Shayakhmetov",
      "Daniil Polykovskiy",
      "Sarath Chandar",
      "Alex Zhavoronkov"
    ],
    "url": "http://arxiv.org/abs/2406.03686v1",
    "timestamp": 1717639850,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bbc0179a-391f-4e5f-b8e7-bc0bd5b43d63": {
    "pk": "bbc0179a-391f-4e5f-b8e7-bc0bd5b43d63",
    "title": "Towards Dynamic Trend Filtering through Trend Point Detection with Reinforcement Learning",
    "abstract": "Trend filtering simplifies complex time series data by applying smoothness to filter out noise while emphasizing proximity to the original data. However, existing trend filtering methods fail to reflect abrupt changes in the trend due to `approximateness,' resulting in constant smoothness. This approximateness uniformly filters out the tail distribution of time series data, characterized by extreme values, including both abrupt changes and noise. In this paper, we propose Trend Point Detection formulated as a Markov Decision Process (MDP), a novel approach to identifying essential points that should be reflected in the trend, departing from approximations. We term these essential points as Dynamic Trend Points (DTPs) and extract trends by interpolating them. To identify DTPs, we utilize Reinforcement Learning (RL) within a discrete action space and a forecasting sum-of-squares loss function as a reward, referred to as the Dynamic Trend Filtering network (DTF-net). DTF-net integrates flexible noise filtering, preserving critical original subsequences while removing noise as required for other subsequences. We demonstrate that DTF-net excels at capturing abrupt changes compared to other trend filtering algorithms and enhances forecasting performance, as abrupt changes are predicted rather than smoothed out.",
    "authors": [
      "Jihyeon Seong",
      "Sekwang Oh",
      "Jaesik Choi"
    ],
    "url": "http://arxiv.org/abs/2406.03665v1",
    "timestamp": 1717635022,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d0611ce1-da48-41ba-ab25-a6fb5f9d886a": {
    "pk": "d0611ce1-da48-41ba-ab25-a6fb5f9d886a",
    "title": "Inductive Generalization in Reinforcement Learning from Specifications",
    "abstract": "We present a novel inductive generalization framework for RL from logical specifications. Many interesting tasks in RL environments have a natural inductive structure. These inductive tasks have similar overarching goals but they differ inductively in low-level predicates and distributions. We present a generalization procedure that leverages this inductive relationship to learn a higher-order function, a policy generator, that generates appropriately adapted policies for instances of an inductive task in a zero-shot manner. An evaluation of the proposed approach on a set of challenging control benchmarks demonstrates the promise of our framework in generalizing to unseen policies for long-horizon tasks.",
    "authors": [
      "Vignesh Subramanian",
      "Rohit Kushwah",
      "Subhajit Roy",
      "Suguman Bansal"
    ],
    "url": "http://arxiv.org/abs/2406.03651v1",
    "timestamp": 1717628808,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3da1dd11-faa2-4a0a-a14e-6e11ad670968": {
    "pk": "3da1dd11-faa2-4a0a-a14e-6e11ad670968",
    "title": "Knowledge-Infused Legal Wisdom: Navigating LLM Consultation through the Lens of Diagnostics and Positive-Unlabeled Reinforcement Learning",
    "abstract": "The integration of generative Large Language Models (LLMs) into various applications, including the legal domain, has been accelerated by their expansive and versatile nature. However, when facing a legal case, users without a legal background often struggle to formulate professional queries and may inadvertently overlook critical legal factors when presenting their case narrative to LLMs. To address this issue, we propose the Diagnostic Legal Large Language Model (D3LM), which utilizes adaptive lawyer-like diagnostic questions to collect additional case information and then provides high-quality feedback. D3LM incorporates an innovative graph-based Positive-Unlabeled Reinforcement Learning (PURL) algorithm, enabling the generation of critical questions and enhancing user-LLM interactions. Moreover, an integrated LLM-based stopping criterion facilitates precise Court Views Generation (CVG). Our research also introduces a new English-language CVG dataset based on the US case law database, enriching the realm of LLM research and deployment with a vital dimension. D3LM surpasses classical LLMs by delivering outstanding performance and a remarkable user experience in the legal domain.",
    "authors": [
      "Yang Wu",
      "Chenghao Wang",
      "Ece Gumusel",
      "Xiaozhong Liu"
    ],
    "url": "http://arxiv.org/abs/2406.03600v1",
    "timestamp": 1717616855,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e3f2fa41-bca8-4122-ac1e-39e9ca1e1b1a": {
    "pk": "e3f2fa41-bca8-4122-ac1e-39e9ca1e1b1a",
    "title": "LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback",
    "abstract": "Ensuring that online discussions are civil and productive is a major challenge for social media platforms. Such platforms usually rely both on users and on automated detection tools to flag inappropriate arguments of other users, which moderators then review. However, this kind of post-hoc moderation is expensive and time-consuming, and moderators are often overwhelmed by the amount and severity of flagged content. Instead, a promising alternative is to prevent negative behavior during content creation. This paper studies how inappropriate language in arguments can be computationally mitigated. We propose a reinforcement learning-based rewriting approach that balances content preservation and appropriateness based on existing classifiers, prompting an instruction-finetuned large language model (LLM) as our initial policy. Unlike related style transfer tasks, rewriting inappropriate arguments allows deleting and adding content permanently. It is therefore tackled on document level rather than sentence level. We evaluate different weighting schemes for the reward function in both absolute and relative human assessment studies. Systematic experiments on non-parallel data provide evidence that our approach can mitigate the inappropriateness of arguments while largely preserving their content. It significantly outperforms competitive baselines, including few-shot learning, prompting, and humans.",
    "authors": [
      "Timon Ziegenbein",
      "Gabriella Skitalinskaya",
      "Alireza Bayat Makou",
      "Henning Wachsmuth"
    ],
    "url": "http://arxiv.org/abs/2406.03363v1",
    "timestamp": 1717600688,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1877fc1f-dce0-48bd-afc2-8df278b3a22a": {
    "pk": "1877fc1f-dce0-48bd-afc2-8df278b3a22a",
    "title": "UDQL: Bridging The Gap between MSE Loss and The Optimal Value Function in Offline Reinforcement Learning",
    "abstract": "The Mean Square Error (MSE) is commonly utilized to estimate the solution of the optimal value function in the vast majority of offline reinforcement learning (RL) models and has achieved outstanding performance. However, we find that its principle can lead to overestimation phenomenon for the value function. In this paper, we first theoretically analyze overestimation phenomenon led by MSE and provide the theoretical upper bound of the overestimated error. Furthermore, to address it, we propose a novel Bellman underestimated operator to counteract overestimation phenomenon and then prove its contraction characteristics. At last, we propose the offline RL algorithm based on underestimated operator and diffusion policy model. Extensive experimental results on D4RL tasks show that our method can outperform state-of-the-art offline RL algorithms, which demonstrates that our theoretical analysis and underestimation way are effective for offline RL tasks.",
    "authors": [
      "Yu Zhang",
      "Rui Yu",
      "Zhipeng Yao",
      "Wenyuan Zhang",
      "Jun Wang",
      "Liming Zhang"
    ],
    "url": "http://arxiv.org/abs/2406.03324v1",
    "timestamp": 1717598262,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7c7e8158-5ae6-4171-bc23-daf48377c4d9": {
    "pk": "7c7e8158-5ae6-4171-bc23-daf48377c4d9",
    "title": "Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning",
    "abstract": "Second-order information is valuable for many applications but challenging to compute. Several works focus on computing or approximating Hessian diagonals, but even this simplification introduces significant additional costs compared to computing a gradient. In the absence of efficient exact computation schemes for Hessian diagonals, we revisit an early approximation scheme proposed by Becker and LeCun (1989, BL89), which has a cost similar to gradients and appears to have been overlooked by the community. We introduce HesScale, an improvement over BL89, which adds negligible extra computation. On small networks, we find that this improvement is of higher quality than all alternatives, even those with theoretical guarantees, such as unbiasedness, while being much cheaper to compute. We use this insight in reinforcement learning problems where small networks are used and demonstrate HesScale in second-order optimization and scaling the step-size parameter. In our experiments, HesScale optimizes faster than existing methods and improves stability through step-size scaling. These findings are promising for scaling second-order methods in larger models in the future.",
    "authors": [
      "Mohamed Elsayed",
      "Homayoon Farrahi",
      "Felix Dangel",
      "A. Rupam Mahmood"
    ],
    "url": "http://arxiv.org/abs/2406.03276v1",
    "timestamp": 1717595600,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fbdc4b5e-62d7-438e-8ecc-6fb68efdbdbb": {
    "pk": "fbdc4b5e-62d7-438e-8ecc-6fb68efdbdbb",
    "title": "Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning",
    "abstract": "Causal dynamics learning has recently emerged as a promising approach to enhancing robustness in reinforcement learning (RL). Typically, the goal is to build a dynamics model that makes predictions based on the causal relationships among the entities. Despite the fact that causal connections often manifest only under certain contexts, existing approaches overlook such fine-grained relationships and lack a detailed understanding of the dynamics. In this work, we propose a novel dynamics model that infers fine-grained causal structures and employs them for prediction, leading to improved robustness in RL. The key idea is to jointly learn the dynamics model with a discrete latent variable that quantizes the state-action space into subgroups. This leads to recognizing meaningful context that displays sparse dependencies, where causal structures are learned for each subgroup throughout the training. Experimental results demonstrate the robustness of our method to unseen states and locally spurious correlations in downstream tasks where fine-grained causal reasoning is crucial. We further illustrate the effectiveness of our subgroup-based approach with quantization in discovering fine-grained causal relationships compared to prior methods.",
    "authors": [
      "Inwoo Hwang",
      "Yunhyeok Kwak",
      "Suhyung Choi",
      "Byoung-Tak Zhang",
      "Sanghack Lee"
    ],
    "url": "http://arxiv.org/abs/2406.03234v1",
    "timestamp": 1717593238,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7c23d4a6-7440-4574-a7c1-e42d71f8ffaa": {
    "pk": "7c23d4a6-7440-4574-a7c1-e42d71f8ffaa",
    "title": "Object Manipulation in Marine Environments using Reinforcement Learning",
    "abstract": "Performing intervention tasks in the maritime domain is crucial for safety and operational efficiency. The unpredictable and dynamic marine environment makes the intervention tasks such as object manipulation extremely challenging. This study proposes a robust solution for object manipulation from a dock in the presence of disturbances caused by sea waves. To tackle this challenging problem, we apply a deep reinforcement learning (DRL) based algorithm called Soft. Actor-Critic (SAC). SAC employs an actor-critic framework; the actors learn a policy that minimizes an objective function while the critic evaluates the learned policy and provides feedback to guide the actor-learning process. We trained the agent using the PyBullet dynamic simulator and tested it in a realistic simulation environment called MBZIRC maritime simulator. This simulator allows the simulation of different wave conditions according to the World Meteorological Organization (WMO) sea state code. Simulation results demonstrate a high success rate in retrieving the objects from the dock. The trained agent achieved an 80 percent success rate when applied in the simulation environment in the presence of waves characterized by sea state 2, according to the WMO sea state code",
    "authors": [
      "Ahmed Nader",
      "Muhayy Ud Din",
      "Mughni Irfan",
      "Irfan Hussain"
    ],
    "url": "http://arxiv.org/abs/2406.03223v1",
    "timestamp": 1717592496,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "616d3dc5-8616-49ca-b2b4-dd84c76e76e8": {
    "pk": "616d3dc5-8616-49ca-b2b4-dd84c76e76e8",
    "title": "DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays",
    "abstract": "Classic reinforcement learning (RL) frequently confronts challenges in tasks involving delays, which cause a mismatch between received observations and subsequent actions, thereby deviating from the Markov assumption. Existing methods usually tackle this issue with end-to-end solutions using state augmentation. However, these black-box approaches often involve incomprehensible processes and redundant information in the information states, causing instability and potentially undermining the overall performance. To alleviate the delay challenges in RL, we propose $\\textbf{DEER (Delay-resilient Encoder-Enhanced RL)}$, a framework designed to effectively enhance the interpretability and address the random delay issues. DEER employs a pretrained encoder to map delayed states, along with their variable-length past action sequences resulting from different delays, into hidden states, which is trained on delay-free environment datasets. In a variety of delayed scenarios, the trained encoder can seamlessly integrate with standard RL algorithms without requiring additional modifications and enhance the delay-solving capability by simply adapting the input dimension of the original algorithms. We evaluate DEER through extensive experiments on Gym and Mujoco environments. The results confirm that DEER is superior to state-of-the-art RL algorithms in both constant and random delay settings.",
    "authors": [
      "Bo Xia",
      "Yilun Kong",
      "Yongzhe Chang",
      "Bo Yuan",
      "Zhiheng Li",
      "Xueqian Wang",
      "Bin Liang"
    ],
    "url": "http://arxiv.org/abs/2406.03102v1",
    "timestamp": 1717580726,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "96350229-a2dc-44a3-a174-b35814635822": {
    "pk": "96350229-a2dc-44a3-a174-b35814635822",
    "title": "\"Give Me an Example Like This\": Episodic Active Reinforcement Learning from Demonstrations",
    "abstract": "Reinforcement Learning (RL) has achieved great success in sequential decision-making problems, but often at the cost of a large number of agent-environment interactions. To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) introduce external expert demonstrations to facilitate agent exploration during the learning process. In practice, these demonstrations, which are often collected from human users, are costly and hence often constrained to a limited amount. How to select the best set of human demonstrations that is most beneficial for learning therefore becomes a major concern. This paper presents EARLY (Episodic Active Learning from demonstration querY), an algorithm that enables a learning agent to generate optimized queries of expert demonstrations in a trajectory-based feature space. Based on a trajectory-level estimate of uncertainty in the agent's current policy, EARLY determines the optimized timing and content for feature-based queries. By querying episodic demonstrations as opposed to isolated state-action pairs, EARLY improves the human teaching experience and achieves better learning performance. We validate the effectiveness of our method in three simulated navigation tasks of increasing difficulty. The results show that our method is able to achieve expert-level performance for all three tasks with convergence over 30\\% faster than other baseline methods when demonstrations are generated by simulated oracle policies. The results of a follow-up pilot user study (N=18) further validate that our method can still maintain a significantly better convergence in the case of human expert demonstrators while achieving a better user experience in perceived task load and consuming significantly less human time.",
    "authors": [
      "Muhan Hou",
      "Koen Hindriks",
      "A. E. Eiben",
      "Kim Baraka"
    ],
    "url": "http://arxiv.org/abs/2406.03069v2",
    "timestamp": 1717577541,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ba7c0fff-ae23-4e90-81d8-9208c9312950": {
    "pk": "ba7c0fff-ae23-4e90-81d8-9208c9312950",
    "title": "Representation Learning For Efficient Deep Multi-Agent Reinforcement Learning",
    "abstract": "Sample efficiency remains a key challenge in multi-agent reinforcement learning (MARL). A promising approach is to learn a meaningful latent representation space through auxiliary learning objectives alongside the MARL objective to aid in learning a successful control policy. In our work, we present MAPO-LSO (Multi-Agent Policy Optimization with Latent Space Optimization) which applies a form of comprehensive representation learning devised to supplement MARL training. Specifically, MAPO-LSO proposes a multi-agent extension of transition dynamics reconstruction and self-predictive learning that constructs a latent state optimization scheme that can be trivially extended to current state-of-the-art MARL algorithms. Empirical results demonstrate MAPO-LSO to show notable improvements in sample efficiency and learning performance compared to its vanilla MARL counterpart without any additional MARL hyperparameter tuning on a diverse suite of MARL tasks.",
    "authors": [
      "Dom Huh",
      "Prasant Mohapatra"
    ],
    "url": "http://arxiv.org/abs/2406.02890v1",
    "timestamp": 1717557104,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.MA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "99a4aed4-30eb-435a-846e-b991b9b7fba1": {
    "pk": "99a4aed4-30eb-435a-846e-b991b9b7fba1",
    "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.",
    "authors": [
      "Ilgee Hong",
      "Zichong Li",
      "Alexander Bukharin",
      "Yixiao Li",
      "Haoming Jiang",
      "Tianbao Yang",
      "Tuo Zhao"
    ],
    "url": "http://arxiv.org/abs/2406.02764v1",
    "timestamp": 1717533202,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "027eec98-15aa-4440-a96d-6a507c05c2ae": {
    "pk": "027eec98-15aa-4440-a96d-6a507c05c2ae",
    "title": "Reinforcement learning-based architecture search for quantum machine learning",
    "abstract": "Quantum machine learning (QML) leverages the large Hilbert space provided by quantum computing for data encoding, typically realized by parameterized quantum circuits (PQCs). While classical machine learning deals extensively with problem-specific model design, QML models mainly use hardwareefficient and heuristic circuit designs for PQCs. This work presents a novel approach employing the reinforcement learning algorithm MuZero to generate problem-specific PQCs to improve the QML performance. Diverging from previous search algorithms, we adopt a layered circuit design to significantly reduce the search space. Furthermore, we utilize cross-validation scoring to train the reinforcement learning algorithm, rewarding the discovery of high-performing circuits. In benchmarks we compare our tailored circuits with reference circuits from the literature, randomly generated circuits, and circuits generated by genetic algorithms. Our findings underscore the efficacy of problem-tailored encoding circuits in enhancing QML model performance.",
    "authors": [
      "Frederic Rapp",
      "David A. Kreplin",
      "Marco Roth"
    ],
    "url": "http://arxiv.org/abs/2406.02717v1",
    "timestamp": 1717527642,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ca03ab92-72a4-4a4b-9b39-35f135ce8ed0": {
    "pk": "ca03ab92-72a4-4a4b-9b39-35f135ce8ed0",
    "title": "iQRL -- Implicitly Quantized Representations for Sample-efficient Reinforcement Learning",
    "abstract": "Learning representations for reinforcement learning (RL) has shown much promise for continuous control. We propose an efficient representation learning method using only a self-supervised latent-state consistency loss. Our approach employs an encoder and a dynamics model to map observations to latent states and predict future latent states, respectively. We achieve high performance and prevent representation collapse by quantizing the latent representation such that the rank of the representation is empirically preserved. Our method, named iQRL: implicitly Quantized Reinforcement Learning, is straightforward, compatible with any model-free RL algorithm, and demonstrates excellent performance by outperforming other recently proposed representation learning methods in continuous control benchmarks from DeepMind Control Suite.",
    "authors": [
      "Aidan Scannell",
      "Kalle Kujanp\u00e4\u00e4",
      "Yi Zhao",
      "Mohammadreza Nakhaei",
      "Arno Solin",
      "Joni Pajarinen"
    ],
    "url": "http://arxiv.org/abs/2406.02696v1",
    "timestamp": 1717524944,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "18e5fd08-bca7-4b67-88d2-8fb1e2026370": {
    "pk": "18e5fd08-bca7-4b67-88d2-8fb1e2026370",
    "title": "Algorithmic Collusion in Dynamic Pricing with Deep Reinforcement Learning",
    "abstract": "Nowadays, a significant share of the Business-to-Consumer sector is based on online platforms like Amazon and Alibaba and uses Artificial Intelligence for pricing strategies. This has sparked debate on whether pricing algorithms may tacitly collude to set supra-competitive prices without being explicitly designed to do so. Our study addresses these concerns by examining the risk of collusion when Reinforcement Learning algorithms are used to decide on pricing strategies in competitive markets. Prior research in this field focused on Tabular Q-learning (TQL) and led to opposing views on whether learning-based algorithms can lead to supra-competitive prices. Our work contributes to this ongoing discussion by providing a more nuanced numerical study that goes beyond TQL by additionally capturing off- and on-policy Deep Reinforcement Learning (DRL) algorithms. We study multiple Bertrand oligopoly variants and show that algorithmic collusion depends on the algorithm used. In our experiments, TQL exhibits higher collusion and price dispersion phenomena compared to DRL algorithms. We show that the severity of collusion depends not only on the algorithm used but also on the characteristics of the market environment. We further find that Proximal Policy Optimization appears to be less sensitive to collusive outcomes compared to other state-of-the-art DRL algorithms.",
    "authors": [
      "Shidi Deng",
      "Maximilian Schiffer",
      "Martin Bichler"
    ],
    "url": "http://arxiv.org/abs/2406.02437v1",
    "timestamp": 1717516776,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "econ.GN",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ab09853d-5f41-4b01-ad3b-d04c91a20824": {
    "pk": "ab09853d-5f41-4b01-ad3b-d04c91a20824",
    "title": "By Fair Means or Foul: Quantifying Collusion in a Market Simulation with Deep Reinforcement Learning",
    "abstract": "In the rapidly evolving landscape of eCommerce, Artificial Intelligence (AI) based pricing algorithms, particularly those utilizing Reinforcement Learning (RL), are becoming increasingly prevalent. This rise has led to an inextricable pricing situation with the potential for market collusion. Our research employs an experimental oligopoly model of repeated price competition, systematically varying the environment to cover scenarios from basic economic theory to subjective consumer demand preferences. We also introduce a novel demand framework that enables the implementation of various demand models, allowing for a weighted blending of different models. In contrast to existing research in this domain, we aim to investigate the strategies and emerging pricing patterns developed by the agents, which may lead to a collusive outcome. Furthermore, we investigate a scenario where agents cannot observe their competitors' prices. Finally, we provide a comprehensive legal analysis across all scenarios. Our findings indicate that RL-based AI agents converge to a collusive state characterized by the charging of supracompetitive prices, without necessarily requiring inter-agent communication. Implementing alternative RL algorithms, altering the number of agents or simulation settings, and restricting the scope of the agents' observation space does not significantly impact the collusive market outcome behavior.",
    "authors": [
      "Michael Schlechtinger",
      "Damaris Kosack",
      "Franz Krause",
      "Heiko Paulheim"
    ],
    "url": "http://arxiv.org/abs/2406.02650v1",
    "timestamp": 1717515308,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "387b2794-e8bf-4462-bc68-05814b89525d": {
    "pk": "387b2794-e8bf-4462-bc68-05814b89525d",
    "title": "Query-based Semantic Gaussian Field for Scene Representation in Reinforcement Learning",
    "abstract": "Latent scene representation plays a significant role in training reinforcement learning (RL) agents. To obtain good latent vectors describing the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF pipeline into scene representation learning. However, these NeRF-related methods struggle to perceive 3D structural information due to the inefficient dense sampling in volumetric rendering. Moreover, they lack fine-grained semantic information included in their scene representation vectors because they evenly consider free and occupied spaces. Both of them can destroy the performance of downstream RL tasks. To address the above challenges, we propose a novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to learn 3D scene representation for the first time. In brief, we present the Query-based Generalizable 3DGS to bridge the 3DGS technique and scene representations with more geometrical awareness than those in NeRFs. Moreover, we present the Hierarchical Semantics Encoding to ground the fine-grained semantic features to 3D Gaussians and further distilled to the scene representation vectors. We conduct extensive experiments on two RL platforms including Maniskill2 and Robomimic across 10 different tasks. The results show that our method outperforms the other 5 baselines by a large margin. We achieve the best success rates on 8 tasks and the second-best on the other two tasks.",
    "authors": [
      "Jiaxu Wang",
      "Ziyi Zhang",
      "Qiang Zhang",
      "Jia Li",
      "Jingkai Sun",
      "Mingyuan Sun",
      "Junhao He",
      "Renjing Xu"
    ],
    "url": "http://arxiv.org/abs/2406.02370v2",
    "timestamp": 1717512547,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a32ac7bf-1946-4252-8212-7d158d23c130": {
    "pk": "a32ac7bf-1946-4252-8212-7d158d23c130",
    "title": "Smaller Batches, Bigger Gains? Investigating the Impact of Batch Sizes on Reinforcement Learning Based Real-World Production Scheduling",
    "abstract": "Production scheduling is an essential task in manufacturing, with Reinforcement Learning (RL) emerging as a key solution. In a previous work, RL was utilized to solve an extended permutation flow shop scheduling problem (PFSSP) for a real-world production line with two stages, linked by a central buffer. The RL agent was trained to sequence equallysized product batches to minimize setup efforts and idle times. However, the substantial impact caused by varying the size of these product batches has not yet been explored. In this follow-up study, we investigate the effects of varying batch sizes, exploring both the quality of solutions and the training dynamics of the RL agent. The results demonstrate that it is possible to methodically identify reasonable boundaries for the batch size. These boundaries are determined on one side by the increasing sample complexity associated with smaller batch sizes, and on the other side by the decreasing flexibility of the agent when dealing with larger batch sizes. This provides the practitioner the ability to make an informed decision regarding the selection of an appropriate batch size. Moreover, we introduce and investigate two new curriculum learning strategies to enable the training with small batch sizes. The findings of this work offer the potential for application in several industrial use cases with comparable scheduling problems.",
    "authors": [
      "Arthur M\u00fcller",
      "Felix Grumbach",
      "Matthia Sabatelli"
    ],
    "url": "http://arxiv.org/abs/2406.02294v1",
    "timestamp": 1717506968,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fe951748-6930-46f4-9ea2-44d5d0c1a22b": {
    "pk": "fe951748-6930-46f4-9ea2-44d5d0c1a22b",
    "title": "Test-Time Regret Minimization in Meta Reinforcement Learning",
    "abstract": "Meta reinforcement learning sets a distribution over a set of tasks on which the agent can train at will, then is asked to learn an optimal policy for any test task efficiently. In this paper, we consider a finite set of tasks modeled through Markov decision processes with various dynamics. We assume to have endured a long training phase, from which the set of tasks is perfectly recovered, and we focus on regret minimization against the optimal policy in the unknown test task. Under a separation condition that states the existence of a state-action pair revealing a task against another, Chen et al. (2022) show that $O(M^2 \\log(H))$ regret can be achieved, where $M, H$ are the number of tasks in the set and test episodes, respectively. In our first contribution, we demonstrate that the latter rate is nearly optimal by developing a novel lower bound for test-time regret minimization under separation, showing that a linear dependence with $M$ is unavoidable. Then, we present a family of stronger yet reasonable assumptions beyond separation, which we call strong identifiability, enabling algorithms achieving fast rates $\\log (H)$ and sublinear dependence with $M$ simultaneously. Our paper provides a new understanding of the statistical barriers of test-time regret minimization and when fast rates can be achieved.",
    "authors": [
      "Mirco Mutti",
      "Aviv Tamar"
    ],
    "url": "http://arxiv.org/abs/2406.02282v1",
    "timestamp": 1717505770,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "12038cd2-659b-4873-8c8c-fe8cc9f0ee48": {
    "pk": "12038cd2-659b-4873-8c8c-fe8cc9f0ee48",
    "title": "Reinforcement Learning with Lookahead Information",
    "abstract": "We study reinforcement learning (RL) problems in which agents observe the reward or transition realizations at their current state before deciding which action to take. Such observations are available in many applications, including transactions, navigation and more. When the environment is known, previous work shows that this lookahead information can drastically increase the collected reward. However, outside of specific applications, existing approaches for interacting with unknown environments are not well-adapted to these observations. In this work, we close this gap and design provably-efficient learning algorithms able to incorporate lookahead information. To achieve this, we perform planning using the empirical distribution of the reward and transition observations, in contrast to vanilla approaches that only rely on estimated expectations. We prove that our algorithms achieve tight regret versus a baseline that also has access to lookahead information - linearly increasing the amount of collected reward compared to agents that cannot handle lookahead information.",
    "authors": [
      "Nadav Merlis"
    ],
    "url": "http://arxiv.org/abs/2406.02258v1",
    "timestamp": 1717504191,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "db5efede-a7fd-45fa-ade8-078faf2f0b60": {
    "pk": "db5efede-a7fd-45fa-ade8-078faf2f0b60",
    "title": "Rectifying Reinforcement Learning for Reward Matching",
    "abstract": "The Generative Flow Network (GFlowNet) is a probabilistic framework in which an agent learns a stochastic policy and flow functions to sample objects with probability proportional to an unnormalized reward function. GFlowNets share a strong resemblance to reinforcement learning (RL), that typically aims to maximize reward, due to their sequential decision-making processes. Recent works have studied connections between GFlowNets and maximum entropy (MaxEnt) RL, which modifies the standard objective of RL agents by learning an entropy-regularized objective. However, a critical theoretical gap persists: despite the apparent similarities in their sequential decision-making nature, a direct link between GFlowNets and standard RL has yet to be discovered, while bridging this gap could further unlock the potential of both fields. In this paper, we establish a new connection between GFlowNets and policy evaluation for a uniform policy. Surprisingly, we find that the resulting value function for the uniform policy has a close relationship to the flows in GFlowNets. Leveraging these insights, we further propose a novel rectified policy evaluation (RPE) algorithm, which achieves the same reward-matching effect as GFlowNets, offering a new perspective. We compare RPE, MaxEnt RL, and GFlowNets in a number of benchmarks, and show that RPE achieves competitive results compared to previous approaches. This work sheds light on the previously unexplored connection between (non-MaxEnt) RL and GFlowNets, potentially opening new avenues for future research in both fields.",
    "authors": [
      "Haoran He",
      "Emmanuel Bengio",
      "Qingpeng Cai",
      "Ling Pan"
    ],
    "url": "http://arxiv.org/abs/2406.02213v1",
    "timestamp": 1717499513,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9e89a28c-c8e9-4a71-9425-9de9bfaeace3": {
    "pk": "9e89a28c-c8e9-4a71-9425-9de9bfaeace3",
    "title": "FightLadder: A Benchmark for Competitive Multi-Agent Reinforcement Learning",
    "abstract": "Recent advances in reinforcement learning (RL) heavily rely on a variety of well-designed benchmarks, which provide environmental platforms and consistent criteria to evaluate existing and novel algorithms. Specifically, in multi-agent RL (MARL), a plethora of benchmarks based on cooperative games have spurred the development of algorithms that improve the scalability of cooperative multi-agent systems. However, for the competitive setting, a lightweight and open-sourced benchmark with challenging gaming dynamics and visual inputs has not yet been established. In this work, we present FightLadder, a real-time fighting game platform, to empower competitive MARL research. Along with the platform, we provide implementations of state-of-the-art MARL algorithms for competitive games, as well as a set of evaluation metrics to characterize the performance and exploitability of agents. We demonstrate the feasibility of this platform by training a general agent that consistently defeats 12 built-in characters in single-player mode, and expose the difficulty of training a non-exploitable agent without human knowledge and demonstrations in two-player mode. FightLadder provides meticulously designed environments to address critical challenges in competitive MARL research, aiming to catalyze a new era of discovery and advancement in the field. Videos and code at https://sites.google.com/view/fightladder/home.",
    "authors": [
      "Wenzhe Li",
      "Zihan Ding",
      "Seth Karten",
      "Chi Jin"
    ],
    "url": "http://arxiv.org/abs/2406.02081v1",
    "timestamp": 1717488263,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.MA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f949cde2-c1f6-4efb-b472-d9f5e684a9ea": {
    "pk": "f949cde2-c1f6-4efb-b472-d9f5e684a9ea",
    "title": "A Unifying Framework for Action-Conditional Self-Predictive Reinforcement Learning",
    "abstract": "Learning a good representation is a crucial challenge for Reinforcement Learning (RL) agents. Self-predictive learning provides means to jointly learn a latent representation and dynamics model by bootstrapping from future latent representations (BYOL). Recent work has developed theoretical insights into these algorithms by studying a continuous-time ODE model for self-predictive representation learning under the simplifying assumption that the algorithm depends on a fixed policy (BYOL-$\\Pi$); this assumption is at odds with practical instantiations of such algorithms, which explicitly condition their predictions on future actions. In this work, we take a step towards bridging the gap between theory and practice by analyzing an action-conditional self-predictive objective (BYOL-AC) using the ODE framework, characterizing its convergence properties and highlighting important distinctions between the limiting solutions of the BYOL-$\\Pi$ and BYOL-AC dynamics. We show how the two representations are related by a variance equation. This connection leads to a novel variance-like action-conditional objective (BYOL-VAR) and its corresponding ODE. We unify the study of all three objectives through two complementary lenses; a model-based perspective, where each objective is shown to be equivalent to a low-rank approximation of certain dynamics, and a model-free perspective, which establishes relationships between the objectives and their respective value, Q-value, and advantage function. Our empirical investigations, encompassing both linear function approximation and Deep RL environments, demonstrates that BYOL-AC is better overall in a variety of different settings.",
    "authors": [
      "Khimya Khetarpal",
      "Zhaohan Daniel Guo",
      "Bernardo Avila Pires",
      "Yunhao Tang",
      "Clare Lyle",
      "Mark Rowland",
      "Nicolas Heess",
      "Diana Borsa",
      "Arthur Guez",
      "Will Dabney"
    ],
    "url": "http://arxiv.org/abs/2406.02035v1",
    "timestamp": 1717485732,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "13f1ac28-3e4f-4402-ad80-8d6cc6211508": {
    "pk": "13f1ac28-3e4f-4402-ad80-8d6cc6211508",
    "title": "Mamba as Decision Maker: Exploring Multi-scale Sequence Modeling in Offline Reinforcement Learning",
    "abstract": "Sequential modeling has demonstrated remarkable capabilities in offline reinforcement learning (RL), with Decision Transformer (DT) being one of the most notable representatives, achieving significant success. However, RL trajectories possess unique properties to be distinguished from the conventional sequence (e.g., text or audio): (1) local correlation, where the next states in RL are theoretically determined solely by current states and actions based on the Markov Decision Process (MDP), and (2) global correlation, where each step's features are related to long-term historical information due to the time-continuous nature of trajectories. In this paper, we propose a novel action sequence predictor, named Mamba Decision Maker (MambaDM), where Mamba is expected to be a promising alternative for sequence modeling paradigms, owing to its efficient modeling of multi-scale dependencies. In particular, we introduce a novel mixer module that proficiently extracts and integrates both global and local features of the input sequence, effectively capturing interrelationships in RL datasets. Extensive experiments demonstrate that MambaDM achieves state-of-the-art performance in Atari and OpenAI Gym datasets. Furthermore, we empirically investigate the scaling laws of MambaDM, finding that increasing model size does not bring performance improvement, but scaling the dataset amount by 2x for MambaDM can obtain up to 33.7% score improvement on Atari dataset. This paper delves into the sequence modeling capabilities of MambaDM in the RL domain, paving the way for future advancements in robust and efficient decision-making systems. Our code will be available at https://github.com/AndyCao1125/MambaDM.",
    "authors": [
      "Jiahang Cao",
      "Qiang Zhang",
      "Ziqing Wang",
      "Jiaxu Wang",
      "Hao Cheng",
      "Yecheng Shao",
      "Wen Zhao",
      "Gang Han",
      "Yijie Guo",
      "Renjing Xu"
    ],
    "url": "http://arxiv.org/abs/2406.02013v1",
    "timestamp": 1717483758,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "125b4d4d-c0fc-4312-bb58-d46efecb0040": {
    "pk": "125b4d4d-c0fc-4312-bb58-d46efecb0040",
    "title": "Multi-Agent Reinforcement Learning Meets Leaf Sequencing in Radiotherapy",
    "abstract": "In contemporary radiotherapy planning (RTP), a key module leaf sequencing is predominantly addressed by optimization-based approaches. In this paper, we propose a novel deep reinforcement learning (DRL) model termed as Reinforced Leaf Sequencer (RLS) in a multi-agent framework for leaf sequencing. The RLS model offers improvements to time-consuming iterative optimization steps via large-scale training and can control movement patterns through the design of reward mechanisms. We have conducted experiments on four datasets with four metrics and compared our model with a leading optimization sequencer. Our findings reveal that the proposed RLS model can achieve reduced fluence reconstruction errors, and potential faster convergence when integrated in an optimization planner. Additionally, RLS has shown promising results in a full artificial intelligence RTP pipeline. We hope this pioneer multi-agent RL leaf sequencer can foster future research on machine learning for RTP.",
    "authors": [
      "Riqiang Gao",
      "Florin C. Ghesu",
      "Simon Arberet",
      "Shahab Basiri",
      "Esa Kuusela",
      "Martin Kraus",
      "Dorin Comaniciu",
      "Ali Kamen"
    ],
    "url": "http://arxiv.org/abs/2406.01853v1",
    "timestamp": 1717458920,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "28428cf5-4610-42fb-a07a-aaa51a255e22": {
    "pk": "28428cf5-4610-42fb-a07a-aaa51a255e22",
    "title": "Towards the Transferability of Rewards Recovered via Regularized Inverse Reinforcement Learning",
    "abstract": "Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task [Ng et al., 2000]. However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward. Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [Rolland et al., 2022]. In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts.",
    "authors": [
      "Andreas Schlaginhaufen",
      "Maryam Kamgarpour"
    ],
    "url": "http://arxiv.org/abs/2406.01793v1",
    "timestamp": 1717449488,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "65d58bf0-6f8b-4a77-b598-468f2631c79a": {
    "pk": "65d58bf0-6f8b-4a77-b598-468f2631c79a",
    "title": "Multi-agent assignment via state augmented reinforcement learning",
    "abstract": "We address the conflicting requirements of a multi-agent assignment problem through constrained reinforcement learning, emphasizing the inadequacy of standard regularization techniques for this purpose. Instead, we recur to a state augmentation approach in which the oscillation of dual variables is exploited by agents to alternate between tasks. In addition, we coordinate the actions of the multiple agents acting on their local states through these multipliers, which are gossiped through a communication network, eliminating the need to access other agent states. By these means, we propose a distributed multi-agent assignment protocol with theoretical feasibility guarantees that we corroborate in a monitoring numerical experiment.",
    "authors": [
      "Leopoldo Agorio",
      "Sean Van Alen",
      "Miguel Calvo-Fullana",
      "Santiago Paternain",
      "Juan Andres Bazerque"
    ],
    "url": "http://arxiv.org/abs/2406.01782v1",
    "timestamp": 1717448172,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5b7239cf-30ba-4efc-a70a-262885ad1cc7": {
    "pk": "5b7239cf-30ba-4efc-a70a-262885ad1cc7",
    "title": "A New View on Planning in Online Reinforcement Learning",
    "abstract": "This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a set of (abstract) subgoals and learning only local, subgoal-conditioned models. This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning and avoids learning the transition dynamics entirely. We show that our GSP algorithm can propagate value from an abstract space in a manner that helps a variety of base learners learn significantly faster in different domains.",
    "authors": [
      "Kevin Roice",
      "Parham Mohammad Panahi",
      "Scott M. Jordan",
      "Adam White",
      "Martha White"
    ],
    "url": "http://arxiv.org/abs/2406.01562v1",
    "timestamp": 1717436719,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "24f430c1-56d5-4d26-867c-c744e5162965": {
    "pk": "24f430c1-56d5-4d26-867c-c744e5162965",
    "title": "MOSEAC: Streamlined Variable Time Step Reinforcement Learning",
    "abstract": "Traditional reinforcement learning (RL) methods typically employ a fixed control loop, where each cycle corresponds to an action. This rigidity poses challenges in practical applications, as the optimal control frequency is task-dependent. A suboptimal choice can lead to high computational demands and reduced exploration efficiency. Variable Time Step Reinforcement Learning (VTS-RL) addresses these issues by using adaptive frequencies for the control loop, executing actions only when necessary. This approach, rooted in reactive programming principles, reduces computational load and extends the action space by including action durations. However, VTS-RL's implementation is often complicated by the need to tune multiple hyperparameters that govern exploration in the multi-objective action-duration space (i.e., balancing task performance and number of time steps to achieve a goal). To overcome these challenges, we introduce the Multi-Objective Soft Elastic Actor-Critic (MOSEAC) method. This method features an adaptive reward scheme that adjusts hyperparameters based on observed trends in task rewards during training. This scheme reduces the complexity of hyperparameter tuning, requiring a single hyperparameter to guide exploration, thereby simplifying the learning process and lowering deployment costs. We validate the MOSEAC method through simulations in a Newtonian kinematics environment, demonstrating high task and training performance with fewer time steps, ultimately lowering energy consumption. This validation shows that MOSEAC streamlines RL algorithm deployment by automatically tuning the agent control loop frequency using a single parameter. Its principles can be applied to enhance any RL algorithm, making it a versatile solution for various applications.",
    "authors": [
      "Dong Wang",
      "Giovanni Beltrame"
    ],
    "url": "http://arxiv.org/abs/2406.01521v1",
    "timestamp": 1717433517,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bc0f2d2a-310f-4b92-ad13-17a13afba0e4": {
    "pk": "bc0f2d2a-310f-4b92-ad13-17a13afba0e4",
    "title": "Recent Trends in Insect and Robot Navigation through the Lens of Reinforcement Learning",
    "abstract": "Bees are among the master navigators of the insect world. Despite impressive advances in robot navigation research, the performance of these insects is still unrivaled by any artificial system in terms of training efficiency and generalization capabilities, particularly considering the limited computational capacity. On the other hand, computational principles underlying these extraordinary feats are still only partially understood. The theoretical framework of reinforcement learning (RL) provides an ideal focal point to bring the two fields together for mutual benefit. While RL has long been at the core of robot navigation research, current computational theories of insect navigation are not commonly formulated within this framework, but largely as an associative learning process implemented in the insect brain, especially in a region called the mushroom body. Here we argue that this neural substrate can be understood as implementing a certain class of relatively simple RL algorithms, capable of integrating distinct components of a navigation task, reminiscent of hierarchical RL models used in robot navigation. The efficiency of insect navigation is likely rooted in an efficient and robust internal representation of space, linking retinotopic (egocentric) visual input with the geometry of the environment. We discuss how current models of insect and robot navigation are exploring representations beyond classical, complete map-like representations, with spatial information being embedded in the respective latent representations to varying degrees.",
    "authors": [
      "Stephan Lochner",
      "Daniel Honerkamp",
      "Abhinav Valada",
      "Andrew D. Straw"
    ],
    "url": "http://arxiv.org/abs/2406.01501v1",
    "timestamp": 1717432089,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "q-bio.NC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fe3c61d2-a240-4e9a-b5ea-3438908638b0": {
    "pk": "fe3c61d2-a240-4e9a-b5ea-3438908638b0",
    "title": "Combinatorial Multivariant Multi-Armed Bandits with Applications to Episodic Reinforcement Learning and Beyond",
    "abstract": "We introduce a novel framework of combinatorial multi-armed bandits (CMAB) with multivariant and probabilistically triggering arms (CMAB-MT), where the outcome of each arm is a $d$-dimensional multivariant random variable and the feedback follows a general arm triggering process. Compared with existing CMAB works, CMAB-MT not only enhances the modeling power but also allows improved results by leveraging distinct statistical properties for multivariant random variables. For CMAB-MT, we propose a general 1-norm multivariant and triggering probability-modulated smoothness condition, and an optimistic CUCB-MT algorithm built upon this condition. Our framework can include many important problems as applications, such as episodic reinforcement learning (RL) and probabilistic maximum coverage for goods distribution, all of which meet the above smoothness condition and achieve matching or improved regret bounds compared to existing works. Through our new framework, we build the first connection between the episodic RL and CMAB literature, by offering a new angle to solve the episodic RL through the lens of CMAB, which may encourage more interactions between these two important directions.",
    "authors": [
      "Xutong Liu",
      "Siwei Wang",
      "Jinhang Zuo",
      "Han Zhong",
      "Xuchuang Wang",
      "Zhiyong Wang",
      "Shuai Li",
      "Mohammad Hajiesmaili",
      "John C. S. Lui",
      "Wei Chen"
    ],
    "url": "http://arxiv.org/abs/2406.01386v1",
    "timestamp": 1717426133,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "38bce113-8796-4e54-afb9-499216d4f158": {
    "pk": "38bce113-8796-4e54-afb9-499216d4f158",
    "title": "Deep Reinforcement Learning Behavioral Mode Switching Using Optimal Control Based on a Latent Space Objective",
    "abstract": "In this work, we use optimal control to change the behavior of a deep reinforcement learning policy by optimizing directly in the policy's latent space. We hypothesize that distinct behavioral patterns, termed behavioral modes, can be identified within certain regions of a deep reinforcement learning policy's latent space, meaning that specific actions or strategies are preferred within these regions. We identify these behavioral modes using latent space dimension-reduction with \\ac*{pacmap}. Using the actions generated by the optimal control procedure, we move the system from one behavioral mode to another. We subsequently utilize these actions as a filter for interpreting the neural network policy. The results show that this approach can impose desired behavioral modes in the policy, demonstrated by showing how a failed episode can be made successful and vice versa using the lunar lander reinforcement learning environment.",
    "authors": [
      "Sindre Benjamin Remman",
      "Bj\u00f8rn Andreas Kristiansen",
      "Anastasios M. Lekkas"
    ],
    "url": "http://arxiv.org/abs/2406.01178v1",
    "timestamp": 1717410060,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ae4f17fa-7de1-451e-a7a1-657430a13d5a": {
    "pk": "ae4f17fa-7de1-451e-a7a1-657430a13d5a",
    "title": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A Model-Based Reinforcement Learning Approach",
    "abstract": "Optimizing the deployment of large language models (LLMs) in edge computing environments is critical for enhancing privacy and computational efficiency. Toward efficient wireless LLM inference in edge computing, this study comprehensively analyzes the impact of different splitting points in mainstream open-source LLMs. On this basis, this study introduces a framework taking inspiration from model-based reinforcement learning (MBRL) to determine the optimal splitting point across the edge and user equipment (UE). By incorporating a reward surrogate model, our approach significantly reduces the computational cost of frequent performance evaluations. Extensive simulations demonstrate that this method effectively balances inference performance and computational load under varying network conditions, providing a robust solution for LLM deployment in decentralized settings.",
    "authors": [
      "Yuxuan Chen",
      "Rongpeng Li",
      "Xiaoxue Yu",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "url": "http://arxiv.org/abs/2406.02616v3",
    "timestamp": 1717407702,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0bffeb6b-a448-4064-a868-973ea4774f8d": {
    "pk": "0bffeb6b-a448-4064-a868-973ea4774f8d",
    "title": "Deep reinforcement learning for weakly coupled MDP's with continuous actions",
    "abstract": "This paper introduces the Lagrange Policy for Continuous Actions (LPCA), a reinforcement learning algorithm specifically designed for weakly coupled MDP problems with continuous action spaces. LPCA addresses the challenge of resource constraints dependent on continuous actions by introducing a Lagrange relaxation of the weakly coupled MDP problem within a neural network framework for Q-value computation. This approach effectively decouples the MDP, enabling efficient policy learning in resource-constrained environments. We present two variations of LPCA: LPCA-DE, which utilizes differential evolution for global optimization, and LPCA-Greedy, a method that incrementally and greadily selects actions based on Q-value gradients. Comparative analysis against other state-of-the-art techniques across various settings highlight LPCA's robustness and efficiency in managing resource allocation while maximizing rewards.",
    "authors": [
      "Francisco Robledo",
      "Urtzi Ayesta",
      "Konstantin Avrachenkov"
    ],
    "url": "http://arxiv.org/abs/2406.01099v1",
    "timestamp": 1717403672,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b11d39de-5065-4ce0-9a34-b3a71ede4f5a": {
    "pk": "b11d39de-5065-4ce0-9a34-b3a71ede4f5a",
    "title": "Causal prompting model-based offline reinforcement learning",
    "abstract": "Model-based offline Reinforcement Learning (RL) allows agents to fully utilise pre-collected datasets without requiring additional or unethical explorations. However, applying model-based offline RL to online systems presents challenges, primarily due to the highly suboptimal (noise-filled) and diverse nature of datasets generated by online systems. To tackle these issues, we introduce the Causal Prompting Reinforcement Learning (CPRL) framework, designed for highly suboptimal and resource-constrained online scenarios. The initial phase of CPRL involves the introduction of the Hidden-Parameter Block Causal Prompting Dynamic (Hip-BCPD) to model environmental dynamics. This approach utilises invariant causal prompts and aligns hidden parameters to generalise to new and diverse online users. In the subsequent phase, a single policy is trained to address multiple tasks through the amalgamation of reusable skills, circumventing the need for training from scratch. Experiments conducted across datasets with varying levels of noise, including simulation-based and real-world offline datasets from the Dnurse APP, demonstrate that our proposed method can make robust decisions in out-of-distribution and noisy environments, outperforming contemporary algorithms. Additionally, we separately verify the contributions of Hip-BCPDs and the skill-reuse strategy to the robustness of performance. We further analyse the visualised structure of Hip-BCPD and the interpretability of sub-skills. We released our source code and the first ever real-world medical dataset for precise medical decision-making tasks.",
    "authors": [
      "Xuehui Yu",
      "Yi Guan",
      "Rujia Shen",
      "Xin Li",
      "Chen Tang",
      "Jingchi Jiang"
    ],
    "url": "http://arxiv.org/abs/2406.01065v1",
    "timestamp": 1717399737,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9f63592f-1ae2-4383-b6bc-e9933618780a": {
    "pk": "9f63592f-1ae2-4383-b6bc-e9933618780a",
    "title": "Satellites swarm cooperation for pursuit-attachment tasks with transformer-based reinforcement learning",
    "abstract": "The on-orbit intelligent planning of satellites swarm has attracted increasing attention from scholars. Especially in tasks such as the pursuit and attachment of non-cooperative satellites, satellites swarm must achieve coordinated cooperation with limited resources. The study proposes a reinforcement learning framework that integrates the transformer and expert networks. Firstly, under the constraints of incomplete information about non-cooperative satellites, an implicit multi-satellites cooperation strategy was designed using a communication sharing mechanism. Subsequently, for the characteristics of the pursuit-attachment tasks, the multi-agent reinforcement learning framework is improved by introducing transformers and expert networks inspired by transfer learning ideas. To address the issue of satellites swarm scalability, sequence modelling based on transformers is utilized to craft memory-augmented policy networks, meanwhile increasing the scalability of the swarm. By comparing the convergence curves with other algorithms, it is shown that the proposed method is qualified for pursuit-attachment tasks of satellites swarm. Additionally, simulations under different maneuvering strategies of non-cooperative satellites respectively demonstrate the robustness of the algorithm and the task efficiency of the swarm system. The success rate of pursuit-attachment tasks is analyzed through Monte Carlo simulations.",
    "authors": [
      "yonghao Li"
    ],
    "url": "http://arxiv.org/abs/2406.01061v1",
    "timestamp": 1717399036,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.MA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ea673ca8-8346-4a16-9e97-373ee042f84f": {
    "pk": "ea673ca8-8346-4a16-9e97-373ee042f84f",
    "title": "An Advanced Reinforcement Learning Framework for Online Scheduling of Deferrable Workloads in Cloud Computing",
    "abstract": "Efficient resource utilization and perfect user experience usually conflict with each other in cloud computing platforms. Great efforts have been invested in increasing resource utilization but trying not to affect users' experience for cloud computing platforms. In order to better utilize the remaining pieces of computing resources spread over the whole platform, deferrable jobs are provided with a discounted price to users. For this type of deferrable jobs, users are allowed to submit jobs that will run for a specific uninterrupted duration in a flexible range of time in the future with a great discount. With these deferrable jobs to be scheduled under the remaining capacity after deploying those on-demand jobs, it remains a challenge to achieve high resource utilization and meanwhile shorten the waiting time for users as much as possible in an online manner. In this paper, we propose an online deferrable job scheduling method called \\textit{Online Scheduling for DEferrable jobs in Cloud} (\\OSDEC{}), where a deep reinforcement learning model is adopted to learn the scheduling policy, and several auxiliary tasks are utilized to provide better state representations and improve the performance of the model. With the integrated reinforcement learning framework, the proposed method can well plan the deployment schedule and achieve a short waiting time for users while maintaining a high resource utilization for the platform. The proposed method is validated on a public dataset and shows superior performance.",
    "authors": [
      "Hang Dong",
      "Liwen Zhu",
      "Zhao Shan",
      "Bo Qiao",
      "Fangkai Yang",
      "Si Qin",
      "Chuan Luo",
      "Qingwei Lin",
      "Yuwen Yang",
      "Gurpreet Virdi",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Thomas Moscibroda"
    ],
    "url": "http://arxiv.org/abs/2406.01047v1",
    "timestamp": 1717397726,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2ca2420a-1040-4742-94ec-0cdd91f10a51": {
    "pk": "2ca2420a-1040-4742-94ec-0cdd91f10a51",
    "title": "Evaluating MEDIRL: A Replication and Ablation Study of Maximum Entropy Deep Inverse Reinforcement Learning for Human Social Navigation",
    "abstract": "In this study, we enhance the Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) framework, targeting its application in human robot interaction (HRI) for modeling pedestrian behavior in crowded environments. Our work is grounded in the pioneering research by Fahad, Chen, and Guo, and aims to elevate MEDIRL's efficacy in real world HRI settings. We replicated the original MEDIRL model and conducted detailed ablation studies, focusing on key model components like learning rates, state dimensions, and network layers. Our findings reveal the effectiveness of a two dimensional state representation over three dimensional approach, significantly improving model accuracy for pedestrian behavior prediction in HRI scenarios. These results not only demonstrate MEDIRL's enhanced performance but also offer valuable insights for future HRI system development, emphasizing the importance of model customization to specific environmental contexts. Our research contributes to advancing the field of socially intelligent navigation systems, promoting more intuitive and safer human robot interactions.",
    "authors": [
      "Vinay Gupta",
      "Nihal Gunukula"
    ],
    "url": "http://arxiv.org/abs/2406.00968v1",
    "timestamp": 1717386318,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "25672867-9854-4a55-977b-d92b3300e616": {
    "pk": "25672867-9854-4a55-977b-d92b3300e616",
    "title": "Deep Reinforcement Learning for Sim-to-Real Policy Transfer of VTOL-UAVs Offshore Docking Operations",
    "abstract": "This paper proposes a novel Reinforcement Learning (RL) approach for sim-to-real policy transfer of Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL-UAV). The proposed approach is designed for VTOL-UAV landing on offshore docking stations in maritime operations. VTOL-UAVs in maritime operations encounter limitations in their operational range, primarily stemming from constraints imposed by their battery capacity. The concept of autonomous landing on a charging platform presents an intriguing prospect for mitigating these limitations by facilitating battery charging and data transfer. However, current Deep Reinforcement Learning (DRL) methods exhibit drawbacks, including lengthy training times, and modest success rates. In this paper, we tackle these concerns comprehensively by decomposing the landing procedure into a sequence of more manageable but analogous tasks in terms of an approach phase and a landing phase. The proposed architecture utilizes a model-based control scheme for the approach phase, where the VTOL-UAV is approaching the offshore docking station. In the Landing phase, DRL agents were trained offline to learn the optimal policy to dock on the offshore station. The Joint North Sea Wave Project (JONSWAP) spectrum model has been employed to create a wave model for each episode, enhancing policy generalization for sim2real transfer. A set of DRL algorithms have been tested through numerical simulations including value-based agents and policy-based agents such as Deep \\textit{Q} Networks (DQN) and Proximal Policy Optimization (PPO) respectively. The numerical experiments show that the PPO agent can learn complicated and efficient policies to land in uncertain environments, which in turn enhances the likelihood of successful sim-to-real transfer.",
    "authors": [
      "Ali M. Ali",
      "Aryaman Gupta",
      "Hashim A. Hashim"
    ],
    "url": "http://arxiv.org/abs/2406.00887v1",
    "timestamp": 1717369543,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "93caad93-80d7-4ae0-ab6b-eb56d423bdb2": {
    "pk": "93caad93-80d7-4ae0-ab6b-eb56d423bdb2",
    "title": "Dual Policy Reinforcement Learning for Real-time Rebalancing in Bike-sharing Systems",
    "abstract": "Bike-sharing systems play a crucial role in easing traffic congestion and promoting healthier lifestyles. However, ensuring their reliability and user acceptance requires effective strategies for rebalancing bikes. This study introduces a novel approach to address the real-time rebalancing problem with a fleet of vehicles. It employs a dual policy reinforcement learning algorithm that decouples inventory and routing decisions, enhancing realism and efficiency compared to previous methods where both decisions were made simultaneously. We first formulate the inventory and routing subproblems as a multi-agent Markov Decision Process within a continuous time framework. Subsequently, we propose a DQN-based dual policy framework to jointly estimate the value functions, minimizing the lost demand. To facilitate learning, a comprehensive simulator is applied to operate under a first-arrive-first-serve rule, which enables the computation of immediate rewards across diverse demand scenarios. We conduct extensive experiments on various datasets generated from historical real-world data, affected by both temporal and weather factors. Our proposed algorithm demonstrates significant performance improvements over previous baseline methods. It offers valuable practical insights for operators and further explores the incorporation of reinforcement learning into real-world dynamic programming problems, paving the way for more intelligent and robust urban mobility solutions.",
    "authors": [
      "Jiaqi Liang",
      "Defeng Liu",
      "Sanjay Dominik Jena",
      "Andrea Lodi",
      "Thibaut Vidal"
    ],
    "url": "http://arxiv.org/abs/2406.00868v1",
    "timestamp": 1717362323,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bb72e26c-342f-48a7-be93-8d27a2bbe7fd": {
    "pk": "bb72e26c-342f-48a7-be93-8d27a2bbe7fd",
    "title": "Chiplet-Gym: Optimizing Chiplet-based AI Accelerator Design with Reinforcement Learning",
    "abstract": "Modern Artificial Intelligence (AI) workloads demand computing systems with large silicon area to sustain throughput and competitive performance. However, prohibitive manufacturing costs and yield limitations at advanced tech nodes and die-size reaching the reticle limit restrain us from achieving this. With the recent innovations in advanced packaging technologies, chiplet-based architectures have gained significant attention in the AI hardware domain. However, the vast design space of chiplet-based AI accelerator design and the absence of system and package-level co-design methodology make it difficult for the designer to find the optimum design point regarding Power, Performance, Area, and manufacturing Cost (PPAC). This paper presents Chiplet-Gym, a Reinforcement Learning (RL)-based optimization framework to explore the vast design space of chiplet-based AI accelerators, encompassing the resource allocation, placement, and packaging architecture. We analytically model the PPAC of the chiplet-based AI accelerator and integrate it into an OpenAI gym environment to evaluate the design points. We also explore non-RL-based optimization approaches and combine these two approaches to ensure the robustness of the optimizer. The optimizer-suggested design point achieves 1.52X throughput, 0.27X energy, and 0.01X die cost while incurring only 1.62X package cost of its monolithic counterpart at iso-area.",
    "authors": [
      "Kaniz Mishty",
      "Mehdi Sadi"
    ],
    "url": "http://arxiv.org/abs/2406.00858v1",
    "timestamp": 1717360242,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "75bb46d9-1b26-4fef-89cd-791944e39265": {
    "pk": "75bb46d9-1b26-4fef-89cd-791944e39265",
    "title": "Shared-unique Features and Task-aware Prioritized Sampling on Multi-task Reinforcement Learning",
    "abstract": "We observe that current state-of-the-art (SOTA) methods suffer from the performance imbalance issue when performing multi-task reinforcement learning (MTRL) tasks. While these methods may achieve impressive performance on average, they perform extremely poorly on a few tasks. To address this, we propose a new and effective method called STARS, which consists of two novel strategies: a shared-unique feature extractor and task-aware prioritized sampling. First, the shared-unique feature extractor learns both shared and task-specific features to enable better synergy of knowledge between different tasks. Second, the task-aware sampling strategy is combined with the prioritized experience replay for efficient learning on tasks with poor performance. The effectiveness and stability of our STARS are verified through experiments on the mainstream Meta-World benchmark. From the results, our STARS statistically outperforms current SOTA methods and alleviates the performance imbalance issue. Besides, we visualize the learned features to support our claims and enhance the interpretability of STARS.",
    "authors": [
      "Po-Shao Lin",
      "Jia-Fong Yeh",
      "Yi-Ting Chen",
      "Winston H. Hsu"
    ],
    "url": "http://arxiv.org/abs/2406.00761v1",
    "timestamp": 1717338829,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2690dcf0-1bc2-4115-b0dd-e446d497d12c": {
    "pk": "2690dcf0-1bc2-4115-b0dd-e446d497d12c",
    "title": "A Digital Twin Framework for Reinforcement Learning with Real-Time Self-Improvement via Human Assistive Teleoperation",
    "abstract": "Reinforcement Learning (RL) or Deep Reinforcement Learning (DRL) is a powerful approach to solving Markov Decision Processes (MDPs) when the model of the environment is not known a priori. However, RL models are still faced with challenges such as handling covariate shifts and ensuring the quality of human demonstration. To address these challenges and further advance DRL models, our work develops a human-in-the-loop DRL framework via digital twin that leverages human intelligence after deployment to retrain the DRL model in real time. First, we develop a pre-trained model fully based on learning through trial and error in the simulated environment allowing scalability and automation while eliminating variability and biases that can come from subjective human guidance. Second, instead of deploying the trained model directly on the UGV, we create a digital twin which controls the physical UGV from the virtual environment. Third, to allow continuous learning without catastrophic forgetting, we introduce the ability of the model to self-improve with the help of small human guidance at the start of the retraining. We test the performance of our proposed model in both simulation and real-world environments with both static and dynamic obstacles. The results indicate that our proposed approach not only outperforms the baseline models in terms of reward accumulation but also demonstrates superior training efficiency.",
    "authors": [
      "Kabirat Olayemi",
      "Mien Van",
      "Luke Maguire",
      "Sean McLoone"
    ],
    "url": "http://arxiv.org/abs/2406.00732v1",
    "timestamp": 1717332331,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "872bacc9-1f23-4bde-b45c-2211c6817559": {
    "pk": "872bacc9-1f23-4bde-b45c-2211c6817559",
    "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning",
    "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem of reward misalignment when applying VLM as a reward in RL tasks. To address this issue, we introduce a lightweight fine-tuning method, named Fuzzy VLM reward-aided RL (FuRL), based on reward alignment and relay RL. Specifically, we enhance the performance of SAC/DrQ baseline agents on sparse reward tasks by fine-tuning VLM representations and using relay RL to avoid local minima. Extensive experiments on the Meta-world benchmark tasks demonstrate the efficacy of the proposed method. Code is available at: https://github.com/fuyw/FuRL.",
    "authors": [
      "Yuwei Fu",
      "Haichao Zhang",
      "Di Wu",
      "Wei Xu",
      "Benoit Boulet"
    ],
    "url": "http://arxiv.org/abs/2406.00645v2",
    "timestamp": 1717312808,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "565d6a74-4a55-4ed6-a398-93def1d038d0": {
    "pk": "565d6a74-4a55-4ed6-a398-93def1d038d0",
    "title": "Model Predictive Control and Reinforcement Learning: A Unified Framework Based on Dynamic Programming",
    "abstract": "In this paper we describe a new conceptual framework that connects approximate Dynamic Programming (DP), Model Predictive Control (MPC), and Reinforcement Learning (RL). This framework centers around two algorithms, which are designed largely independently of each other and operate in synergy through the powerful mechanism of Newton's method. We call them the off-line training and the on-line play algorithms. The names are borrowed from some of the major successes of RL involving games; primary examples are the recent (2017) AlphaZero program (which plays chess, [SHS17], [SSS17]), and the similarly structured and earlier (1990s) TD-Gammon program (which plays backgammon, [Tes94], [Tes95], [TeG96]). In these game contexts, the off-line training algorithm is the method used to teach the program how to evaluate positions and to generate good moves at any given position, while the on-line play algorithm is the method used to play in real time against human or computer opponents.   Significantly, the synergy between off-line training and on-line play also underlies MPC (as well as other major classes of sequential decision problems), and indeed the MPC design architecture is very similar to the one of AlphaZero and TD-Gammon. This conceptual insight provides a vehicle for bridging the cultural gap between RL and MPC, and sheds new light on some fundamental issues in MPC. These include the enhancement of stability properties through rollout, the treatment of uncertainty through the use of certainty equivalence, the resilience of MPC in adaptive control settings that involve changing system parameters, and the insights provided by the superlinear performance bounds implied by Newton's method.",
    "authors": [
      "Dimitri P. Bertsekas"
    ],
    "url": "http://arxiv.org/abs/2406.00592v2",
    "timestamp": 1717293663,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "543603be-6f70-4ecd-a090-443c35374732": {
    "pk": "543603be-6f70-4ecd-a090-443c35374732",
    "title": "Learning to Play Air Hockey with Model-Based Deep Reinforcement Learning",
    "abstract": "In the context of addressing the Robot Air Hockey Challenge 2023, we investigate the applicability of model-based deep reinforcement learning to acquire a policy capable of autonomously playing air hockey. Our agents learn solely from sparse rewards while incorporating self-play to iteratively refine their behaviour over time. The robotic manipulator is interfaced using continuous high-level actions for position-based control in the Cartesian plane while having partial observability of the environment with stochastic transitions. We demonstrate that agents are prone to overfitting when trained solely against a single playstyle, highlighting the importance of self-play for generalization to novel strategies of unseen opponents. Furthermore, the impact of the imagination horizon is explored in the competitive setting of the highly dynamic game of air hockey, with longer horizons resulting in more stable learning and better overall performance.",
    "authors": [
      "Andrej Orsula"
    ],
    "url": "http://arxiv.org/abs/2406.00518v1",
    "timestamp": 1717264801,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d0f903e8-aa3a-4046-88f5-ffcfbe5a8a14": {
    "pk": "d0f903e8-aa3a-4046-88f5-ffcfbe5a8a14",
    "title": "Exploring the limits of Hierarchical World Models in Reinforcement Learning",
    "abstract": "Hierarchical model-based reinforcement learning (HMBRL) aims to combine the benefits of better sample efficiency of model based reinforcement learning (MBRL) with the abstraction capability of hierarchical reinforcement learning (HRL) to solve complex tasks efficiently. While HMBRL has great potential, it still lacks wide adoption. In this work we describe a novel HMBRL framework and evaluate it thoroughly. To complement the multi-layered decision making idiom characteristic for HRL, we construct hierarchical world models that simulate environment dynamics at various levels of temporal abstraction. These models are used to train a stack of agents that communicate in a top-down manner by proposing goals to their subordinate agents. A significant focus of this study is the exploration of a static and environment agnostic temporal abstraction, which allows concurrent training of models and agents throughout the hierarchy. Unlike most goal-conditioned H(MB)RL approaches, it also leads to comparatively low dimensional abstract actions. Although our HMBRL approach did not outperform traditional methods in terms of final episode returns, it successfully facilitated decision making across two levels of abstraction using compact, low dimensional abstract actions. A central challenge in enhancing our method's performance, as uncovered through comprehensive experimentation, is model exploitation on the abstract level of our world model stack. We provide an in depth examination of this issue, discussing its implications for the field and suggesting directions for future research to overcome this challenge. By sharing these findings, we aim to contribute to the broader discourse on refining HMBRL methodologies and to assist in the development of more effective autonomous learning systems for complex decision-making environments.",
    "authors": [
      "Robin Schiewer",
      "Anand Subramoney",
      "Laurenz Wiskott"
    ],
    "url": "http://arxiv.org/abs/2406.00483v1",
    "timestamp": 1717259343,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bc783bee-121e-425a-a6fe-26f92bcb7b69": {
    "pk": "bc783bee-121e-425a-a6fe-26f92bcb7b69",
    "title": "AlignSAM: Aligning Segment Anything Model to Open Context via Reinforcement Learning",
    "abstract": "Powered by massive curated training data, Segment Anything Model (SAM) has demonstrated its impressive generalization capabilities in open-world scenarios with the guidance of prompts. However, the vanilla SAM is class agnostic and heavily relies on user-provided prompts to segment objects of interest. Adapting this method to diverse tasks is crucial for accurate target identification and to avoid suboptimal segmentation results. In this paper, we propose a novel framework, termed AlignSAM, designed for automatic prompting for aligning SAM to an open context through reinforcement learning. Anchored by an agent, AlignSAM enables the generality of the SAM model across diverse downstream tasks while keeping its parameters frozen. Specifically, AlignSAM initiates a prompting agent to iteratively refine segmentation predictions by interacting with the foundational model. It integrates a reinforcement learning policy network to provide informative prompts to the foundational models. Additionally, a semantic recalibration module is introduced to provide fine-grained labels of prompts, enhancing the model's proficiency in handling tasks encompassing explicit and implicit semantics. Experiments conducted on various challenging segmentation tasks among existing foundation models demonstrate the superiority of the proposed AlignSAM over state-of-the-art approaches. Project page: \\url{https://github.com/Duojun-Huang/AlignSAM-CVPR2024}.",
    "authors": [
      "Duojun Huang",
      "Xinyu Xiong",
      "Jie Ma",
      "Jichang Li",
      "Zequn Jie",
      "Lin Ma",
      "Guanbin Li"
    ],
    "url": "http://arxiv.org/abs/2406.00480v1",
    "timestamp": 1717258899,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5e9692be-95dd-4441-980a-33de386ca194": {
    "pk": "5e9692be-95dd-4441-980a-33de386ca194",
    "title": "Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning",
    "abstract": "Cultural accumulation drives the open-ended and diverse progress in capabilities spanning human history. It builds an expanding body of knowledge and skills by combining individual exploration with inter-generational information transmission. Despite its widespread success among humans, the capacity for artificial learning agents to accumulate culture remains under-explored. In particular, approaches to reinforcement learning typically strive for improvements over only a single lifetime. Generational algorithms that do exist fail to capture the open-ended, emergent nature of cultural accumulation, which allows individuals to trade-off innovation and imitation. Building on the previously demonstrated ability for reinforcement learning agents to perform social learning, we find that training setups which balance this with independent learning give rise to cultural accumulation. These accumulating agents outperform those trained for a single lifetime with the same cumulative experience. We explore this accumulation by constructing two models under two distinct notions of a generation: episodic generations, in which accumulation occurs via in-context learning and train-time generations, in which accumulation occurs via in-weights learning. In-context and in-weights cultural accumulation can be interpreted as analogous to knowledge and skill accumulation, respectively. To the best of our knowledge, this work is the first to present general models that achieve emergent cultural accumulation in reinforcement learning, opening up new avenues towards more open-ended learning systems, as well as presenting new opportunities for modelling human culture.",
    "authors": [
      "Jonathan Cook",
      "Chris Lu",
      "Edward Hughes",
      "Joel Z. Leibo",
      "Jakob Foerster"
    ],
    "url": "http://arxiv.org/abs/2406.00392v1",
    "timestamp": 1717238012,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  }
}