{
  "ef5f192d-0b54-48ec-bc37-3876b9ef3f61": {
    "pk": "ef5f192d-0b54-48ec-bc37-3876b9ef3f61",
    "title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography",
    "abstract": "Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current radiologist shortage, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies. Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs). However, current medical VLMs are generally limited to 2D images and short reports, and do not leverage electronic health record (EHR) data for supervision. We introduce Merlin - a 3D VLM that we train using paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens). We evaluate Merlin on 6 task types and 752 individual tasks. The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs). We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines. We derive data scaling laws to empirically assess training data needs for requisite downstream task performance. Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU.",
    "authors": [
      "Louis Blankemeier",
      "Joseph Paul Cohen",
      "Ashwin Kumar",
      "Dave Van Veen",
      "Syed Jamal Safdar Gardezi",
      "Magdalini Paschali",
      "Zhihong Chen",
      "Jean-Benoit Delbrouck",
      "Eduardo Reis",
      "Cesar Truyts",
      "Christian Bluethgen",
      "Malte Engmann Kjeldskov Jensen",
      "Sophie Ostmeier",
      "Maya Varma",
      "Jeya Maria Jose Valanarasu",
      "Zhongnan Fang",
      "Zepeng Huo",
      "Zaid Nabulsi",
      "Diego Ardila",
      "Wei-Hung Weng",
      "Edson Amaro Junior",
      "Neera Ahuja",
      "Jason Fries",
      "Nigam H. Shah",
      "Andrew Johnston",
      "Robert D. Boutin",
      "Andrew Wentland",
      "Curtis P. Langlotz",
      "Jason Hom",
      "Sergios Gatidis",
      "Akshay S. Chaudhari"
    ],
    "url": "http://arxiv.org/abs/2406.06512v1",
    "timestamp": 1718041981,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b7aea0c2-7c1c-48c9-bf80-9d7d94605637": {
    "pk": "b7aea0c2-7c1c-48c9-bf80-9d7d94605637",
    "title": "Which Backbone to Use: A Resource-efficient Domain Specific Comparison for Computer Vision",
    "abstract": "In contemporary computer vision applications, particularly image classification, architectural backbones pre-trained on large datasets like ImageNet are commonly employed as feature extractors. Despite the widespread use of these pre-trained convolutional neural networks (CNNs), there remains a gap in understanding the performance of various resource-efficient backbones across diverse domains and dataset sizes. Our study systematically evaluates multiple lightweight, pre-trained CNN backbones under consistent training settings across a variety of datasets, including natural images, medical images, galaxy images, and remote sensing images. This comprehensive analysis aims to aid machine learning practitioners in selecting the most suitable backbone for their specific problem, especially in scenarios involving small datasets where fine-tuning a pre-trained network is crucial. Even though attention-based architectures are gaining popularity, we observed that they tend to perform poorly under low data finetuning tasks compared to CNNs. We also observed that some CNN architectures such as ConvNeXt, RegNet and EfficientNet performs well compared to others on a diverse set of domains consistently. Our findings provide actionable insights into the performance trade-offs and effectiveness of different backbones, facilitating informed decision-making in model selection for a broad spectrum of computer vision domains. Our code is available here: https://github.com/pranavphoenix/Backbones",
    "authors": [
      "Pranav Jeevan",
      "Amit Sethi"
    ],
    "url": "http://arxiv.org/abs/2406.05612v1",
    "timestamp": 1717898485,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "17e2d0e4-021b-4be2-943b-15ae3d653fd3": {
    "pk": "17e2d0e4-021b-4be2-943b-15ae3d653fd3",
    "title": "Research on the Application of Computer Vision Based on Deep Learning in Autonomous Driving Technology",
    "abstract": "This research aims to explore the application of deep learning in autonomous driving computer vision technology and its impact on improving system performance. By using advanced technologies such as convolutional neural networks (CNN), multi-task joint learning methods, and deep reinforcement learning, this article analyzes in detail the application of deep learning in image recognition, real-time target tracking and classification, environment perception and decision support, and path planning and navigation. Application process in key areas. Research results show that the proposed system has an accuracy of over 98% in image recognition, target tracking and classification, and also demonstrates efficient performance and practicality in environmental perception and decision support, path planning and navigation. The conclusion points out that deep learning technology can significantly improve the accuracy and real-time response capabilities of autonomous driving systems. Although there are still challenges in environmental perception and decision support, with the advancement of technology, it is expected to achieve wider applications and greater capabilities in the future. potential.",
    "authors": [
      "Jingyu Zhang",
      "Jin Cao",
      "Jinghao Chang",
      "Xinjin Li",
      "Houze Liu",
      "Zhenglin Li"
    ],
    "url": "http://arxiv.org/abs/2406.00490v2",
    "timestamp": 1717260084,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1b8c6446-a83a-4fc9-b7b6-965c910d0f58": {
    "pk": "1b8c6446-a83a-4fc9-b7b6-965c910d0f58",
    "title": "DroneVis: Versatile Computer Vision Library for Drones",
    "abstract": "This paper introduces DroneVis, a novel library designed to automate computer vision algorithms on Parrot drones. DroneVis offers a versatile set of features and provides a diverse range of computer vision tasks along with a variety of models to choose from. Implemented in Python, the library adheres to high-quality code standards, facilitating effortless customization and feature expansion according to user requirements. In addition, comprehensive documentation is provided, encompassing usage guidelines and illustrative use cases. Our documentation, code, and examples are available in https://github.com/ahmedheakl/drone-vis.",
    "authors": [
      "Ahmed Heakl",
      "Fatma Youssef",
      "Victor Parque",
      "Walid Gomaa"
    ],
    "url": "http://arxiv.org/abs/2406.00447v1",
    "timestamp": 1717250806,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "adc73829-87bc-4bd6-a366-2072e5c5bd0e": {
    "pk": "adc73829-87bc-4bd6-a366-2072e5c5bd0e",
    "title": "A Review of Pulse-Coupled Neural Network Applications in Computer Vision and Image Processing",
    "abstract": "Research in neural models inspired by mammal's visual cortex has led to many spiking neural networks such as pulse-coupled neural networks (PCNNs). These models are oscillating, spatio-temporal models stimulated with images to produce several time-based responses. This paper reviews PCNN's state of the art, covering its mathematical formulation, variants, and other simplifications found in the literature. We present several applications in which PCNN architectures have successfully addressed some fundamental image processing and computer vision challenges, including image segmentation, edge detection, medical imaging, image fusion, image compression, object recognition, and remote sensing. Results achieved in these applications suggest that the PCNN architecture generates useful perceptual information relevant to a wide variety of computer vision tasks.",
    "authors": [
      "Nurul Rafi",
      "Pablo Rivas"
    ],
    "url": "http://arxiv.org/abs/2406.00239v1",
    "timestamp": 1717200605,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ee29e6c9-3c04-4fd7-ac1f-9e4557c68f63": {
    "pk": "ee29e6c9-3c04-4fd7-ac1f-9e4557c68f63",
    "title": "From Structured to Unstructured:A Comparative Analysis of Computer Vision and Graph Models in solving Mesh-based PDEs",
    "abstract": "This article investigates the application of computer vision and graph-based models in solving mesh-based partial differential equations within high-performance computing environments. Focusing on structured, graded structured, and unstructured meshes, the study compares the performance and computational efficiency of three computer vision-based models against three graph-based models across three data\\-sets. The research aims to identify the most suitable models for different mesh topographies, particularly highlighting the exploration of graded meshes, a less studied area. Results demonstrate that computer vision-based models, notably U-Net, outperform the graph models in prediction performance and efficiency in two (structured and graded) out of three mesh topographies. The study also reveals the unexpected effectiveness of computer vision-based models in handling unstructured meshes, suggesting a potential shift in methodological approaches for data-driven partial differential equation learning. The article underscores deep learning as a viable and potentially sustainable way to enhance traditional high-performance computing methods, advocating for informed model selection based on the topography of the mesh.",
    "authors": [
      "Jens Decke",
      "Olaf W\u00fcnsch",
      "Bernhard Sick",
      "Christian Gruhl"
    ],
    "url": "http://arxiv.org/abs/2406.00081v1",
    "timestamp": 1717158086,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "78608523-ebb5-438e-b57c-535d1e1111d7": {
    "pk": "78608523-ebb5-438e-b57c-535d1e1111d7",
    "title": "Universal and Extensible Language-Vision Models for Organ Segmentation and Tumor Detection from Abdominal Computed Tomography",
    "abstract": "The advancement of artificial intelligence (AI) for organ segmentation and tumor detection is propelled by the growing availability of computed tomography (CT) datasets with detailed, per-voxel annotations. However, these AI models often struggle with flexibility for partially annotated datasets and extensibility for new classes due to limitations in the one-hot encoding, architectural design, and learning scheme. To overcome these limitations, we propose a universal, extensible framework enabling a single model, termed Universal Model, to deal with multiple public datasets and adapt to new classes (e.g., organs/tumors). Firstly, we introduce a novel language-driven parameter generator that leverages language embeddings from large language models, enriching semantic encoding compared with one-hot encoding. Secondly, the conventional output layers are replaced with lightweight, class-specific heads, allowing Universal Model to simultaneously segment 25 organs and six types of tumors and ease the addition of new classes. We train our Universal Model on 3,410 CT volumes assembled from 14 publicly available datasets and then test it on 6,173 CT volumes from four external datasets. Universal Model achieves first place on six CT tasks in the Medical Segmentation Decathlon (MSD) public leaderboard and leading performance on the Beyond The Cranial Vault (BTCV) dataset. In summary, Universal Model exhibits remarkable computational efficiency (6x faster than other dataset-specific models), demonstrates strong generalization across different hospitals, transfers well to numerous downstream tasks, and more importantly, facilitates the extensibility to new classes while alleviating the catastrophic forgetting of previously learned classes. Codes, models, and datasets are available at https://github.com/ljwztc/CLIP-Driven-Universal-Model",
    "authors": [
      "Jie Liu",
      "Yixiao Zhang",
      "Kang Wang",
      "Mehmet Can Yavuz",
      "Xiaoxi Chen",
      "Yixuan Yuan",
      "Haoliang Li",
      "Yang Yang",
      "Alan Yuille",
      "Yucheng Tang",
      "Zongwei Zhou"
    ],
    "url": "http://arxiv.org/abs/2405.18356v1",
    "timestamp": 1716915315,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b3b4efee-6eb0-4d88-b2a0-684dd2126c47": {
    "pk": "b3b4efee-6eb0-4d88-b2a0-684dd2126c47",
    "title": "LucidPPN: Unambiguous Prototypical Parts Network for User-centric Interpretable Computer Vision",
    "abstract": "Prototypical parts networks combine the power of deep learning with the explainability of case-based reasoning to make accurate, interpretable decisions. They follow the this looks like that reasoning, representing each prototypical part with patches from training images. However, a single image patch comprises multiple visual features, such as color, shape, and texture, making it difficult for users to identify which feature is important to the model.   To reduce this ambiguity, we introduce the Lucid Prototypical Parts Network (LucidPPN), a novel prototypical parts network that separates color prototypes from other visual features. Our method employs two reasoning branches: one for non-color visual features, processing grayscale images, and another focusing solely on color information. This separation allows us to clarify whether the model's decisions are based on color, shape, or texture. Additionally, LucidPPN identifies prototypical parts corresponding to semantic parts of classified objects, making comparisons between data classes more intuitive, e.g., when two bird species might differ primarily in belly color.   Our experiments demonstrate that the two branches are complementary and together achieve results comparable to baseline methods. More importantly, LucidPPN generates less ambiguous prototypical parts, enhancing user understanding.",
    "authors": [
      "Mateusz Pach",
      "Dawid Rymarczyk",
      "Koryna Lewandowska",
      "Jacek Tabor",
      "Bartosz Zieli\u0144ski"
    ],
    "url": "http://arxiv.org/abs/2405.14331v1",
    "timestamp": 1716454859,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0f7dc705-2052-4a00-ad20-0c71d4f485d2": {
    "pk": "0f7dc705-2052-4a00-ad20-0c71d4f485d2",
    "title": "SmartCS: Enabling the Creation of ML-Powered Computer Vision Mobile Apps for Citizen Science Applications without Coding",
    "abstract": "It is undeniable that citizen science contributes to the advancement of various fields of study. There are now software tools that facilitate the development of citizen science apps. However, apps developed with these tools rely on individual human skills to correctly collect useful data. Machine learning (ML)-aided apps provide on-field guidance to citizen scientists on data collection tasks. However, these apps rely on server-side ML support, and therefore need a reliable internet connection. Furthermore, the development of citizen science apps with ML support requires a significant investment of time and money. For some projects, this barrier may preclude the use of citizen science effectively. We present a platform that democratizes citizen science by making it accessible to a much broader audience of both researchers and participants. The SmartCS platform allows one to create citizen science apps with ML support quickly and without coding skills. Apps developed using SmartCS have client-side ML support, making them usable in the field, even when there is no internet connection. The client-side ML helps educate users to better recognize the subjects, thereby enabling high-quality data collection. We present several citizen science apps created using SmartCS, some of which were conceived and created by high school students.",
    "authors": [
      "Fahim Hasan Khan",
      "Akila de Silva",
      "Gregory Dusek",
      "James Davis",
      "Alex Pang"
    ],
    "url": "http://arxiv.org/abs/2405.14323v1",
    "timestamp": 1716454490,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "57f66640-fbec-417b-a695-b72aa9a55552": {
    "pk": "57f66640-fbec-417b-a695-b72aa9a55552",
    "title": "Computer-Vision-Enabled Worker Video Analysis for Motion Amount Quantification",
    "abstract": "The performance of physical workers is significantly influenced by the quantity of their motions. However, monitoring and assessing these motions is challenging due to the complexities of motion sensing, tracking, and quantification. Recent advancements have utilized in-situ video analysis for real-time observation of worker behaviors, enabling data-driven quantification of motion amounts. Nevertheless, there are limitations to monitoring worker movements using video data. This paper introduces a novel framework based on computer vision to track and quantify the motion of workers' upper and lower limbs, issuing alerts when the motion reaches critical thresholds. Using joint position data from posture estimation, the framework employs Hotelling's T$^2$ statistic to quantify and monitor motion amounts, integrating computer vision tools to address challenges in automated worker training and enhance exploratory research in this field. We collected data of participants performing lifting and moving tasks with large boxes and small wooden cubes, to simulate macro and micro assembly tasks respectively. It was found that the correlation between workers' joint motion amount and the Hotelling's T$^2$ statistic was approximately 35% greater for micro tasks compared to macro tasks, highlighting the framework's ability to identify fine-grained motion differences. This study demonstrates the effectiveness of the proposed system in real-time applications across various industry settings. It provides a tool for enhancing worker safety and productivity through precision motion analysis and proactive ergonomic adjustments.",
    "authors": [
      "Hari Iyer",
      "Neel Macwan",
      "Shenghan Guo",
      "Heejin Jeong"
    ],
    "url": "http://arxiv.org/abs/2405.13999v1",
    "timestamp": 1716412503,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b23794d1-71ce-45e6-b329-e3462dd2d53b": {
    "pk": "b23794d1-71ce-45e6-b329-e3462dd2d53b",
    "title": "Optimizing Curvature Learning for Robust Hyperbolic Deep Learning in Computer Vision",
    "abstract": "Hyperbolic deep learning has become a growing research direction in computer vision for the unique properties afforded by the alternate embedding space. The negative curvature and exponentially growing distance metric provide a natural framework for capturing hierarchical relationships between datapoints and allowing for finer separability between their embeddings. However, these methods are still computationally expensive and prone to instability, especially when attempting to learn the negative curvature that best suits the task and the data. Current Riemannian optimizers do not account for changes in the manifold which greatly harms performance and forces lower learning rates to minimize projection errors. Our paper focuses on curvature learning by introducing an improved schema for popular learning algorithms and providing a novel normalization approach to constrain embeddings within the variable representative radius of the manifold. Additionally, we introduce a novel formulation for Riemannian AdamW, and alternative hybrid encoder techniques and foundational formulations for current convolutional hyperbolic operations, greatly reducing the computational penalty of the hyperbolic embedding space. Our approach demonstrates consistent performance improvements across both direct classification and hierarchical metric learning tasks while allowing for larger hyperbolic models.",
    "authors": [
      "Ahmad Bdeir",
      "Niels Landwehr"
    ],
    "url": "http://arxiv.org/abs/2405.13979v1",
    "timestamp": 1716409814,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f1e557be-549f-476f-9785-8ff730253e61": {
    "pk": "f1e557be-549f-476f-9785-8ff730253e61",
    "title": "Automating Attendance Management in Human Resources: A Design Science Approach Using Computer Vision and Facial Recognition",
    "abstract": "Haar Cascade is a cost-effective and user-friendly machine learning-based algorithm for detecting objects in images and videos. Unlike Deep Learning algorithms, which typically require significant resources and expensive computing costs, it uses simple image processing techniques like edge detection and Haar features that are easy to comprehend and implement. By combining Haar Cascade with OpenCV2 on an embedded computer like the NVIDIA Jetson Nano, this system can accurately detect and match faces in a database for attendance tracking. This system aims to achieve several specific objectives that set it apart from existing solutions. It leverages Haar Cascade, enriched with carefully selected Haar features, such as Haar-like wavelets, and employs advanced edge detection techniques. These techniques enable precise face detection and matching in both images and videos, contributing to high accuracy and robust performance. By doing so, it minimizes manual intervention and reduces errors, thereby strengthening accountability. Additionally, the integration of OpenCV2 and the NVIDIA Jetson Nano optimizes processing efficiency, making it suitable for resource-constrained environments. This system caters to a diverse range of educational institutions, including schools, colleges, vocational training centers, and various workplace settings such as small businesses, offices, and factories. ... The system's affordability and efficiency democratize attendance management technology, making it accessible to a broader audience. Consequently, it has the potential to transform attendance tracking and management practices, ultimately leading to heightened productivity and accountability. In conclusion, this system represents a groundbreaking approach to attendance tracking and management...",
    "authors": [
      "Bao-Thien Nguyen-Tat",
      "Minh-Quoc Bui",
      "Vuong M. Ngo"
    ],
    "url": "http://arxiv.org/abs/2405.12633v1",
    "timestamp": 1716284336,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9b2e99ec-d7a2-46cd-9949-3166089fa0b0": {
    "pk": "9b2e99ec-d7a2-46cd-9949-3166089fa0b0",
    "title": "Computer Vision in the Food Industry: Accurate, Real-time, and Automatic Food Recognition with Pretrained MobileNetV2",
    "abstract": "In contemporary society, the application of artificial intelligence for automatic food recognition offers substantial potential for nutrition tracking, reducing food waste, and enhancing productivity in food production and consumption scenarios. Modern technologies such as Computer Vision and Deep Learning are highly beneficial, enabling machines to learn automatically, thereby facilitating automatic visual recognition. Despite some research in this field, the challenge of achieving accurate automatic food recognition quickly remains a significant research gap. Some models have been developed and implemented, but maintaining high performance swiftly, with low computational cost and low access to expensive hardware accelerators, still needs further exploration and research. This study employs the pretrained MobileNetV2 model, which is efficient and fast, for food recognition on the public Food11 dataset, comprising 16643 images. It also utilizes various techniques such as dataset understanding, transfer learning, data augmentation, regularization, dynamic learning rate, hyperparameter tuning, and consideration of images in different sizes to enhance performance and robustness. These techniques aid in choosing appropriate metrics, achieving better performance, avoiding overfitting and accuracy fluctuations, speeding up the model, and increasing the generalization of findings, making the study and its results applicable to practical applications. Despite employing a light model with a simpler structure and fewer trainable parameters compared to some deep and dense models in the deep learning area, it achieved commendable accuracy in a short time. This underscores the potential for practical implementation, which is the main intention of this study.",
    "authors": [
      "Shayan Rokhva",
      "Babak Teimourpour",
      "Amir Hossein Soltani"
    ],
    "url": "http://arxiv.org/abs/2405.11621v1",
    "timestamp": 1716139220,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1afc1ff8-1df0-440e-8c7f-cd6aaadb0dab": {
    "pk": "1afc1ff8-1df0-440e-8c7f-cd6aaadb0dab",
    "title": "Analysis and Predictive Modeling of Solar Coronal Holes Using Computer Vision and LSTM Networks",
    "abstract": "In the era of space exploration, coronal holes on the sun play a significant role due to their impact on satellites and aircraft through their open magnetic fields and increased solar wind emissions. This study employs computer vision techniques to detect coronal hole regions and estimate their sizes using imagery from the Solar Dynamics Observatory (SDO). Additionally, we utilize deep learning methods, specifically Long Short-Term Memory (LSTM) networks, to analyze trends in the area of coronal holes and predict their areas across various solar regions over a span of seven days. By examining time series data, we aim to identify patterns in coronal hole behavior and understand their potential effects on space weather. This research enhances our ability to anticipate and prepare for space weather events that could affect Earth's technological systems.",
    "authors": [
      "Juyoung Yun",
      "Jungmin Shin"
    ],
    "url": "http://arxiv.org/abs/2405.09802v1",
    "timestamp": 1715833269,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.SR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ce11dc39-2834-47ba-86c6-8d94deb12a93": {
    "pk": "ce11dc39-2834-47ba-86c6-8d94deb12a93",
    "title": "Malayalam Sign Language Identification using Finetuned YOLOv8 and Computer Vision Techniques",
    "abstract": "Technological advancements and innovations are advancing our daily life in all the ways possible but there is a larger section of society who are deprived of accessing the benefits due to their physical inabilities. To reap the real benefits and make it accessible to society, these talented and gifted people should also use such innovations without any hurdles. Many applications developed these days address these challenges, but localized communities and other constrained linguistic groups may find it difficult to use them. Malayalam, a Dravidian language spoken in the Indian state of Kerala is one of the twenty-two scheduled languages in India. Recent years have witnessed a surge in the development of systems and tools in Malayalam, addressing the needs of Kerala, but many of them are not empathetically designed to cater to the needs of hearing-impaired people. One of the major challenges is the limited or no availability of sign language data for the Malayalam language and sufficient efforts are not made in this direction. In this connection, this paper proposes an approach for sign language identification for the Malayalam language using advanced deep learning and computer vision techniques. We start by developing a labeled dataset for Malayalam letters and for the identification we use advanced deep learning techniques such as YOLOv8 and computer vision. Experimental results show that the identification accuracy is comparable to other sign language identification systems and other researchers in sign language identification can use the model as a baseline to develop advanced models.",
    "authors": [
      "Abhinand K.",
      "Abhiram B. Nair",
      "Dhananjay C.",
      "Hanan Hamza",
      "Mohammed Fawaz J.",
      "Rahma Fahim K.",
      "Anoop V. S"
    ],
    "url": "http://arxiv.org/abs/2405.06702v1",
    "timestamp": 1715168274,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5650dee2-0cd0-4dd0-8e79-041e7793ffb3": {
    "pk": "5650dee2-0cd0-4dd0-8e79-041e7793ffb3",
    "title": "Role of Sensing and Computer Vision in 6G Wireless Communications",
    "abstract": "Recently, we are witnessing the remarkable progress and widespread adoption of sensing technologies in autonomous driving, robotics, and metaverse. Considering the rapid advancement of computer vision (CV) technology to analyze the sensing information, we anticipate a proliferation of wireless applications exploiting the sensing and CV technologies in 6G. In this article, we provide a holistic overview of the sensing and CV-aided wireless communications (SVWC) framework for 6G. By analyzing the high-resolution sensing information through the powerful CV techniques, SVWC can quickly and accurately understand the wireless environments and then perform the wireless tasks. To demonstrate the efficacy of SVWC, we design the whole process of SVWC including the sensing dataset collection, DL model training, and execution of realistic wireless tasks. From the numerical evaluations on 6G communication scenarios, we show that SVWC achieves considerable performance gains over the conventional 5G systems in terms of positioning accuracy, data rate, and access latency.",
    "authors": [
      "Seungnyun Kim",
      "Jihoon Moon",
      "Jinhong Kim",
      "Yongjun Ahn",
      "Donghoon Kim",
      "Sunwoo Kim",
      "Kyuhong Shim",
      "Byonghyo Shim"
    ],
    "url": "http://arxiv.org/abs/2405.03945v1",
    "timestamp": 1715047830,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "18325c5b-1b3a-4a15-a468-1da8dc668d7d": {
    "pk": "18325c5b-1b3a-4a15-a468-1da8dc668d7d",
    "title": "Development of Skip Connection in Deep Neural Networks for Computer Vision and Medical Image Analysis: A Survey",
    "abstract": "Deep learning has made significant progress in computer vision, specifically in image classification, object detection, and semantic segmentation. The skip connection has played an essential role in the architecture of deep neural networks,enabling easier optimization through residual learning during the training stage and improving accuracy during testing. Many neural networks have inherited the idea of residual learning with skip connections for various tasks, and it has been the standard choice for designing neural networks. This survey provides a comprehensive summary and outlook on the development of skip connections in deep neural networks. The short history of skip connections is outlined, and the development of residual learning in deep neural networks is surveyed. The effectiveness of skip connections in the training and testing stages is summarized, and future directions for using skip connections in residual learning are discussed. Finally, we summarize seminal papers, source code, models, and datasets that utilize skip connections in computer vision, including image classification, object detection, semantic segmentation, and image reconstruction. We hope this survey could inspire peer researchers in the community to develop further skip connections in various forms and tasks and the theory of residual learning in deep neural networks. The project page can be found at https://github.com/apple1986/Residual_Learning_For_Images",
    "authors": [
      "Guoping Xu",
      "Xiaxia Wang",
      "Xinglong Wu",
      "Xuesong Leng",
      "Yongchao Xu"
    ],
    "url": "http://arxiv.org/abs/2405.01725v1",
    "timestamp": 1714682638,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a0ef698b-b243-4825-81a2-2a218765dc31": {
    "pk": "a0ef698b-b243-4825-81a2-2a218765dc31",
    "title": "A Computer Vision-Based Quality Assessment Technique for the automatic control of consumables for analytical laboratories",
    "abstract": "The rapid growth of the Industry 4.0 paradigm is increasing the pressure to develop effective automated monitoring systems. Artificial Intelligence (AI) is a convenient tool to improve the efficiency of industrial processes while reducing errors and waste. In fact, it allows the use of real-time data to increase the effectiveness of monitoring systems, minimize errors, make the production process more sustainable, and save costs. In this paper, a novel automatic monitoring system is proposed in the context of production process of plastic consumables used in analysis laboratories, with the aim to increase the effectiveness of the control process currently performed by a human operator. In particular, we considered the problem of classifying the presence or absence of a transparent anticoagulant substance inside test tubes. Specifically, a hand-designed deep network model is used and compared with some state-of-the-art models for its ability to categorize different images of vials that can be either filled with the anticoagulant or empty. Collected results indicate that the proposed approach is competitive with state-of-the-art models in terms of accuracy. Furthermore, we increased the complexity of the task by training the models on the ability to discriminate not only the presence or absence of the anticoagulant inside the vial, but also the size of the test tube. The analysis performed in the latter scenario confirms the competitiveness of our approach. Moreover, our model is remarkably superior in terms of its generalization ability and requires significantly fewer resources. These results suggest the possibility of successfully implementing such a model in the production process of a plastic consumables company.",
    "authors": [
      "Meriam Zribi",
      "Paolo Pagliuca",
      "Francesca Pitolli"
    ],
    "url": "http://arxiv.org/abs/2404.10454v1",
    "timestamp": 1713264616,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7f1174ee-c36c-4908-a7f9-9b0dea79a1d8": {
    "pk": "7f1174ee-c36c-4908-a7f9-9b0dea79a1d8",
    "title": "GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA",
    "abstract": "Graph neural networks (GNNs) have recently empowered various novel computer vision (CV) tasks. In GNN-based CV tasks, a combination of CNN layers and GNN layers or only GNN layers are employed. This paper introduces GCV-Turbo, a domain-specific accelerator on FPGA for end-to-end acceleration of GNN-based CV tasks. GCV-Turbo consists of two key components: (1) a \\emph{novel} hardware architecture optimized for the computation kernels in both CNNs and GNNs using the same set of computation resources. (2) a PyTorch-compatible compiler that takes a user-defined model as input, performs end-to-end optimization for the computation graph of a given GNN-based CV task, and produces optimized code for hardware execution. The hardware architecture and the compiler work synergistically to support a variety of GNN-based CV tasks. We implement GCV-Turbo on a state-of-the-art FPGA and evaluate its performance across six representative GNN-based CV tasks with diverse input data modalities (e.g., image, human skeleton, point cloud). Compared with state-of-the-art CPU (GPU) implementations, GCV-Turbo achieves an average latency reduction of $68.4\\times$ ($4.1\\times$) on these six GNN-based CV tasks. Moreover, GCV-Turbo supports the execution of the standalone CNNs or GNNs, achieving performance comparable to that of state-of-the-art CNN (GNN) accelerators for widely used CNN-only (GNN-only) models.",
    "authors": [
      "Bingyi Zhang",
      "Rajgopal Kannan",
      "Carl Busart",
      "Viktor Prasanna"
    ],
    "url": "http://arxiv.org/abs/2404.07188v1",
    "timestamp": 1712770901,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a040b093-6732-44c1-99d1-e55b866d4097": {
    "pk": "a040b093-6732-44c1-99d1-e55b866d4097",
    "title": "Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images",
    "abstract": "With fine-grained classification, we identify unique characteristics to distinguish among classes of the same super-class. We are focusing on species recognition in Insecta, as they are critical for biodiversity monitoring and at the base of many ecosystems. With citizen science campaigns, billions of images are collected in the wild. Once these are labelled, experts can use them to create distribution maps. However, the labelling process is time-consuming, which is where computer vision comes in. The field of computer vision offers a wide range of algorithms, each with its strengths and weaknesses; how do we identify the algorithm that is in line with our application? To answer this question, we provide a full and detailed evaluation of nine algorithms among deep convolutional networks (CNN), vision transformers (ViT), and locality-based vision transformers (LBVT) on 4 different aspects: classification performance, embedding quality, computational cost, and gradient activity. We offer insights that we haven't yet had in this domain proving to which extent these algorithms solve the fine-grained tasks in Insecta. We found that the ViT performs the best on inference speed and computational cost while the LBVT outperforms the others on performance and embedding quality; the CNN provide a trade-off among the metrics.",
    "authors": [
      "Rita Pucci",
      "Vincent J. Kalkman",
      "Dan Stowell"
    ],
    "url": "http://arxiv.org/abs/2404.03474v1",
    "timestamp": 1712240818,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8c7a170f-cd33-4029-834e-c4b1f4547ebf": {
    "pk": "8c7a170f-cd33-4029-834e-c4b1f4547ebf",
    "title": "Utilizing Computer Vision for Continuous Monitoring of Vaccine Side Effects in Experimental Mice",
    "abstract": "The demand for improved efficiency and accuracy in vaccine safety assessments is increasing. Here, we explore the application of computer vision technologies to automate the monitoring of experimental mice for potential side effects after vaccine administration. Traditional observation methods are labor-intensive and lack the capability for continuous monitoring. By deploying a computer vision system, our research aims to improve the efficiency and accuracy of vaccine safety assessments. The methodology involves training machine learning models on annotated video data of mice behaviors pre- and post-vaccination. Preliminary results indicate that computer vision effectively identify subtle changes, signaling possible side effects. Therefore, our approach has the potential to significantly enhance the monitoring process in vaccine trials in animals, providing a practical solution to the limitations of human observation.",
    "authors": [
      "Chuang Li",
      "Shuai Shao",
      "Willian Mikason",
      "Rubing Lin",
      "Yantong Liu"
    ],
    "url": "http://arxiv.org/abs/2404.03121v1",
    "timestamp": 1712188799,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "77b0b0a0-03b7-4d4a-a7d8-639c6981b5e8": {
    "pk": "77b0b0a0-03b7-4d4a-a7d8-639c6981b5e8",
    "title": "Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns",
    "abstract": "Recent advancements in Computer Assisted Diagnosis have shown promising performance in medical imaging tasks, particularly in chest X-ray analysis. However, the interaction between these models and radiologists has been primarily limited to input images. This work proposes a novel approach to enhance human-computer interaction in chest X-ray analysis using Vision-Language Models (VLMs) enhanced with radiologists' attention by incorporating eye gaze data alongside textual prompts. Our approach leverages heatmaps generated from eye gaze data, overlaying them onto medical images to highlight areas of intense radiologist's focus during chest X-ray evaluation. We evaluate this methodology in tasks such as visual question answering, chest X-ray report automation, error detection, and differential diagnosis. Our results demonstrate the inclusion of eye gaze information significantly enhances the accuracy of chest X-ray analysis. Also, the impact of eye gaze on fine-tuning was confirmed as it outperformed other medical VLMs in all tasks except visual question answering. This work marks the potential of leveraging both the VLM's capabilities and the radiologist's domain knowledge to improve the capabilities of AI models in medical imaging, paving a novel way for Computer Assisted Diagnosis with a human-centred AI.",
    "authors": [
      "Yunsoo Kim",
      "Jinge Wu",
      "Yusuf Abdulle",
      "Yue Gao",
      "Honghan Wu"
    ],
    "url": "http://arxiv.org/abs/2404.02370v1",
    "timestamp": 1712102945,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "de1f2d9c-10b1-4e1f-9d4f-24a3660f8336": {
    "pk": "de1f2d9c-10b1-4e1f-9d4f-24a3660f8336",
    "title": "AI WALKUP: A Computer-Vision Approach to Quantifying MDS-UPDRS in Parkinson's Disease",
    "abstract": "Parkinson's Disease (PD) is the second most common neurodegenerative disorder. The existing assessment method for PD is usually the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to assess the severity of various types of motor symptoms and disease progression. However, manual assessment suffers from high subjectivity, lack of consistency, and high cost and low efficiency of manual communication. We want to use a computer vision based solution to capture human pose images based on a camera, reconstruct and perform motion analysis using algorithms, and extract the features of the amount of motion through feature engineering. The proposed approach can be deployed on different smartphones, and the video recording and artificial intelligence analysis can be done quickly and easily through our APP.",
    "authors": [
      "Xiang Xiang",
      "Zihan Zhang",
      "Jing Ma",
      "Yao Deng"
    ],
    "url": "http://arxiv.org/abs/2404.01654v1",
    "timestamp": 1712037214,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5cb43cdf-67da-475d-a58d-fc78e8be5fda": {
    "pk": "5cb43cdf-67da-475d-a58d-fc78e8be5fda",
    "title": "Exploring Quantum-Enhanced Machine Learning for Computer Vision: Applications and Insights on Noisy Intermediate-Scale Quantum Devices",
    "abstract": "As medium-scale quantum computers progress, the application of quantum algorithms across diverse fields like simulating physical systems, chemistry, optimization, and cryptography becomes more prevalent. However, these quantum computers, known as Noisy Intermediate Scale Quantum (NISQ), are susceptible to noise, prompting the search for applications that can capitalize on quantum advantage without extensive error correction procedures. Since, Machine Learning (ML), particularly Deep Learning (DL), faces challenges due to resource-intensive training and algorithmic opacity. Therefore, this study explores the intersection of quantum computing and ML, focusing on computer vision tasks. Specifically, it evaluates the effectiveness of hybrid quantum-classical algorithms, such as the data re-uploading scheme and the patch Generative Adversarial Networks (GAN) model, on small-scale quantum devices. Through practical implementation and testing, the study reveals comparable or superior performance of these algorithms compared to classical counterparts, highlighting the potential of leveraging quantum algorithms in ML tasks.",
    "authors": [
      "Purnachandra Mandadapu"
    ],
    "url": "http://arxiv.org/abs/2404.02177v1",
    "timestamp": 1712004903,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b589860c-4c47-489c-9fb3-602f3ceec1df": {
    "pk": "b589860c-4c47-489c-9fb3-602f3ceec1df",
    "title": "Intelligent Robotic Control System Based on Computer Vision Technology",
    "abstract": "The article explores the intersection of computer vision technology and robotic control, highlighting its importance in various fields such as industrial automation, healthcare, and environmental protection. Computer vision technology, which simulates human visual observation, plays a crucial role in enabling robots to perceive and understand their surroundings, leading to advancements in tasks like autonomous navigation, object recognition, and waste management. By integrating computer vision with robot control, robots gain the ability to interact intelligently with their environment, improving efficiency.",
    "authors": [
      "Chang Che",
      "Haotian Zheng",
      "Zengyi Huang",
      "Wei Jiang",
      "Bo Liu"
    ],
    "url": "http://arxiv.org/abs/2404.01116v1",
    "timestamp": 1711979183,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "873aa5c5-b3c0-460b-8e24-905249cc064a": {
    "pk": "873aa5c5-b3c0-460b-8e24-905249cc064a",
    "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
    "abstract": "Deep learning techniques have been demonstrated to surpass preceding cutting-edge machine learning techniques in recent years, with computer vision being one of the most prominent examples. However, deep learning models suffer from significant drawbacks when deployed in resource-constrained environments due to their large model size and high complexity. Knowledge Distillation is one of the prominent solutions to overcome this challenge. This review paper examines the current state of research on knowledge distillation, a technique for compressing complex models into smaller and simpler ones. The paper provides an overview of the major principles and techniques associated with knowledge distillation and reviews the applications of knowledge distillation in the domain of computer vision. The review focuses on the benefits of knowledge distillation, as well as the problems that must be overcome to improve its effectiveness.",
    "authors": [
      "Sheikh Musa Kaleem",
      "Tufail Rouf",
      "Gousia Habib",
      "Tausifa jan Saleem",
      "Brejesh Lall"
    ],
    "url": "http://arxiv.org/abs/2404.00936v3",
    "timestamp": 1711950375,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0e5f0e15-8419-44c4-a0af-877f9606b1eb": {
    "pk": "0e5f0e15-8419-44c4-a0af-877f9606b1eb",
    "title": "Statistical Analysis by Semiparametric Additive Regression and LSTM-FCN Based Hierarchical Classification for Computer Vision Quantification of Parkinsonian Bradykinesia",
    "abstract": "Bradykinesia, characterized by involuntary slowing or decrement of movement, is a fundamental symptom of Parkinson's Disease (PD) and is vital for its clinical diagnosis. Despite various methodologies explored to quantify bradykinesia, computer vision-based approaches have shown promising results. However, these methods often fall short in adequately addressing key bradykinesia characteristics in repetitive limb movements: \"occasional arrest\" and \"decrement in amplitude.\"   This research advances vision-based quantification of bradykinesia by introducing nuanced numerical analysis to capture decrement in amplitudes and employing a simple deep learning technique, LSTM-FCN, for precise classification of occasional arrests. Our approach structures the classification process hierarchically, tailoring it to the unique dynamics of bradykinesia in PD.   Statistical analysis of the extracted features, including those representing arrest and fatigue, has demonstrated their statistical significance in most cases. This finding underscores the importance of considering \"occasional arrest\" and \"decrement in amplitude\" in bradykinesia quantification of limb movement. Our enhanced diagnostic tool has been rigorously tested on an extensive dataset comprising 1396 motion videos from 310 PD patients, achieving an accuracy of 80.3%. The results confirm the robustness and reliability of our method.",
    "authors": [
      "Youngseo Cho",
      "In Hee Kwak",
      "Dohyeon Kim",
      "Jinhee Na",
      "Hanjoo Sung",
      "Jeongjae Lee",
      "Young Eun Kim",
      "Hyeo-il Ma"
    ],
    "url": "http://arxiv.org/abs/2404.00670v1",
    "timestamp": 1711888523,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "33c67392-cda1-4279-9dd5-4ffa9dce3993": {
    "pk": "33c67392-cda1-4279-9dd5-4ffa9dce3993",
    "title": "Optimal Blackjack Strategy Recommender: A Comprehensive Study on Computer Vision Integration for Enhanced Gameplay",
    "abstract": "This research project investigates the application of several computer vision techniques for playing card detection and recognition in the context of the popular casino game, blackjack. The primary objective is to develop a robust system that is capable of detecting and accurately classifying playing cards in real-time, and displaying the optimal move recommendation based on the given image of the current game. The proposed methodology involves using K-Means for image segmentation, card reprojection and feature extraction, training of the KNN classifier using a labeled dataset, and integration of the detection system into a Blackjack Basic Strategy recommendation algorithm. Further, the study aims to observe the effectiveness of this approach in detecting various card designs under different lighting conditions and occlusions. Overall, the project examines the potential benefits of incorporating computer vision techniques, with a specific focus on card detection, into commonly played games aiming to enhance player decision-making and optimize strategic outcomes. The results obtained from our experimental evaluations with models developed under considerable time constraints, highlight the potential for practical implementation in real-world casino environments and across other similarly structured games.",
    "authors": [
      "Krishnanshu Gupta",
      "Devon Bolt",
      "Ben Hinchliff"
    ],
    "url": "http://arxiv.org/abs/2404.00191v1",
    "timestamp": 1711754269,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d0e180db-79e5-47e3-a973-e827dc03c81b": {
    "pk": "d0e180db-79e5-47e3-a973-e827dc03c81b",
    "title": "Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks",
    "abstract": "Deep Neural Network (DNN) pruning has emerged as a key strategy to reduce model size, improve inference latency, and lower power consumption on DNN accelerators. Among various pruning techniques, block and output channel pruning have shown significant potential in accelerating hardware performance. However, their accuracy often requires further improvement. In response to this challenge, we introduce a separate, dynamic and differentiable (SMART) pruner. This pruner stands out by utilizing a separate, learnable probability mask for weight importance ranking, employing a differentiable Top k operator to achieve target sparsity, and leveraging a dynamic temperature parameter trick to escape from non-sparse local minima. In our experiments, the SMART pruner consistently demonstrated its superiority over existing pruning methods across a wide range of tasks and models on block and output channel pruning. Additionally, we extend our testing to Transformer-based models in N:M pruning scenarios, where SMART pruner also yields state-of-the-art results, demonstrating its adaptability and robustness across various neural network architectures, and pruning types.",
    "authors": [
      "Guanhua Ding",
      "Zexi Ye",
      "Zhen Zhong",
      "Gang Li",
      "David Shao"
    ],
    "url": "http://arxiv.org/abs/2403.19969v1",
    "timestamp": 1711686486,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5f774638-76d9-46ce-920f-0116c98e2b41": {
    "pk": "5f774638-76d9-46ce-920f-0116c98e2b41",
    "title": "Deep Learning for Robust and Explainable Models in Computer Vision",
    "abstract": "Recent breakthroughs in machine and deep learning (ML and DL) research have provided excellent tools for leveraging enormous amounts of data and optimizing huge models with millions of parameters to obtain accurate networks for image processing. These developments open up tremendous opportunities for using artificial intelligence (AI) in the automation and human assisted AI industry. However, as more and more models are deployed and used in practice, many challenges have emerged. This thesis presents various approaches that address robustness and explainability challenges for using ML and DL in practice.   Robustness and reliability are the critical components of any model before certification and deployment in practice. Deep convolutional neural networks (CNNs) exhibit vulnerability to transformations of their inputs, such as rotation and scaling, or intentional manipulations as described in the adversarial attack literature. In addition, building trust in AI-based models requires a better understanding of current models and developing methods that are more explainable and interpretable a priori.   This thesis presents developments in computer vision models' robustness and explainability. Furthermore, this thesis offers an example of using vision models' feature response visualization (models' interpretations) to improve robustness despite interpretability and robustness being seemingly unrelated in the related research. Besides methodological developments for robust and explainable vision models, a key message of this thesis is introducing model interpretation techniques as a tool for understanding vision models and improving their design and robustness. In addition to the theoretical developments, this thesis demonstrates several applications of ML and DL in different contexts, such as medical imaging and affective computing.",
    "authors": [
      "Mohammadreza Amirian"
    ],
    "url": "http://arxiv.org/abs/2403.18674v1",
    "timestamp": 1711552630,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "21727a59-0d3e-4043-b8a2-edd2e3d3f946": {
    "pk": "21727a59-0d3e-4043-b8a2-edd2e3d3f946",
    "title": "The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency Attacks in Computer Vision",
    "abstract": "Resource efficiency plays an important role for machine learning nowadays. The energy and decision latency are two critical aspects to ensure a sustainable and practical application. Unfortunately, the energy consumption and decision latency are not robust against adversaries. Researchers have recently demonstrated that attackers can compute and submit so-called sponge examples at inference time to increase the energy consumption and decision latency of neural networks. In computer vision, the proposed strategy crafts inputs with less activation sparsity which could otherwise be used to accelerate the computation. In this paper, we analyze the mechanism how these energy-latency attacks reduce activation sparsity. In particular, we find that input uniformity is a key enabler. A uniform image, that is, an image with mostly flat, uniformly colored surfaces, triggers more activations due to a specific interplay of convolution, batch normalization, and ReLU activation. Based on these insights, we propose two new simple, yet effective strategies for crafting sponge examples: sampling images from a probability distribution and identifying dense, yet inconspicuous inputs in natural datasets. We empirically examine our findings in a comprehensive evaluation with multiple image classification models and show that our attack achieves the same sparsity effect as prior sponge-example methods, but at a fraction of computation effort. We also show that our sponge examples transfer between different neural networks. Finally, we discuss applications of our findings for the good by improving efficiency by increasing sparsity.",
    "authors": [
      "Andreas M\u00fcller",
      "Erwin Quiring"
    ],
    "url": "http://arxiv.org/abs/2403.18587v1",
    "timestamp": 1711548683,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "396998da-f732-46f1-820f-377fd480d269": {
    "pk": "396998da-f732-46f1-820f-377fd480d269",
    "title": "TGGLinesPlus: A robust topological graph-guided computer vision algorithm for line detection from images",
    "abstract": "Line detection is a classic and essential problem in image processing, computer vision and machine intelligence. Line detection has many important applications, including image vectorization (e.g., document recognition and art design), indoor mapping, and important societal challenges (e.g., sea ice fracture line extraction from satellite imagery). Many line detection algorithms and methods have been developed, but robust and intuitive methods are still lacking. In this paper, we proposed and implemented a topological graph-guided algorithm, named TGGLinesPlus, for line detection. Our experiments on images from a wide range of domains have demonstrated the flexibility of our TGGLinesPlus algorithm. We also benchmarked our algorithm with five classic and state-of-the-art line detection methods and the results demonstrate the robustness of TGGLinesPlus. We hope our open-source implementation of TGGLinesPlus will inspire and pave the way for many applications where spatial science matters.",
    "authors": [
      "Liping Yang",
      "Joshua Driscol",
      "Ming Gong",
      "Shujie Wang",
      "Catherine G. Potts"
    ],
    "url": "http://arxiv.org/abs/2403.18038v1",
    "timestamp": 1711478996,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ca207334-a22d-4299-ab05-ca93e58369da": {
    "pk": "ca207334-a22d-4299-ab05-ca93e58369da",
    "title": "Improve accessibility for Low Vision and Blind people using Machine Learning and Computer Vision",
    "abstract": "With the ever-growing expansion of mobile technology worldwide, there is an increasing need for accommodation for those who are disabled. This project explores how machine learning and computer vision could be utilized to improve accessibility for people with visual impairments. There have been many attempts to develop various software that would improve accessibility in the day-to-day lives of blind people. However, applications on the market have low accuracy and only provide audio feedback. This project will concentrate on building a mobile application that helps blind people to orient in space by receiving audio and haptic feedback, e.g. vibrations, about their surroundings in real-time. The mobile application will have 3 main features. The initial feature is scanning text from the camera and reading it to a user. This feature can be used on paper with text, in the environment, and on road signs. The second feature is detecting objects around the user, and providing audio feedback about those objects. It also includes providing the description of the objects and their location, and giving haptic feedback if the user is too close to an object. The last feature is currency detection which provides a total amount of currency value to the user via the camera.",
    "authors": [
      "Jasur Shukurov"
    ],
    "url": "http://arxiv.org/abs/2404.00043v1",
    "timestamp": 1711315157,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.HC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cab7e914-822f-440d-b75f-877fce6c0ec0": {
    "pk": "cab7e914-822f-440d-b75f-877fce6c0ec0",
    "title": "A survey of synthetic data augmentation methods in computer vision",
    "abstract": "The standard approach to tackling computer vision problems is to train deep convolutional neural network (CNN) models using large-scale image datasets which are representative of the target task. However, in many scenarios, it is often challenging to obtain sufficient image data for the target task. Data augmentation is a way to mitigate this challenge. A common practice is to explicitly transform existing images in desired ways so as to create the required volume and variability of training data necessary to achieve good generalization performance. In situations where data for the target domain is not accessible, a viable workaround is to synthesize training data from scratch--i.e., synthetic data augmentation. This paper presents an extensive review of synthetic data augmentation techniques. It covers data synthesis approaches based on realistic 3D graphics modeling, neural style transfer (NST), differential neural rendering, and generative artificial intelligence (AI) techniques such as generative adversarial networks (GANs) and variational autoencoders (VAEs). For each of these classes of methods, we focus on the important data generation and augmentation techniques, general scope of application and specific use-cases, as well as existing limitations and possible workarounds. Additionally, we provide a summary of common synthetic datasets for training computer vision models, highlighting the main features, application domains and supported tasks. Finally, we discuss the effectiveness of synthetic data augmentation methods. Since this is the first paper to explore synthetic data augmentation methods in great detail, we are hoping to equip readers with the necessary background information and in-depth knowledge of existing methods and their attendant issues.",
    "authors": [
      "Alhassan Mumuni",
      "Fuseini Mumuni",
      "Nana Kobina Gerrar"
    ],
    "url": "http://arxiv.org/abs/2403.10075v2",
    "timestamp": 1710488048,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a29379cb-f97d-4d4d-bdec-d142d77daeba": {
    "pk": "a29379cb-f97d-4d4d-bdec-d142d77daeba",
    "title": "QCSHQD: Quantum computing as a service for Hybrid classical-quantum software development: A Vision",
    "abstract": "Quantum Computing (QC) is transitioning from theoretical frameworks to an indispensable powerhouse of computational capability, resulting in extensive adoption across both industrial and academic domains. QC presents exceptional advantages, including unparalleled processing speed and the potential to solve complex problems beyond the capabilities of classical computers. Nevertheless, academic researchers and industry practitioners encounter various challenges in harnessing the benefits of this technology. The limited accessibility of QC resources for classical developers, and a general lack of domain knowledge and expertise, represent insurmountable barrier, hence to address these challenges, we introduce a framework- Quantum Computing as a Service for Hybrid Classical-Quantum Software Development (QCSHQD), which leverages service-oriented strategies. Our framework comprises three principal components: an Integrated Development Environment (IDE) for user interaction, an abstraction layer dedicated to orchestrating quantum services, and a service provider responsible for executing services on quantum computer. This study presents a blueprint for QCSHQD, designed to democratize access to QC resources for classical developers who want to seamless harness QC power. The vision of QCSHQD paves the way for groundbreaking innovations by addressing key challenges of hybridization between classical and quantum computers.",
    "authors": [
      "Maryam Tavassoli Sabzevari",
      "Matteo Esposito",
      "Arif Ali Khan",
      "Davide Taibi"
    ],
    "url": "http://arxiv.org/abs/2403.08663v3",
    "timestamp": 1710346603,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6ea18678-e921-4aad-a28b-e46f045dad14": {
    "pk": "6ea18678-e921-4aad-a28b-e46f045dad14",
    "title": "2023 Low-Power Computer Vision Challenge (LPCVC) Summary",
    "abstract": "This article describes the 2023 IEEE Low-Power Computer Vision Challenge (LPCVC). Since 2015, LPCVC has been an international competition devoted to tackling the challenge of computer vision (CV) on edge devices. Most CV researchers focus on improving accuracy, at the expense of ever-growing sizes of machine models. LPCVC balances accuracy with resource requirements. Winners must achieve high accuracy with short execution time when their CV solutions run on an embedded device, such as Raspberry PI or Nvidia Jetson Nano. The vision problem for 2023 LPCVC is segmentation of images acquired by Unmanned Aerial Vehicles (UAVs, also called drones) after disasters. The 2023 LPCVC attracted 60 international teams that submitted 676 solutions during the submission window of one month. This article explains the setup of the competition and highlights the winners' methods that improve accuracy and shorten execution time.",
    "authors": [
      "Leo Chen",
      "Benjamin Boardley",
      "Ping Hu",
      "Yiru Wang",
      "Yifan Pu",
      "Xin Jin",
      "Yongqiang Yao",
      "Ruihao Gong",
      "Bo Li",
      "Gao Huang",
      "Xianglong Liu",
      "Zifu Wan",
      "Xinwang Chen",
      "Ning Liu",
      "Ziyi Zhang",
      "Dongping Liu",
      "Ruijie Shan",
      "Zhengping Che",
      "Fachao Zhang",
      "Xiaofeng Mou",
      "Jian Tang",
      "Maxim Chuprov",
      "Ivan Malofeev",
      "Alexander Goncharenko",
      "Andrey Shcherbin",
      "Arseny Yanchenko",
      "Sergey Alyamkin",
      "Xiao Hu",
      "George K. Thiruvathukal",
      "Yung Hsiang Lu"
    ],
    "url": "http://arxiv.org/abs/2403.07153v1",
    "timestamp": 1710190278,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "05775a75-c28b-4a95-a1aa-b84fd69f7d41": {
    "pk": "05775a75-c28b-4a95-a1aa-b84fd69f7d41",
    "title": "Leveraging Computer Vision in the Intensive Care Unit (ICU) for Examining Visitation and Mobility",
    "abstract": "Despite the importance of closely monitoring patients in the Intensive Care Unit (ICU), many aspects are still assessed in a limited manner due to the time constraints imposed on healthcare providers. For example, although excessive visitations during rest hours can potentially exacerbate the risk of circadian rhythm disruption and delirium, it is not captured in the ICU. Likewise, while mobility can be an important indicator of recovery or deterioration in ICU patients, it is only captured sporadically or not captured at all. In the past few years, the computer vision field has found application in many domains by reducing the human burden. Using computer vision systems in the ICU can also potentially enable non-existing assessments or enhance the frequency and accuracy of existing assessments while reducing the staff workload. In this study, we leverage a state-of-the-art noninvasive computer vision system based on depth imaging to characterize ICU visitations and patients' mobility. We then examine the relationship between visitation and several patient outcomes, such as pain, acuity, and delirium. We found an association between deteriorating patient acuity and the incidence of delirium with increased visitations. In contrast, self-reported pain, reported using the Defense and Veteran Pain Rating Scale (DVPRS), was correlated with decreased visitations. Our findings highlight the feasibility and potential of using noninvasive autonomous systems to monitor ICU patients.",
    "authors": [
      "Scott Siegel",
      "Jiaqing Zhang",
      "Sabyasachi Bandyopadhyay",
      "Subhash Nerella",
      "Brandon Silva",
      "Tezcan Baslanti",
      "Azra Bihorac",
      "Parisa Rashidi"
    ],
    "url": "http://arxiv.org/abs/2403.06322v1",
    "timestamp": 1710107027,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d142b8b9-c8aa-4d4c-adee-7a5330c9c390": {
    "pk": "d142b8b9-c8aa-4d4c-adee-7a5330c9c390",
    "title": "CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming",
    "abstract": "We introduce a new approach using computer vision to predict the land surface displacement from subsurface geometry images for Carbon Capture and Sequestration (CCS). CCS has been proved to be a key component for a carbon neutral society. However, scientists see there are challenges along the way including the high computational cost due to the large model scale and limitations to generalize a pre-trained model with complex physics. We tackle those challenges by training models directly from the subsurface geometry images. The goal is to understand the respons of land surface displacement due to carbon injection and utilize our trained models to inform decision making in CCS projects.   We implement multiple models (CNN, ResNet, and ResNetUNet) for static mechanics problem, which is a image prediction problem. Next, we use the LSTM and transformer for transient mechanics scenario, which is a video prediction problem. It shows ResNetUNet outperforms the others thanks to its architecture in static mechanics problem, and LSTM shows comparable performance to transformer in transient problem. This report proceeds by outlining our dataset in detail followed by model descriptions in method section. Result and discussion state the key learning, observations, and conclusion with future work rounds out the paper.",
    "authors": [
      "Wei Chen",
      "Yunan Li",
      "Yuan Tian"
    ],
    "url": "http://arxiv.org/abs/2403.06025v3",
    "timestamp": 1710023114,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8f213acc-4c13-45ce-8ec7-ac8f7e4cacab": {
    "pk": "8f213acc-4c13-45ce-8ec7-ac8f7e4cacab",
    "title": "Micro-Fracture Detection in Photovoltaic Cells with Hardware-Constrained Devices and Computer Vision",
    "abstract": "Solar energy is rapidly becoming a robust renewable energy source to conventional finite resources such as fossil fuels. It is harvested using interconnected photovoltaic panels, typically built with crystalline silicon cells, i.e. semiconducting materials that convert effectively the solar radiation into electricity. However, crystalline silicon is fragile and vulnerable to cracking over time or in predictive maintenance tasks, which can lead to electric isolation of parts of the solar cell and even failure, thus affecting the panel performance and reducing electricity generation. This work aims to developing a system for detecting cell cracks in solar panels to anticipate and alaert of a potential failure of the photovoltaic system by using computer vision techniques. Three scenarios are defined where these techniques will bring value. In scenario A, images are taken manually and the system detecting failures in the solar cells is not subject to any computationa constraints. In scenario B, an Edge device is placed near the solar farm, able to make inferences. Finally, in scenario C, a small microcontroller is placed in a drone flying over the solar farm and making inferences about the solar cells' states. Three different architectures are found the most suitable solutions, one for each scenario, namely the InceptionV3 model, an EfficientNetB0 model shrunk into full integer quantization, and a customized CNN architechture built with VGG16 blocks.",
    "authors": [
      "Booy Vitas Faassen",
      "Jorge Serrano",
      "Paul D. Rosero-Montalvo"
    ],
    "url": "http://arxiv.org/abs/2403.05694v1",
    "timestamp": 1709935750,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "94e51b8a-8a81-4b3f-882d-9084ed1a53e9": {
    "pk": "94e51b8a-8a81-4b3f-882d-9084ed1a53e9",
    "title": "Quantum Computing: Vision and Challenges",
    "abstract": "The recent development of quantum computing, which uses entanglement, superposition, and other quantum fundamental concepts, can provide substantial processing advantages over traditional computing. These quantum features help solve many complex problems that cannot be solved with conventional computing methods. These problems include modeling quantum mechanics, logistics, chemical-based advances, drug design, statistical science, sustainable energy, banking, reliable communication, and quantum chemical engineering. The last few years have witnessed remarkable advancements in quantum software and algorithm creation and quantum hardware research, which has significantly advanced the prospect of realizing quantum computers. It would be helpful to have comprehensive literature research on this area to grasp the current status and find outstanding problems that require considerable attention from the research community working in the quantum computing industry. To better understand quantum computing, this paper examines the foundations and vision based on current research in this area. We discuss cutting-edge developments in quantum computer hardware advancement and subsequent advances in quantum cryptography, quantum software, and high-scalability quantum computers. Many potential challenges and exciting new trends for quantum technology research and development are highlighted in this paper for a broader debate.",
    "authors": [
      "Sukhpal Singh Gill",
      "Oktay Cetinkaya",
      "Stefano Marrone",
      "Daniel Claudino",
      "David Haunschild",
      "Leon Schlote",
      "Huaming Wu",
      "Carlo Ottaviani",
      "Xiaoyuan Liu",
      "Sree Pragna Machupalli",
      "Kamalpreet Kaur",
      "Priyansh Arora",
      "Ji Liu",
      "Ahmed Farouk",
      "Houbing Herbert Song",
      "Steve Uhlig",
      "Kotagiri Ramamohanarao"
    ],
    "url": "http://arxiv.org/abs/2403.02240v3",
    "timestamp": 1709573598,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c0905d74-cc95-4486-8795-c1e69785c2c3": {
    "pk": "c0905d74-cc95-4486-8795-c1e69785c2c3",
    "title": "Beyond Inference: Performance Analysis of DNN Server Overheads for Computer Vision",
    "abstract": "Deep neural network (DNN) inference has become an important part of many data-center workloads. This has prompted focused efforts to design ever-faster deep learning accelerators such as GPUs and TPUs. However, an end-to-end DNN-based vision application contains more than just DNN inference, including input decompression, resizing, sampling, normalization, and data transfer. In this paper, we perform a thorough evaluation of computer vision inference requests performed on a throughput-optimized serving system. We quantify the performance impact of server overheads such as data movement, preprocessing, and message brokers between two DNNs producing outputs at different rates. Our empirical analysis encompasses many computer vision tasks including image classification, segmentation, detection, depth-estimation, and more complex processing pipelines with multiple DNNs. Our results consistently demonstrate that end-to-end application performance can easily be dominated by data processing and data movement functions (up to 56% of end-to-end latency in a medium-sized image, and $\\sim$ 80% impact on system throughput in a large image), even though these functions have been conventionally overlooked in deep learning system design. Our work identifies important performance bottlenecks in different application scenarios, achieves 2.25$\\times$ better throughput compared to prior work, and paves the way for more holistic deep learning system design.",
    "authors": [
      "Ahmed F. AbouElhamayed",
      "Susanne Balle",
      "Deshanand Singh",
      "Mohamed S. Abdelfattah"
    ],
    "url": "http://arxiv.org/abs/2403.12981v1",
    "timestamp": 1709346908,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2f8b2bd8-92cb-4afe-ba28-142c81c1f2f9": {
    "pk": "2f8b2bd8-92cb-4afe-ba28-142c81c1f2f9",
    "title": "Hydra: Computer Vision for Data Quality Monitoring",
    "abstract": "Hydra is a system which utilizes computer vision to perform near real time data quality management, initially developed for Hall-D in 2019. Since then, it has been deployed across all experimental halls at Jefferson Lab, with the CLAS12 collaboration in Hall-B being the first outside of GlueX to fully utilize Hydra. The system comprises back end processes that manage the models, their inferences, and the data flow. The front-end components, accessible via web pages, allow detector experts and shift crews to view and interact with the system. This talk will give an overview of the Hydra system as well as highlight significant developments in Hydra's feature set, acute challenges with operating Hydra in all halls, and lessons learned along the way.",
    "authors": [
      "Thomas Britton",
      "Torri Jeske",
      "David Lawrence",
      "Kishansingh Rajput"
    ],
    "url": "http://arxiv.org/abs/2403.00689v1",
    "timestamp": 1709313658,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7f50eb36-864f-4f22-9f4a-8d9ca0a7b01f": {
    "pk": "7f50eb36-864f-4f22-9f4a-8d9ca0a7b01f",
    "title": "Computer Vision for Multimedia Geolocation in Human Trafficking Investigation: A Systematic Literature Review",
    "abstract": "The task of multimedia geolocation is becoming an increasingly essential component of the digital forensics toolkit to effectively combat human trafficking, child sexual exploitation, and other illegal acts. Typically, metadata-based geolocation information is stripped when multimedia content is shared via instant messaging and social media. The intricacy of geolocating, geotagging, or finding geographical clues in this content is often overly burdensome for investigators. Recent research has shown that contemporary advancements in artificial intelligence, specifically computer vision and deep learning, show significant promise towards expediting the multimedia geolocation task. This systematic literature review thoroughly examines the state-of-the-art leveraging computer vision techniques for multimedia geolocation and assesses their potential to expedite human trafficking investigation. This includes a comprehensive overview of the application of computer vision-based approaches to multimedia geolocation, identifies their applicability in combating human trafficking, and highlights the potential implications of enhanced multimedia geolocation for prosecuting human trafficking. 123 articles inform this systematic literature review. The findings suggest numerous potential paths for future impactful research on the subject.",
    "authors": [
      "Opeyemi Bamigbade",
      "John Sheppard",
      "Mark Scanlon"
    ],
    "url": "http://arxiv.org/abs/2402.15448v1",
    "timestamp": 1708708986,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ac32d96e-0d6e-4507-96e8-de291312b2ab": {
    "pk": "ac32d96e-0d6e-4507-96e8-de291312b2ab",
    "title": "AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks",
    "abstract": "Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning development process. While traditional AutoML approaches have been successfully applied in several critical steps of model development (e.g. hyperparameter optimization), there lacks a AutoML system that automates the entire end-to-end model production workflow. To fill this blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that follows user's language instructions to automate the whole model production workflow for computer vision tasks. The proposed AutoMMLab system effectively employs LLMs as the bridge to connect AutoML and OpenMMLab community, empowering non-expert individuals to easily build task-specific models via a user-friendly language interface. Specifically, we propose RU-LLaMA to understand users' request and schedule the whole pipeline, and propose a novel LLM-based hyperparameter optimizer called HPO-LLaMA to effectively search for the optimal hyperparameters. Experiments show that our AutoMMLab system is versatile and covers a wide range of mainstream tasks, including classification, detection, segmentation and keypoint estimation. We further develop a new benchmark, called LAMP, for studying key components in the end-to-end prompt-based model training pipeline. Code, model, and data will be released.",
    "authors": [
      "Zekang Yang",
      "Wang Zeng",
      "Sheng Jin",
      "Chen Qian",
      "Ping Luo",
      "Wentao Liu"
    ],
    "url": "http://arxiv.org/abs/2402.15351v1",
    "timestamp": 1708699099,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a7cc2089-b77f-4e0a-992a-d02ab45a815a": {
    "pk": "a7cc2089-b77f-4e0a-992a-d02ab45a815a",
    "title": "ASCEND: Accurate yet Efficient End-to-End Stochastic Computing Acceleration of Vision Transformer",
    "abstract": "Stochastic computing (SC) has emerged as a promising computing paradigm for neural acceleration. However, how to accelerate the state-of-the-art Vision Transformer (ViT) with SC remains unclear. Unlike convolutional neural networks, ViTs introduce notable compatibility and efficiency challenges because of their nonlinear functions, e.g., softmax and Gaussian Error Linear Units (GELU). In this paper, for the first time, a ViT accelerator based on end-to-end SC, dubbed ASCEND, is proposed. ASCEND co-designs the SC circuits and ViT networks to enable accurate yet efficient acceleration. To overcome the compatibility challenges, ASCEND proposes a novel deterministic SC block for GELU and leverages an SC-friendly iterative approximate algorithm to design an accurate and efficient softmax circuit. To improve inference efficiency, ASCEND develops a two-stage training pipeline to produce accurate low-precision ViTs. With extensive experiments, we show the proposed GELU and softmax blocks achieve 56.3% and 22.6% error reduction compared to existing SC designs, respectively and reduce the area-delay product (ADP) by 5.29x and 12.6x, respectively. Moreover, compared to the baseline low-precision ViTs, ASCEND also achieves significant accuracy improvements on CIFAR10 and CIFAR100.",
    "authors": [
      "Tong Xie",
      "Yixuan Hu",
      "Renjie Wei",
      "Meng Li",
      "Yuan Wang",
      "Runsheng Wang",
      "Ru Huang"
    ],
    "url": "http://arxiv.org/abs/2402.12820v1",
    "timestamp": 1708418463,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "42fd4edb-fd4b-474b-9880-9f6cab87f7a2": {
    "pk": "42fd4edb-fd4b-474b-9880-9f6cab87f7a2",
    "title": "Designing High-Performing Networks for Multi-Scale Computer Vision",
    "abstract": "Since the emergence of deep learning, the computer vision field has flourished with models improving at a rapid pace on more and more complex tasks. We distinguish three main ways to improve a computer vision model: (1) improving the data aspect by for example training on a large, more diverse dataset, (2) improving the training aspect by for example designing a better optimizer, and (3) improving the network architecture (or network for short). In this thesis, we chose to improve the latter, i.e. improving the network designs of computer vision models. More specifically, we investigate new network designs for multi-scale computer vision tasks, which are tasks requiring to make predictions about concepts at different scales. The goal of these new network designs is to outperform existing baseline designs from the literature. Specific care is taken to make sure the comparisons are fair, by guaranteeing that the different network designs were trained and evaluated with the same settings. Code is publicly available at https://github.com/CedricPicron/DetSeg.",
    "authors": [
      "C\u00e9dric Picron"
    ],
    "url": "http://arxiv.org/abs/2402.12536v1",
    "timestamp": 1708375855,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7a515127-9fdb-4018-a14b-4a623756ac59": {
    "pk": "7a515127-9fdb-4018-a14b-4a623756ac59",
    "title": "Surround-View Fisheye Optics in Computer Vision and Simulation: Survey and Challenges",
    "abstract": "In this paper, we provide a survey on automotive surround-view fisheye optics, with an emphasis on the impact of optical artifacts on computer vision tasks in autonomous driving and ADAS. The automotive industry has advanced in applying state-of-the-art computer vision to enhance road safety and provide automated driving functionality. When using camera systems on vehicles, there is a particular need for a wide field of view to capture the entire vehicle's surroundings, in areas such as low-speed maneuvering, automated parking, and cocoon sensing. However, one crucial challenge in surround-view cameras is the strong optical aberrations of the fisheye camera, which is an area that has received little attention in the literature. Additionally, a comprehensive dataset is needed for testing safety-critical scenarios in vehicle automation. The industry has turned to simulation as a cost-effective strategy for creating synthetic datasets with surround-view camera imagery. We examine different simulation methods (such as model-driven and data-driven simulations) and discuss the simulators' ability (or lack thereof) to model real-world optical performance. Overall, this paper highlights the optical aberrations in automotive fisheye datasets, and the limitations of optical reality in simulated fisheye datasets, with a focus on computer vision in surround-view optical systems.",
    "authors": [
      "Daniel Jakab",
      "Brian Michael Deegan",
      "Sushil Sharma",
      "Eoin Martino Grua",
      "Jonathan Horgan",
      "Enda Ward",
      "Pepijn Van De Ven",
      "Anthony Scanlan",
      "Ciar\u00e1n Eising"
    ],
    "url": "http://arxiv.org/abs/2402.12041v2",
    "timestamp": 1708340188,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7310fea1-bdb2-40b3-8c26-817d342610c5": {
    "pk": "7310fea1-bdb2-40b3-8c26-817d342610c5",
    "title": "DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and Computer Vision Applications",
    "abstract": "The creation of accurate virtual models of real-world objects is imperative to robotic simulations and applications such as computer vision, artificial intelligence, and machine learning. This paper documents the different methods employed for generating a database of mesh models of real-world objects. These methods address the tedious and time-intensive process of manually generating the models using CAD software. Essentially, DSLR/phone cameras were employed to acquire images of target objects. These images were processed using a photogrammetry software known as Meshroom to generate a dense surface reconstruction of the scene. The result produced by Meshroom was edited and simplified using MeshLab, a mesh-editing software to produce the final model. Based on the obtained models, this process was effective in modelling the geometry and texture of real-world objects with high fidelity. An active 3D scanner was also utilized to accelerate the process for large objects. All generated models and captured images are made available on the website of the project.",
    "authors": [
      "Nillan Nimal",
      "Wenbin Li",
      "Ronald Clark",
      "Sajad Saeedi"
    ],
    "url": "http://arxiv.org/abs/2402.11836v1",
    "timestamp": 1708318720,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "35486e45-b4bb-467c-a93a-9d5edebeae61": {
    "pk": "35486e45-b4bb-467c-a93a-9d5edebeae61",
    "title": "Enhancing Surgical Performance in Cardiothoracic Surgery with Innovations from Computer Vision and Artificial Intelligence: A Narrative Review",
    "abstract": "When technical requirements are high, and patient outcomes are critical, opportunities for monitoring and improving surgical skills via objective motion analysis feedback may be particularly beneficial. This narrative review synthesises work on technical and non-technical surgical skills, collaborative task performance, and pose estimation to illustrate new opportunities to advance cardiothoracic surgical performance with innovations from computer vision and artificial intelligence. These technological innovations are critically evaluated in terms of the benefits they could offer the cardiothoracic surgical community, and any barriers to the uptake of the technology are elaborated upon. Like some other specialities, cardiothoracic surgery has relatively few opportunities to benefit from tools with data capture technology embedded within them (as with robotic-assisted laparoscopic surgery, for example). In such cases, pose estimation techniques that allow for movement tracking across a conventional operating field without using specialist equipment or markers offer considerable potential. With video data from either simulated or real surgical procedures, these tools can (1) provide insight into the development of expertise and surgical performance over a surgeon's career, (2) provide feedback to trainee surgeons regarding areas for improvement, (3) provide the opportunity to investigate what aspects of skill may be linked to patient outcomes which can (4) inform the aspects of surgical skill which should be focused on within training or mentoring programmes. Classifier or assessment algorithms that use artificial intelligence to 'learn' what expertise is from expert surgical evaluators could further assist educators in determining if trainees meet competency thresholds.",
    "authors": [
      "Merryn D. Constable",
      "Hubert P. H. Shum",
      "Stephen Clark"
    ],
    "url": "http://arxiv.org/abs/2402.11288v1",
    "timestamp": 1708179385,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0c83639e-b449-45bc-a8d6-2d70ddee4173": {
    "pk": "0c83639e-b449-45bc-a8d6-2d70ddee4173",
    "title": "A novel integrated industrial approach with cobots in the age of industry 4.0 through conversational interaction and computer vision",
    "abstract": "From robots that replace workers to robots that serve as helpful colleagues, the field of robotic automation is experiencing a new trend that represents a huge challenge for component manufacturers. The contribution starts from an innovative vision that sees an ever closer collaboration between Cobot, able to do a specific physical job with precision, the AI world, able to analyze information and support the decision-making process, and the man able to have a strategic vision of the future.",
    "authors": [
      "Andrea Pazienza",
      "Nicola Macchiarulo",
      "Felice Vitulano",
      "Antonio Fiorentini",
      "Marco Cammisa",
      "Leonardo Rigutini",
      "Ernesto Di Iorio",
      "Achille Globo",
      "Antonio Trevisi"
    ],
    "url": "http://arxiv.org/abs/2402.10553v1",
    "timestamp": 1708079701,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3e518397-1654-4f42-8fd0-d6883fe5b7c1": {
    "pk": "3e518397-1654-4f42-8fd0-d6883fe5b7c1",
    "title": "A Comprehensive Review on Computer Vision Analysis of Aerial Data",
    "abstract": "With the emergence of new technologies in the field of airborne platforms and imaging sensors, aerial data analysis is becoming very popular, capitalizing on its advantages over land data. This paper presents a comprehensive review of the computer vision tasks within the domain of aerial data analysis. While addressing fundamental aspects such as object detection and tracking, the primary focus is on pivotal tasks like change detection, object segmentation, and scene-level analysis. The paper provides the comparison of various hyper parameters employed across diverse architectures and tasks. A substantial section is dedicated to an in-depth discussion on libraries, their categorization, and their relevance to different domain expertise. The paper encompasses aerial datasets, the architectural nuances adopted, and the evaluation metrics associated with all the tasks in aerial data analysis. Applications of computer vision tasks in aerial data across different domains are explored, with case studies providing further insights. The paper thoroughly examines the challenges inherent in aerial data analysis, offering practical solutions. Additionally, unresolved issues of significance are identified, paving the way for future research directions in the field of aerial data analysis.",
    "authors": [
      "Vivek Tetarwal",
      "Sandeep Kumar"
    ],
    "url": "http://arxiv.org/abs/2402.09781v1",
    "timestamp": 1707984609,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6edfaee6-62d9-4c58-8640-ab5a835829d0": {
    "pk": "6edfaee6-62d9-4c58-8640-ab5a835829d0",
    "title": "Beyond the Mud: Datasets and Benchmarks for Computer Vision in Off-Road Racing",
    "abstract": "Despite significant progress in optical character recognition (OCR) and computer vision systems, robustly recognizing text and identifying people in images taken in unconstrained \\emph{in-the-wild} environments remain an ongoing challenge. However, such obstacles must be overcome in practical applications of vision systems, such as identifying racers in photos taken during off-road racing events. To this end, we introduce two new challenging real-world datasets - the off-road motorcycle Racer Number Dataset (RND) and the Muddy Racer re-iDentification Dataset (MUDD) - to highlight the shortcomings of current methods and drive advances in OCR and person re-identification (ReID) under extreme conditions. These two datasets feature over 6,300 images taken during off-road competitions which exhibit a variety of factors that undermine even modern vision systems, namely mud, complex poses, and motion blur. We establish benchmark performance on both datasets using state-of-the-art models. Off-the-shelf models transfer poorly, reaching only 15% end-to-end (E2E) F1 score on text spotting, and 33% rank-1 accuracy on ReID. Fine-tuning yields major improvements, bringing model performance to 53% F1 score for E2E text spotting and 79% rank-1 accuracy on ReID, but still falls short of good performance. Our analysis exposes open problems in real-world OCR and ReID that necessitate domain-targeted techniques. With these datasets and analysis of model limitations, we aim to foster innovations in handling real-world conditions like mud and complex poses to drive progress in robust computer vision. All data was sourced from PerformancePhoto.co, a website used by professional motorsports photographers, racers, and fans. The top-performing text spotting and ReID models are deployed on this platform to power real-time race photo search.",
    "authors": [
      "Jacob Tyo",
      "Motolani Olarinre",
      "Youngseog Chung",
      "Zachary C. Lipton"
    ],
    "url": "http://arxiv.org/abs/2402.08025v1",
    "timestamp": 1707766925,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4b130d54-b771-4b00-b1a3-83e2207d1de1": {
    "pk": "4b130d54-b771-4b00-b1a3-83e2207d1de1",
    "title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
    "abstract": "Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, ScreenAgent, which achieved computer control capabilities comparable to GPT-4V and demonstrated more precise UI positioning capabilities. Our attempts could inspire further research on building a generalist LLM agent. The code is available at \\url{https://github.com/niuzaisheng/ScreenAgent}.",
    "authors": [
      "Runliang Niu",
      "Jindong Li",
      "Shiqi Wang",
      "Yali Fu",
      "Xiyu Hu",
      "Xueyuan Leng",
      "He Kong",
      "Yi Chang",
      "Qi Wang"
    ],
    "url": "http://arxiv.org/abs/2402.07945v1",
    "timestamp": 1707446025,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.HC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "812ba6d8-7b7f-4e8d-9be2-a9d8a0f86a98": {
    "pk": "812ba6d8-7b7f-4e8d-9be2-a9d8a0f86a98",
    "title": "Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision Photometric Redshift Model",
    "abstract": "The availability of large, public, multi-modal astronomical datasets presents an opportunity to execute novel research that straddles the line between science of AI and science of astronomy. Photometric redshift estimation is a well-established subfield of astronomy. Prior works show that computer vision models typically outperform catalog-based models, but these models face additional complexities when incorporating images from more than one instrument or sensor. In this report, we detail our progress creating Mantis Shrimp, a multi-survey computer vision model for photometric redshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery. We use deep learning interpretability diagnostics to measure how the model leverages information from the different inputs. We reason about the behavior of the CNNs from the interpretability metrics, specifically framing the result in terms of physically-grounded knowledge of galaxy properties.",
    "authors": [
      "Andrew Engel",
      "Gautham Narayan",
      "Nell Byler"
    ],
    "url": "http://arxiv.org/abs/2402.03535v1",
    "timestamp": 1707169459,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.IM",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a935f658-5929-401a-9fdf-7a3aff9832f7": {
    "pk": "a935f658-5929-401a-9fdf-7a3aff9832f7",
    "title": "A Computer Vision Based Approach for Stalking Detection Using a CNN-LSTM-MLP Hybrid Fusion Model",
    "abstract": "Criminal and suspicious activity detection has become a popular research topic in recent years. The rapid growth of computer vision technologies has had a crucial impact on solving this issue. However, physical stalking detection is still a less explored area despite the evolution of modern technology. Nowadays, stalking in public places has become a common occurrence with women being the most affected. Stalking is a visible action that usually occurs before any criminal activity begins as the stalker begins to follow, loiter, and stare at the victim before committing any criminal activity such as assault, kidnapping, rape, and so on. Therefore, it has become a necessity to detect stalking as all of these criminal activities can be stopped in the first place through stalking detection. In this research, we propose a novel deep learning-based hybrid fusion model to detect potential stalkers from a single video with a minimal number of frames. We extract multiple relevant features, such as facial landmarks, head pose estimation, and relative distance, as numerical values from video frames. This data is fed into a multilayer perceptron (MLP) to perform a classification task between a stalking and a non-stalking scenario. Simultaneously, the video frames are fed into a combination of convolutional and LSTM models to extract the spatio-temporal features. We use a fusion of these numerical and spatio-temporal features to build a classifier to detect stalking incidents. Additionally, we introduce a dataset consisting of stalking and non-stalking videos gathered from various feature films and television series, which is also used to train the model. The experimental results show the efficiency and dynamism of our proposed stalker detection system, achieving 89.58% testing accuracy with a significant improvement as compared to the state-of-the-art approaches.",
    "authors": [
      "Murad Hasan",
      "Shahriar Iqbal",
      "Md. Billal Hossain Faisal",
      "Md. Musnad Hossin Neloy",
      "Md. Tonmoy Kabir",
      "Md. Tanzim Reza",
      "Md. Golam Rabiul Alam",
      "Md Zia Uddin"
    ],
    "url": "http://arxiv.org/abs/2402.03417v1",
    "timestamp": 1707159234,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "367ce816-3e60-4fee-a5b4-af5878e8999e": {
    "pk": "367ce816-3e60-4fee-a5b4-af5878e8999e",
    "title": "Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey",
    "abstract": "The hybrid of Convolutional Neural Network (CNN) and Vision Transformers (ViT) architectures has emerged as a groundbreaking approach, pushing the boundaries of computer vision (CV). This comprehensive review provides a thorough examination of the literature on state-of-the-art hybrid CNN-ViT architectures, exploring the synergies between these two approaches. The main content of this survey includes: (1) a background on the vanilla CNN and ViT, (2) systematic review of various taxonomic hybrid designs to explore the synergy achieved through merging CNNs and ViTs models, (3) comparative analysis and application task-specific synergy between different hybrid architectures, (4) challenges and future directions for hybrid models, (5) lastly, the survey concludes with a summary of key findings and recommendations. Through this exploration of hybrid CV architectures, the survey aims to serve as a guiding resource, fostering a deeper understanding of the intricate dynamics between CNNs and ViTs and their collective impact on shaping the future of CV architectures.",
    "authors": [
      "Haruna Yunusa",
      "Shiyin Qin",
      "Abdulrahman Hamman Adama Chukkol",
      "Abdulganiyu Abdu Yusuf",
      "Isah Bello",
      "Adamu Lawan"
    ],
    "url": "http://arxiv.org/abs/2402.02941v1",
    "timestamp": 1707134897,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "255a6966-acd2-4e1d-a840-387b04766dfb": {
    "pk": "255a6966-acd2-4e1d-a840-387b04766dfb",
    "title": "SynthVision -- Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data",
    "abstract": "Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision model performance in medical image recognition. The study findings revealed significant insights into the performance of the vision model trained on synthetic images generated through diffusion models. The vision model showed exceptional performance in accurately identifying cases of genital warts. It achieved an accuracy rate of 96% underscoring its effectiveness in medical image classification. For HPV cases the model demonstrated a high precision of 99% and a recall of 94%. In normal cases the precision was 95% with an impressive recall of 99%. These metrics indicate the model capability to correctly identify true positive cases and minimize false positives. The model achieved an F1 Score of 96% for HPV cases and 97% for normal cases. The high F1 Score across both categories highlights the balanced nature of the model precision and recall ensuring reliability and robustness in its predictions.",
    "authors": [
      "Yudara Kularathne",
      "Prathapa Janitha",
      "Sithira Ambepitiya",
      "Thanveer Ahamed",
      "Dinuka Wijesundara",
      "Prarththanan Sothyrajah"
    ],
    "url": "http://arxiv.org/abs/2402.02826v1",
    "timestamp": 1707124729,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cc5d8ba1-c903-4398-96e2-2b5307aaf632": {
    "pk": "cc5d8ba1-c903-4398-96e2-2b5307aaf632",
    "title": "Leveraging Human-Machine Interactions for Computer Vision Dataset Quality Enhancement",
    "abstract": "Large-scale datasets for single-label multi-class classification, such as \\emph{ImageNet-1k}, have been instrumental in advancing deep learning and computer vision. However, a critical and often understudied aspect is the comprehensive quality assessment of these datasets, especially regarding potential multi-label annotation errors. In this paper, we introduce a lightweight, user-friendly, and scalable framework that synergizes human and machine intelligence for efficient dataset validation and quality enhancement. We term this novel framework \\emph{Multilabelfy}. Central to Multilabelfy is an adaptable web-based platform that systematically guides annotators through the re-evaluation process, effectively leveraging human-machine interactions to enhance dataset quality. By using Multilabelfy on the ImageNetV2 dataset, we found that approximately $47.88\\%$ of the images contained at least two labels, underscoring the need for more rigorous assessments of such influential datasets. Furthermore, our analysis showed a negative correlation between the number of potential labels per image and model top-1 accuracy, illuminating a crucial factor in model evaluation and selection. Our open-source framework, Multilabelfy, offers a convenient, lightweight solution for dataset enhancement, emphasizing multi-label proportions. This study tackles major challenges in dataset integrity and provides key insights into model performance evaluation. Moreover, it underscores the advantages of integrating human expertise with machine capabilities to produce more robust models and trustworthy data development. The source code for Multilabelfy will be available at https://github.com/esla/Multilabelfy.   \\keywords{Computer Vision \\and Dataset Quality Enhancement \\and Dataset Validation \\and Human-Computer Interaction \\and Multi-label Annotation.}",
    "authors": [
      "Esla Timothy Anzaku",
      "Hyesoo Hong",
      "Jin-Woo Park",
      "Wonjun Yang",
      "Kangmin Kim",
      "JongBum Won",
      "Deshika Vinoshani Kumari Herath",
      "Arnout Van Messem",
      "Wesley De Neve"
    ],
    "url": "http://arxiv.org/abs/2401.17736v1",
    "timestamp": 1706698627,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0943ffa6-2567-41ff-b851-0c5e154c0385": {
    "pk": "0943ffa6-2567-41ff-b851-0c5e154c0385",
    "title": "OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision",
    "abstract": "Omnidirectional and 360{\\deg} images are becoming widespread in industry and in consumer society, causing omnidirectional computer vision to gain attention. Their wide field of view allows the gathering of a great amount of information about the environment from only an image. However, the distortion of these images requires the development of specific algorithms for their treatment and interpretation. Moreover, a high number of images is essential for the correct training of computer vision algorithms based on learning. In this paper, we present a tool for generating datasets of omnidirectional images with semantic and depth information. These images are synthesized from a set of captures that are acquired in a realistic virtual environment for Unreal Engine 4 through an interface plugin. We gather a variety of well-known projection models such as equirectangular and cylindrical panoramas, different fish-eye lenses, catadioptric systems, and empiric models. Furthermore, we include in our tool photorealistic non-central-projection systems as non-central panoramas and non-central catadioptric systems. As far as we know, this is the first reported tool for generating photorealistic non-central images in the literature. Moreover, since the omnidirectional images are made virtually, we provide pixel-wise information about semantics and depth as well as perfect knowledge of the calibration parameters of the cameras. This allows the creation of ground-truth information with pixel precision for training learning algorithms and testing 3D vision approaches. To validate the proposed tool, different computer vision algorithms are tested as line extractions from dioptric and catadioptric central images, 3D Layout recovery and SLAM using equirectangular panoramas, and 3D reconstruction from non-central panoramas.",
    "authors": [
      "Bruno Berenguel-Baeta",
      "Jesus Bermudez-Cameo",
      "Jose J. Guerrero"
    ],
    "url": "http://arxiv.org/abs/2401.17061v1",
    "timestamp": 1706625619,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f14784a6-5d28-482f-a476-fb6c44e18097": {
    "pk": "f14784a6-5d28-482f-a476-fb6c44e18097",
    "title": "Bridging Human Concepts and Computer Vision for Explainable Face Verification",
    "abstract": "With Artificial Intelligence (AI) influencing the decision-making process of sensitive applications such as Face Verification, it is fundamental to ensure the transparency, fairness, and accountability of decisions. Although Explainable Artificial Intelligence (XAI) techniques exist to clarify AI decisions, it is equally important to provide interpretability of these decisions to humans. In this paper, we present an approach to combine computer and human vision to increase the explanation's interpretability of a face verification algorithm. In particular, we are inspired by the human perceptual process to understand how machines perceive face's human-semantic areas during face comparison tasks. We use Mediapipe, which provides a segmentation technique that identifies distinct human-semantic facial regions, enabling the machine's perception analysis. Additionally, we adapted two model-agnostic algorithms to provide human-interpretable insights into the decision-making processes.",
    "authors": [
      "Miriam Doh",
      "Caroline Mazini Rodrigues",
      "Nicolas Boutry",
      "Laurent Najman",
      "Matei Mancas",
      "Hugues Bersini"
    ],
    "url": "http://arxiv.org/abs/2403.08789v1",
    "timestamp": 1706606029,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5a6f523d-c225-4f1d-a799-d3a17bab3af2": {
    "pk": "5a6f523d-c225-4f1d-a799-d3a17bab3af2",
    "title": "Computer Vision for Primate Behavior Analysis in the Wild",
    "abstract": "Advances in computer vision as well as increasingly widespread video-based behavioral monitoring have great potential for transforming how we study animal cognition and behavior. However, there is still a fairly large gap between the exciting prospects and what can actually be achieved in practice today, especially in videos from the wild. With this perspective paper, we want to contribute towards closing this gap, by guiding behavioral scientists in what can be expected from current methods and steering computer vision researchers towards problems that are relevant to advance research in animal behavior. We start with a survey of the state-of-the-art methods for computer vision problems that are directly relevant to the video-based study of animal behavior, including object detection, multi-individual tracking, (inter)action recognition and individual identification. We then review methods for effort-efficient learning, which is one of the biggest challenges from a practical perspective. Finally, we close with an outlook into the future of the emerging field of computer vision for animal behavior, where we argue that the field should move fast beyond the common frame-by-frame processing and treat video as a first-class citizen.",
    "authors": [
      "Richard Vogg",
      "Timo L\u00fcddecke",
      "Jonathan Henrich",
      "Sharmita Dey",
      "Matthias Nuske",
      "Valentin Hassler",
      "Derek Murphy",
      "Julia Fischer",
      "Julia Ostner",
      "Oliver Sch\u00fclke",
      "Peter M. Kappeler",
      "Claudia Fichtel",
      "Alexander Gail",
      "Stefan Treue",
      "Hansj\u00f6rg Scherberger",
      "Florentin W\u00f6rg\u00f6tter",
      "Alexander S. Ecker"
    ],
    "url": "http://arxiv.org/abs/2401.16424v1",
    "timestamp": 1706554796,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "67aab902-7c82-4a44-b939-ae78bc095a70": {
    "pk": "67aab902-7c82-4a44-b939-ae78bc095a70",
    "title": "Deep Learning for Computer Vision based Activity Recognition and Fall Detection of the Elderly: a Systematic Review",
    "abstract": "As the percentage of elderly people in developed countries increases worldwide, the healthcare of this collective is a worrying matter, especially if it includes the preservation of their autonomy. In this direction, many studies are being published on Ambient Assisted Living (AAL) systems, which help to reduce the preoccupations raised by the independent living of the elderly. In this study, a systematic review of the literature is presented on fall detection and Human Activity Recognition (HAR) for the elderly, as the two main tasks to solve to guarantee the safety of elderly people living alone. To address the current tendency to perform these two tasks, the review focuses on the use of Deep Learning (DL) based approaches on computer vision data. In addition, different collections of data like DL models, datasets or hardware (e.g. depth or thermal cameras) are gathered from the reviewed studies and provided for reference in future studies. Strengths and weaknesses of existing approaches are also discussed and, based on them, our recommendations for future works are provided.",
    "authors": [
      "F. Xavier Gaya-Morey",
      "Cristina Manresa-Yee",
      "Jose M. Buades-Rubio"
    ],
    "url": "http://arxiv.org/abs/2401.11790v1",
    "timestamp": 1705916452,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "47d3243e-afee-4bf9-b225-e96082a1353e": {
    "pk": "47d3243e-afee-4bf9-b225-e96082a1353e",
    "title": "A Survey on African Computer Vision Datasets, Topics and Researchers",
    "abstract": "Computer vision encompasses a range of tasks such as object detection, semantic segmentation, and 3D reconstruction. Despite its relevance to African communities, research in this field within Africa represents only 0.06% of top-tier publications over the past decade. This study undertakes a thorough analysis of 63,000 Scopus-indexed computer vision publications from Africa, spanning from 2012 to 2022. The aim is to provide a survey of African computer vision topics, datasets and researchers. A key aspect of our study is the identification and categorization of African Computer Vision datasets using large language models that automatically parse abstracts of these publications. We also provide a compilation of unofficial African Computer Vision datasets distributed through challenges or data hosting platforms, and provide a full taxonomy of dataset categories. Our survey also pinpoints computer vision topics trends specific to different African regions, indicating their unique focus areas. Additionally, we carried out an extensive survey to capture the views of African researchers on the current state of computer vision research in the continent and the structural barriers they believe need urgent attention. In conclusion, this study catalogs and categorizes Computer Vision datasets and topics contributed or initiated by African institutions and identifies barriers to publishing in top-tier Computer Vision venues. This survey underscores the importance of encouraging African researchers and institutions in advancing computer vision research in the continent. It also stresses on the need for research topics to be more aligned with the needs of African communities.",
    "authors": [
      "Abdul-Hakeem Omotayo",
      "Ashery Mbilinyi",
      "Lukman Ismaila",
      "Houcemeddine Turki",
      "Mahmoud Abdien",
      "Karim Gamal",
      "Idriss Tondji",
      "Yvan Pimi",
      "Naome A. Etori",
      "Marwa M. Matar",
      "Clifford Broni-Bediako",
      "Abigail Oppong",
      "Mai Gamal",
      "Eman Ehab",
      "Gbetondji Dovonon",
      "Zainab Akinjobi",
      "Daniel Ajisafe",
      "Oluwabukola G. Adegboro",
      "Mennatullah Siam"
    ],
    "url": "http://arxiv.org/abs/2401.11617v2",
    "timestamp": 1705877444,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "53fd624d-8c0a-47a4-a9e3-3eb18184211a": {
    "pk": "53fd624d-8c0a-47a4-a9e3-3eb18184211a",
    "title": "How does the primate brain combine generative and discriminative computations in vision?",
    "abstract": "Vision is widely understood as an inference problem. However, two contrasting conceptions of the inference process have each been influential in research on biological vision as well as the engineering of machine vision. The first emphasizes bottom-up signal flow, describing vision as a largely feedforward, discriminative inference process that filters and transforms the visual information to remove irrelevant variation and represent behaviorally relevant information in a format suitable for downstream functions of cognition and behavioral control. In this conception, vision is driven by the sensory data, and perception is direct because the processing proceeds from the data to the latent variables of interest. The notion of \"inference\" in this conception is that of the engineering literature on neural networks, where feedforward convolutional neural networks processing images are said to perform inference. The alternative conception is that of vision as an inference process in Helmholtz's sense, where the sensory evidence is evaluated in the context of a generative model of the causal processes giving rise to it. In this conception, vision inverts a generative model through an interrogation of the evidence in a process often thought to involve top-down predictions of sensory data to evaluate the likelihood of alternative hypotheses. The authors include scientists rooted in roughly equal numbers in each of the conceptions and motivated to overcome what might be a false dichotomy between them and engage the other perspective in the realm of theory and experiment. The primate brain employs an unknown algorithm that may combine the advantages of both conceptions. We explain and clarify the terminology, review the key empirical evidence, and propose an empirical research program that transcends the dichotomy and sets the stage for revealing the mysterious hybrid algorithm of primate vision.",
    "authors": [
      "Benjamin Peters",
      "James J. DiCarlo",
      "Todd Gureckis",
      "Ralf Haefner",
      "Leyla Isik",
      "Joshua Tenenbaum",
      "Talia Konkle",
      "Thomas Naselaris",
      "Kimberly Stachenfeld",
      "Zenna Tavares",
      "Doris Tsao",
      "Ilker Yildirim",
      "Nikolaus Kriegeskorte"
    ],
    "url": "http://arxiv.org/abs/2401.06005v1",
    "timestamp": 1704989278,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "q-bio.NC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "16632e66-11a4-4da0-adbf-231078142fb5": {
    "pk": "16632e66-11a4-4da0-adbf-231078142fb5",
    "title": "Big Data and Deep Learning in Smart Cities: A Comprehensive Dataset for AI-Driven Traffic Accident Detection and Computer Vision Systems",
    "abstract": "In the dynamic urban landscape, where the interplay of vehicles and pedestrians defines the rhythm of life, integrating advanced technology for safety and efficiency is increasingly crucial. This study delves into the application of cutting-edge technological methods in smart cities, focusing on enhancing public safety through improved traffic accident detection. Action recognition plays a pivotal role in interpreting visual data and tracking object motion such as human pose estimation in video sequences. The challenges of action recognition include variability in rapid actions, limited dataset, and environmental factors such as (Weather, Illumination, and Occlusions). In this paper, we present a novel comprehensive dataset for traffic accident detection. This datasets is specifically designed to bolster computer vision and action recognition systems in predicting and detecting road traffic accidents. We integrated datasets from wide variety of data sources, road networks, weather conditions, and regions across the globe. This approach is underpinned by empirical studies, aiming to contribute to the discourse on how technology can enhance the quality of life in densely populated areas. This research aims to bridge existing research gaps by introducing benchmark datasets that leverage state-of-the-art algorithms tailored for traffic accident detection in smart cities. These dataset is expected to advance academic research and also enhance real-time accident detection applications, contributing significantly to the evolution of smart urban environments. Our study marks a pivotal step towards safer, more efficient smart cities, harnessing the power of AI and machine learning to transform urban living.",
    "authors": [
      "Victor Adewopo",
      "Nelly Elsayed",
      "Zag Elsayed",
      "Murat Ozer",
      "Constantinos Zekios",
      "Ahmed Abdelgawad",
      "Magdy Bayoumi"
    ],
    "url": "http://arxiv.org/abs/2401.03587v1",
    "timestamp": 1704664224,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "36ec1372-f958-4dcc-a4f8-646dd66b3b0e": {
    "pk": "36ec1372-f958-4dcc-a4f8-646dd66b3b0e",
    "title": "Entanglement Structure Detection via Computer Vision",
    "abstract": "Quantum entanglement plays a pivotal role in various quantum information processing tasks. However, there still lacks a universal and effective way to detecting entanglement structures, especially for high-dimensional and multipartite quantum systems. Noticing the mathematical similarities between the common representations of many-body quantum states and the data structures of images, we are inspired to employ advanced computer vision technologies for data analysis. In this work, we propose a hybrid CNN-Transformer model for both the classification of GHZ and W states and the detection of various entanglement structures. By leveraging the feature extraction capabilities of CNNs and the powerful modeling abilities of Transformers, we can not only effectively reduce the time and computational resources required for the training process but also obtain high detection accuracies. Through numerical simulation and physical verification, it is confirmed that our hybrid model is more effective than traditional techniques and thus offers a powerful tool for independent detection of multipartite entanglement.",
    "authors": [
      "Rui Li",
      "Junling Du",
      "Zheng Qin",
      "Shikun Zhang",
      "Chunxiao Du",
      "Yang Zhou",
      "Zhisong Xiao"
    ],
    "url": "http://arxiv.org/abs/2401.03400v1",
    "timestamp": 1704611482,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b3939e5b-dad0-445c-b438-9eca348ceb72": {
    "pk": "b3939e5b-dad0-445c-b438-9eca348ceb72",
    "title": "Modern Computing: Vision and Challenges",
    "abstract": "Over the past six decades, the computing systems field has experienced significant transformations, profoundly impacting society with transformational developments, such as the Internet and the commodification of computing. Underpinned by technological advancements, computer systems, far from being static, have been continuously evolving and adapting to cover multifaceted societal niches. This has led to new paradigms such as cloud, fog, edge computing, and the Internet of Things (IoT), which offer fresh economic and creative opportunities. Nevertheless, this rapid change poses complex research challenges, especially in maximizing potential and enhancing functionality. As such, to maintain an economical level of performance that meets ever-tighter requirements, one must understand the drivers of new model emergence and expansion, and how contemporary challenges differ from past ones. To that end, this article investigates and assesses the factors influencing the evolution of computing systems, covering established systems and architectures as well as newer developments, such as serverless computing, quantum computing, and on-device AI on edge devices. Trends emerge when one traces technological trajectory, which includes the rapid obsolescence of frameworks due to business and technical constraints, a move towards specialized systems and models, and varying approaches to centralized and decentralized control. This comprehensive review of modern computing systems looks ahead to the future of research in the field, highlighting key challenges and emerging trends, and underscoring their importance in cost-effectively driving technological progress.",
    "authors": [
      "Sukhpal Singh Gill",
      "Huaming Wu",
      "Panos Patros",
      "Carlo Ottaviani",
      "Priyansh Arora",
      "Victor Casamayor Pujol",
      "David Haunschild",
      "Ajith Kumar Parlikad",
      "Oktay Cetinkaya",
      "Hanan Lutfiyya",
      "Vlado Stankovski",
      "Ruidong Li",
      "Yuemin Ding",
      "Junaid Qadir",
      "Ajith Abraham",
      "Soumya K. Ghosh",
      "Houbing Herbert Song",
      "Rizos Sakellariou",
      "Omer Rana",
      "Joel J. P. C. Rodrigues",
      "Salil S. Kanhere",
      "Schahram Dustdar",
      "Steve Uhlig",
      "Kotagiri Ramamohanarao",
      "Rajkumar Buyya"
    ],
    "url": "http://arxiv.org/abs/2401.02469v1",
    "timestamp": 1704380773,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "652d4060-c20a-450f-b08b-301f53b202a5": {
    "pk": "652d4060-c20a-450f-b08b-301f53b202a5",
    "title": "Horizontal Federated Computer Vision",
    "abstract": "In the modern world, the amount of visual data recorded has been rapidly increasing. In many cases, data is stored in geographically distinct locations and thus requires a large amount of time and space to consolidate. Sometimes, there are also regulations for privacy protection which prevent data consolidation. In this work, we present federated implementations for object detection and recognition using a federated Faster R-CNN (FRCNN) and image segmentation using a federated Fully Convolutional Network (FCN). Our FRCNN was trained on 5000 examples of the COCO2017 dataset while our FCN was trained on the entire train set of the CamVid dataset. The proposed federated models address the challenges posed by the increasing volume and decentralized nature of visual data, offering efficient solutions in compliance with privacy regulations.",
    "authors": [
      "Paul K. Mandal",
      "Cole Leo",
      "Connor Hurley"
    ],
    "url": "http://arxiv.org/abs/2401.00390v2",
    "timestamp": 1703995795,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "120ac2d2-2fdd-474d-b040-0d7923a67eac": {
    "pk": "120ac2d2-2fdd-474d-b040-0d7923a67eac",
    "title": "UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection",
    "abstract": "Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources and leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90% in certain English counties, making them highly susceptible to extinction. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and discarded plastics in the United Kingdom's river ecosystem's. The UDEEP platform can play a crucial role in environmental monitoring by performing on-the-fly classification of Signal crayfish and plastic debris while leveraging the efficacy of AI, IoT devices and the power of edge computing (i.e., NJN). By providing accurate data on the presence, spread and abundance of these species, the UDEEP platform can contribute to monitoring efforts and aid in mitigating the spread of invasive species.",
    "authors": [
      "Dennis Monari",
      "Jack Larkin",
      "Pedro Machado",
      "Jordan J. Bird",
      "Isibor Kennedy Ihianle",
      "Salisu Wada Yahaya",
      "Farhad Fassihi Tash",
      "Md Mahmudul Hasan",
      "Ahmad Lotfi"
    ],
    "url": "http://arxiv.org/abs/2401.06157v1",
    "timestamp": 1703174639,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c04637e2-470c-4d29-9b39-e236dc5b780e": {
    "pk": "c04637e2-470c-4d29-9b39-e236dc5b780e",
    "title": "A Comprehensive End-to-End Computer Vision Framework for Restoration and Recognition of Low-Quality Engineering Drawings",
    "abstract": "The digitization of engineering drawings is crucial for efficient reuse, distribution, and archiving. Existing computer vision approaches for digitizing engineering drawings typically assume the input drawings have high quality. However, in reality, engineering drawings are often blurred and distorted due to improper scanning, storage, and transmission, which may jeopardize the effectiveness of existing approaches. This paper focuses on restoring and recognizing low-quality engineering drawings, where an end-to-end framework is proposed to improve the quality of the drawings and identify the graphical symbols on them. The framework uses K-means clustering to classify different engineering drawing patches into simple and complex texture patches based on their gray level co-occurrence matrix statistics. Computer vision operations and a modified Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) model are then used to improve the quality of the two types of patches, respectively. A modified Faster Region-based Convolutional Neural Network (Faster R-CNN) model is used to recognize the quality-enhanced graphical symbols. Additionally, a multi-stage task-driven collaborative learning strategy is proposed to train the modified ESRGAN and Faster R-CNN models to improve the resolution of engineering drawings in the direction that facilitates graphical symbol recognition, rather than human visual perception. A synthetic data generation method is also proposed to construct quality-degraded samples for training the framework. Experiments on real-world electrical diagrams show that the proposed framework achieves an accuracy of 98.98% and a recall of 99.33%, demonstrating its superiority over previous approaches. Moreover, the framework is integrated into a widely-used power system software application to showcase its practicality.",
    "authors": [
      "Lvyang Yang",
      "Jiankang Zhang",
      "Huaiqiang Li",
      "Longfei Ren",
      "Chen Yang",
      "Jingyu Wang",
      "Dongyuan Shi"
    ],
    "url": "http://arxiv.org/abs/2312.13620v1",
    "timestamp": 1703143345,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9d284868-4c3c-4662-853b-030a89e8c164": {
    "pk": "9d284868-4c3c-4662-853b-030a89e8c164",
    "title": "Integration and Performance Analysis of Artificial Intelligence and Computer Vision Based on Deep Learning Algorithms",
    "abstract": "This paper focuses on the analysis of the application effectiveness of the integration of deep learning and computer vision technologies. Deep learning achieves a historic breakthrough by constructing hierarchical neural networks, enabling end-to-end feature learning and semantic understanding of images. The successful experiences in the field of computer vision provide strong support for training deep learning algorithms. The tight integration of these two fields has given rise to a new generation of advanced computer vision systems, significantly surpassing traditional methods in tasks such as machine vision image classification and object detection. In this paper, typical image classification cases are combined to analyze the superior performance of deep neural network models while also pointing out their limitations in generalization and interpretability, proposing directions for future improvements. Overall, the efficient integration and development trend of deep learning with massive visual data will continue to drive technological breakthroughs and application expansion in the field of computer vision, making it possible to build truly intelligent machine vision systems. This deepening fusion paradigm will powerfully promote unprecedented tasks and functions in computer vision, providing stronger development momentum for related disciplines and industries.",
    "authors": [
      "Bo Liu",
      "Liqiang Yu",
      "Chang Che",
      "Qunwei Lin",
      "Hao Hu",
      "Xinyu Zhao"
    ],
    "url": "http://arxiv.org/abs/2312.12872v1",
    "timestamp": 1703065026,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a6bc1dd1-f6ef-43e7-bd52-ac036a7a0186": {
    "pk": "a6bc1dd1-f6ef-43e7-bd52-ac036a7a0186",
    "title": "Quantum Annealing for Computer Vision Minimization Problems",
    "abstract": "Computer Vision (CV) labelling algorithms play a pivotal role in the domain of low-level vision. For decades, it has been known that these problems can be elegantly formulated as discrete energy minimization problems derived from probabilistic graphical models (such as Markov Random Fields). Despite recent advances in inference algorithms (such as graph-cut and message-passing algorithms), the resulting energy minimization problems are generally viewed as intractable. The emergence of quantum computations, which offer the potential for faster solutions to certain problems than classical methods, has led to an increased interest in utilizing quantum properties to overcome intractable problems. Recently, there has also been a growing interest in Quantum Computer Vision (QCV), with the hope of providing a credible alternative or assistant to deep learning solutions in the field. This study investigates a new Quantum Annealing based inference algorithm for CV discrete energy minimization problems. Our contribution is focused on Stereo Matching as a significant CV labeling problem. As a proof of concept, we also use a hybrid quantum-classical solver provided by D-Wave System to compare our results with the best classical inference algorithms in the literature.",
    "authors": [
      "Shahrokh Heidari",
      "Michael J. Dinneen",
      "Patrice Delmas"
    ],
    "url": "http://arxiv.org/abs/2312.12848v1",
    "timestamp": 1703062595,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a3a03dd5-1f94-4955-bd6a-e8aa7e493491": {
    "pk": "a3a03dd5-1f94-4955-bd6a-e8aa7e493491",
    "title": "Tensor Train Decomposition for Adversarial Attacks on Computer Vision Models",
    "abstract": "Deep neural networks (DNNs) are widely used today, but they are vulnerable to adversarial attacks. To develop effective methods of defense, it is important to understand the potential weak spots of DNNs. Often attacks are organized taking into account the architecture of models (white-box approach) and based on gradient methods, but for real-world DNNs this approach in most cases is impossible. At the same time, several gradient-free optimization algorithms are used to attack black-box models. However, classical methods are often ineffective in the multidimensional case. To organize black-box attacks for computer vision models, in this work, we propose the use of an optimizer based on the low-rank tensor train (TT) format, which has gained popularity in various practical multidimensional applications in recent years. Combined with the attribution of the target image, which is built by the auxiliary (white-box) model, the TT-based optimization method makes it possible to organize an effective black-box attack by small perturbation of pixels in the target image. The superiority of the proposed approach over three popular baselines is demonstrated for five modern DNNs on the ImageNet dataset.",
    "authors": [
      "Andrei Chertkov",
      "Ivan Oseledets"
    ],
    "url": "http://arxiv.org/abs/2312.12556v1",
    "timestamp": 1703015561,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.NA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d17a88da-285f-4018-bec9-765d9a041af9": {
    "pk": "d17a88da-285f-4018-bec9-765d9a041af9",
    "title": "Unified framework for diffusion generative models in SO(3): applications in computer vision and astrophysics",
    "abstract": "Diffusion-based generative models represent the current state-of-the-art for image generation. However, standard diffusion models are based on Euclidean geometry and do not translate directly to manifold-valued data. In this work, we develop extensions of both score-based generative models (SGMs) and Denoising Diffusion Probabilistic Models (DDPMs) to the Lie group of 3D rotations, SO(3). SO(3) is of particular interest in many disciplines such as robotics, biochemistry and astronomy/cosmology science. Contrary to more general Riemannian manifolds, SO(3) admits a tractable solution to heat diffusion, and allows us to implement efficient training of diffusion models. We apply both SO(3) DDPMs and SGMs to synthetic densities on SO(3) and demonstrate state-of-the-art results. Additionally, we demonstrate the practicality of our model on pose estimation tasks and in predicting correlated galaxy orientations for astrophysics/cosmology.",
    "authors": [
      "Yesukhei Jagvaral",
      "Francois Lanusse",
      "Rachel Mandelbaum"
    ],
    "url": "http://arxiv.org/abs/2312.11707v1",
    "timestamp": 1702933623,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "74a2864f-be4a-4b7d-8cbf-e2822bec4044": {
    "pk": "74a2864f-be4a-4b7d-8cbf-e2822bec4044",
    "title": "Orientation-Constrained System for Lamp Detection in Buildings Based on Computer Vision",
    "abstract": "Computer vision is used in this work to detect lighting elements in buildings with the goal of improving the accuracy of previous methods to provide a precise inventory of the location and state of lamps. Using the framework developed in our previous works, we introduce two new modifications to enhance the system: first, a constraint on the orientation of the detected poses in the optimization methods for both the initial and the refined estimates based on the geometric information of the building information modelling (BIM) model; second, an additional reprojection error filtering step to discard the erroneous poses introduced with the orientation restrictions, keeping the identification and localization errors low while greatly increasing the number of detections. These~enhancements are tested in five different case studies with more than 30,000 images, with results showing improvements in the number of detections, the percentage of correct model and state identifications, and the distance between detections and reference positions",
    "authors": [
      "Francisco Troncoso-Pastoriza",
      "Pablo Egu\u00eda-Oller",
      "Rebeca P. D\u00edaz-Redondo",
      "Enrique Granada-\u00c1lvarez",
      "Aitor Erkoreka"
    ],
    "url": "http://arxiv.org/abs/2312.11380v1",
    "timestamp": 1702921435,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0221ba7f-dcb7-4342-9dff-3f2c9d5da99d": {
    "pk": "0221ba7f-dcb7-4342-9dff-3f2c9d5da99d",
    "title": "Information Extraction from Unstructured data using Augmented-AI and Computer Vision",
    "abstract": "Process of information extraction (IE) is often used to extract meaningful information from unstructured and unlabeled data. Conventional methods of data extraction including application of OCR and passing extraction engine, are inefficient on large data and have their limitation. In this paper, a peculiar technique of information extraction is proposed using A2I and computer vision technologies, which also includes NLP.",
    "authors": [
      "Aditya Parikh"
    ],
    "url": "http://arxiv.org/abs/2312.09880v1",
    "timestamp": 1702654061,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4ba1da3b-8bf4-49e4-8f68-b4e82d3d8f8c": {
    "pk": "4ba1da3b-8bf4-49e4-8f68-b4e82d3d8f8c",
    "title": "Flexible visual prompts for in-context learning in computer vision",
    "abstract": "In this work, we address in-context learning (ICL) for the task of image segmentation, introducing a novel approach that adapts a modern Video Object Segmentation (VOS) technique for visual in-context learning. This adaptation is inspired by the VOS method's ability to efficiently and flexibly learn objects from a few examples. Through evaluations across a range of support set sizes and on diverse segmentation datasets, our method consistently surpasses existing techniques. Notably, it excels with data containing classes not encountered during training. Additionally, we propose a technique for support set selection, which involves choosing the most relevant images to include in this set. By employing support set selection, the performance increases for all tested methods without the need for additional training or prompt tuning. The code can be found at https://github.com/v7labs/XMem_ICL/.",
    "authors": [
      "Thomas Foster",
      "Ioana Croitoru",
      "Robert Dorfman",
      "Christoffer Edlund",
      "Thomas Varsavsky",
      "Jon Almaz\u00e1n"
    ],
    "url": "http://arxiv.org/abs/2312.06592v1",
    "timestamp": 1702319262,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5e6d2df0-47e1-40b6-83d6-63cf3a1388d0": {
    "pk": "5e6d2df0-47e1-40b6-83d6-63cf3a1388d0",
    "title": "FM-G-CAM: A Holistic Approach for Explainable AI in Computer Vision",
    "abstract": "Explainability is an aspect of modern AI that is vital for impact and usability in the real world. The main objective of this paper is to emphasise the need to understand the predictions of Computer Vision models, specifically Convolutional Neural Network (CNN) based models. Existing methods of explaining CNN predictions are mostly based on Gradient-weighted Class Activation Maps (Grad-CAM) and solely focus on a single target class. We show that from the point of the target class selection, we make an assumption on the prediction process, hence neglecting a large portion of the predictor CNN model's thinking process. In this paper, we present an exhaustive methodology called Fused Multi-class Gradient-weighted Class Activation Map (FM-G-CAM) that considers multiple top predicted classes, which provides a holistic explanation of the predictor CNN's thinking rationale. We also provide a detailed and comprehensive mathematical and algorithmic description of our method. Furthermore, along with a concise comparison of existing methods, we compare FM-G-CAM with Grad-CAM, highlighting its benefits through real-world practical use cases. Finally, we present an open-source Python library with FM-G-CAM implementation to conveniently generate saliency maps for CNN-based model predictions.",
    "authors": [
      "Ravidu Suien Rammuni Silva",
      "Jordan J. Bird"
    ],
    "url": "http://arxiv.org/abs/2312.05975v2",
    "timestamp": 1702236820,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6b5ca1f3-6dd7-4ae4-afc3-4a2ce46a6b6e": {
    "pk": "6b5ca1f3-6dd7-4ae4-afc3-4a2ce46a6b6e",
    "title": "Jumpstarting Surgical Computer Vision",
    "abstract": "Purpose: General consensus amongst researchers and industry points to a lack of large, representative annotated datasets as the biggest obstacle to progress in the field of surgical data science. Self-supervised learning represents a solution to part of this problem, removing the reliance on annotations. However, the robustness of current self-supervised learning methods to domain shifts remains unclear, limiting our understanding of its utility for leveraging diverse sources of surgical data. Methods: In this work, we employ self-supervised learning to flexibly leverage diverse surgical datasets, thereby learning taskagnostic representations that can be used for various surgical downstream tasks. Based on this approach, to elucidate the impact of pre-training on downstream task performance, we explore 22 different pre-training dataset combinations by modulating three variables: source hospital, type of surgical procedure, and pre-training scale (number of videos). We then finetune the resulting model initializations on three diverse downstream tasks: namely, phase recognition and critical view of safety in laparoscopic cholecystectomy and phase recognition in laparoscopic hysterectomy. Results: Controlled experimentation highlights sizable boosts in performance across various tasks, datasets, and labeling budgets. However, this performance is intricately linked to the composition of the pre-training dataset, robustly proven through several study stages. Conclusion: The composition of pre-training datasets can severely affect the effectiveness of SSL methods for various downstream tasks and should critically inform future data collection efforts to scale the application of SSL methodologies.   Keywords: Self-Supervised Learning, Transfer Learning, Surgical Computer Vision, Endoscopic Videos, Critical View of Safety, Phase Recognition",
    "authors": [
      "Deepak Alapatt",
      "Aditya Murali",
      "Vinkle Srivastav",
      "Pietro Mascagni",
      "AI4SafeChole Consortium",
      "Nicolas Padoy"
    ],
    "url": "http://arxiv.org/abs/2312.05968v1",
    "timestamp": 1702234456,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9b1f6fa7-d5ad-463f-9029-c5506b19c0cc": {
    "pk": "9b1f6fa7-d5ad-463f-9029-c5506b19c0cc",
    "title": "Image and AIS Data Fusion Technique for Maritime Computer Vision Applications",
    "abstract": "Deep learning object detection methods, like YOLOv5, are effective in identifying maritime vessels but often lack detailed information important for practical applications. In this paper, we addressed this problem by developing a technique that fuses Automatic Identification System (AIS) data with vessels detected in images to create datasets. This fusion enriches ship images with vessel-related data, such as type, size, speed, and direction. Our approach associates detected ships to their corresponding AIS messages by estimating distance and azimuth using a homography-based method suitable for both fixed and periodically panning cameras. This technique is useful for creating datasets for waterway traffic management, encounter detection, and surveillance. We introduce a novel dataset comprising of images taken in various weather conditions and their corresponding AIS messages. This dataset offers a stable baseline for refining vessel detection algorithms and trajectory prediction models. To assess our method's performance, we manually annotated a portion of this dataset. The results are showing an overall association accuracy of 74.76 %, with the association accuracy for fixed cameras reaching 85.06 %. This demonstrates the potential of our approach in creating datasets for vessel detection, pose estimation and auto-labelling pipelines.",
    "authors": [
      "Emre G\u00fclsoylu",
      "Paul Koch",
      "Mert Y\u0131ld\u0131z",
      "Manfred Constapel",
      "Andr\u00e9 Peter Kelm"
    ],
    "url": "http://arxiv.org/abs/2312.05270v1",
    "timestamp": 1701982489,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3d2add37-fb61-44a2-9608-c25271c48503": {
    "pk": "3d2add37-fb61-44a2-9608-c25271c48503",
    "title": "Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play",
    "abstract": "Infants' ability to recognize and categorize objects develops gradually. The second year of life is marked by both the emergence of more semantic visual representations and a better understanding of word meaning. This suggests that language input may play an important role in shaping visual representations. However, even in suitable contexts for word learning like dyadic play sessions, caregivers utterances are sparse and ambiguous, often referring to objects that are different from the one to which the child attends. Here, we systematically investigate to what extent caregivers' utterances can nevertheless enhance visual representations. For this we propose a computational model of visual representation learning during dyadic play. We introduce a synthetic dataset of ego-centric images perceived by a toddler-agent that moves and rotates toy objects in different parts of its home environment while hearing caregivers' utterances, modeled as captions. We propose to model toddlers' learning as simultaneously aligning representations for 1) close-in-time images and 2) co-occurring images and utterances. We show that utterances with statistics matching those of real caregivers give rise to representations supporting improved category recognition. Our analysis reveals that a small decrease/increase in object-relevant naming frequencies can drastically impact the learned representations. This affects the attention on object names within an utterance, which is required for efficient visuo-linguistic alignment. Overall, our results support the hypothesis that caregivers' naming utterances can improve toddlers' visual representations.",
    "authors": [
      "Timothy Schauml\u00f6ffel",
      "Arthur Aubret",
      "Gemma Roig",
      "Jochen Triesch"
    ],
    "url": "http://arxiv.org/abs/2312.04118v2",
    "timestamp": 1701937120,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f70a6bb1-f46f-46b5-8dfb-aa82b0616c77": {
    "pk": "f70a6bb1-f46f-46b5-8dfb-aa82b0616c77",
    "title": "IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks",
    "abstract": "In-context learning allows adapting a model to new tasks given a task description at test time. In this paper, we present IMProv - a generative model that is able to in-context learn visual tasks from multimodal prompts. Given a textual description of a visual task (e.g. \"Left: input image, Right: foreground segmentation\"), a few input-output visual examples, or both, the model in-context learns to solve it for a new test input. We train a masked generative transformer on a new dataset of figures from computer vision papers and their associated captions, together with a captioned large-scale image-text dataset. During inference time, we prompt the model with text and/or image task example(s) and have the model inpaint the corresponding output. We show that training our model with text conditioning and scaling the dataset size improves in-context learning for computer vision tasks by over +10\\% AP for Foreground Segmentation, over +5\\% gains in AP for Single Object Detection, and almost 20\\% lower LPIPS in Colorization. Our empirical results suggest that vision and language prompts are complementary and it is advantageous to use both to achieve better in-context learning performance. Project page is available at https://jerryxu.net/IMProv .",
    "authors": [
      "Jiarui Xu",
      "Yossi Gandelsman",
      "Amir Bar",
      "Jianwei Yang",
      "Jianfeng Gao",
      "Trevor Darrell",
      "Xiaolong Wang"
    ],
    "url": "http://arxiv.org/abs/2312.01771v1",
    "timestamp": 1701683309,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "770e8959-4d5e-4aba-b52a-6829c3f67f72": {
    "pk": "770e8959-4d5e-4aba-b52a-6829c3f67f72",
    "title": "Robust Computer Vision in an Ever-Changing World: A Survey of Techniques for Tackling Distribution Shifts",
    "abstract": "AI applications are becoming increasingly visible to the general public. There is a notable gap between the theoretical assumptions researchers make about computer vision models and the reality those models face when deployed in the real world. One of the critical reasons for this gap is a challenging problem known as distribution shift. Distribution shifts tend to vary with complexity of the data, dataset size, and application type. In our paper, we discuss the identification of such a prominent gap, exploring the concept of distribution shift and its critical significance. We provide an in-depth overview of various types of distribution shifts, elucidate their distinctions, and explore techniques within the realm of the data-centric domain employed to address them. Distribution shifts can occur during every phase of the machine learning pipeline, from the data collection stage to the stage of training a machine learning model to the stage of final model deployment. As a result, it raises concerns about the overall robustness of the machine learning techniques for computer vision applications that are deployed publicly for consumers. Different deep learning models each tailored for specific type of data and tasks, architectural pipelines; highlighting how variations in data preprocessing and feature extraction can impact robustness., data augmentation strategies (e.g. geometric, synthetic and learning-based); demonstrating their role in enhancing model generalization, and training mechanisms (e.g. transfer learning, zero-shot) fall under the umbrella of data-centric methods. Each of these components form an integral part of the neural-network we analyze contributing uniquely to strengthening model robustness against distribution shifts. We compare and contrast numerous AI models that are built for mitigating shifts in hidden stratification and spurious correlations, ...",
    "authors": [
      "Eashan Adhikarla",
      "Kai Zhang",
      "Jun Yu",
      "Lichao Sun",
      "John Nicholson",
      "Brian D. Davison"
    ],
    "url": "http://arxiv.org/abs/2312.01540v1",
    "timestamp": 1701646812,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "011b7fa9-301c-41d7-8fc1-ac002fd8b6c8": {
    "pk": "011b7fa9-301c-41d7-8fc1-ac002fd8b6c8",
    "title": "Computer Vision for Increased Operative Efficiency via Identification of Instruments in the Neurosurgical Operating Room: A Proof-of-Concept Study",
    "abstract": "Objectives Computer vision (CV) is a field of artificial intelligence that enables machines to interpret and understand images and videos. CV has the potential to be of assistance in the operating room (OR) to track surgical instruments. We built a CV algorithm for identifying surgical instruments in the neurosurgical operating room as a potential solution for surgical instrument tracking and management to decrease surgical waste and opening of unnecessary tools. Methods We collected 1660 images of 27 commonly used neurosurgical instruments. Images were labeled using the VGG Image Annotator and split into 80% training and 20% testing sets in order to train a U-Net Convolutional Neural Network using 5-fold cross validation. Results Our U-Net achieved a tool identification accuracy of 80-100% when distinguishing 25 classes of instruments, with 19/25 classes having accuracy over 90%. The model performance was not adequate for sub classifying Adson, Gerald, and Debakey forceps, which had accuracies of 60-80%. Conclusions We demonstrated the viability of using machine learning to accurately identify surgical instruments. Instrument identification could help optimize surgical tray packing, decrease tool usage and waste, decrease incidence of instrument misplacement events, and assist in timing of routine instrument maintenance. More training data will be needed to increase accuracy across all surgical instruments that would appear in a neurosurgical operating room. Such technology has the potential to be used as a method to be used for proving what tools are truly needed in each type of operation allowing surgeons across the world to do more with less.",
    "authors": [
      "Tanner J. Zachem",
      "Sully F. Chen",
      "Vishal Venkatraman",
      "David AW Sykes",
      "Ravi Prakash",
      "Koumani W. Ntowe",
      "Mikhail A. Bethell",
      "Samantha Spellicy",
      "Alexander D Suarez",
      "Weston Ross",
      "Patrick J. Codd"
    ],
    "url": "http://arxiv.org/abs/2312.03001v2",
    "timestamp": 1701630110,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ae36d1c1-d1ef-4dbf-8582-ae55d0f9e61f": {
    "pk": "ae36d1c1-d1ef-4dbf-8582-ae55d0f9e61f",
    "title": "Deep Metric Learning for Computer Vision: A Brief Overview",
    "abstract": "Objective functions that optimize deep neural networks play a vital role in creating an enhanced feature representation of the input data. Although cross-entropy-based loss formulations have been extensively used in a variety of supervised deep-learning applications, these methods tend to be less adequate when there is large intra-class variance and low inter-class variance in input data distribution. Deep Metric Learning seeks to develop methods that aim to measure the similarity between data samples by learning a representation function that maps these data samples into a representative embedding space. It leverages carefully designed sampling strategies and loss functions that aid in optimizing the generation of a discriminative embedding space even for distributions having low inter-class and high intra-class variances. In this chapter, we will provide an overview of recent progress in this area and discuss state-of-the-art Deep Metric Learning approaches.",
    "authors": [
      "Deen Dayal Mohan",
      "Bhavin Jawade",
      "Srirangaraj Setlur",
      "Venu Govindaraj"
    ],
    "url": "http://arxiv.org/abs/2312.10046v1",
    "timestamp": 1701467616,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "32e1f60f-fc25-402b-8321-4fc04e7b8af6": {
    "pk": "32e1f60f-fc25-402b-8321-4fc04e7b8af6",
    "title": "RadioGalaxyNET: Dataset and Novel Computer Vision Algorithms for the Detection of Extended Radio Galaxies and Infrared Hosts",
    "abstract": "Creating radio galaxy catalogues from next-generation deep surveys requires automated identification of associated components of extended sources and their corresponding infrared hosts. In this paper, we introduce RadioGalaxyNET, a multimodal dataset, and a suite of novel computer vision algorithms designed to automate the detection and localization of multi-component extended radio galaxies and their corresponding infrared hosts. The dataset comprises 4,155 instances of galaxies in 2,800 images with both radio and infrared channels. Each instance provides information about the extended radio galaxy class, its corresponding bounding box encompassing all components, the pixel-level segmentation mask, and the keypoint position of its corresponding infrared host galaxy. RadioGalaxyNET is the first dataset to include images from the highly sensitive Australian Square Kilometre Array Pathfinder (ASKAP) radio telescope, corresponding infrared images, and instance-level annotations for galaxy detection. We benchmark several object detection algorithms on the dataset and propose a novel multimodal approach to simultaneously detect radio galaxies and the positions of infrared hosts.",
    "authors": [
      "Nikhel Gupta",
      "Zeeshan Hayder",
      "Ray P. Norris",
      "Minh Huynh",
      "Lars Petersson"
    ],
    "url": "http://arxiv.org/abs/2312.00306v1",
    "timestamp": 1701399278,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.IM",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "789fd892-6f50-4764-95f9-fc56fe78f0f2": {
    "pk": "789fd892-6f50-4764-95f9-fc56fe78f0f2",
    "title": "Adaptability of Computer Vision at the Tactical Edge: Addressing Environmental Uncertainty",
    "abstract": "Computer Vision (CV) systems are increasingly being adopted into Command and Control (C2) systems to improve intelligence analysis on the battlefield, the tactical edge. CV systems leverage Artificial Intelligence (AI) algorithms to help visualize and interpret the environment, enhancing situational awareness. However, the adaptability of CV systems at the tactical edge remains challenging due to rapidly changing environments and objects which can confuse the deployed models. A CV model leveraged in this environment can become uncertain in its predictions, as the environment and the objects existing in the environment begin to change. Additionally, mission objectives can rapidly change leading to adjustments in technology, camera angles, and image resolutions. All of which can negatively affect the performance of and potentially introduce uncertainty into the system. When the training environment and/or technology differs from the deployment environment, CV models can perform unexpectedly. Unfortunately, most scenarios at the tactical edge do not incorporate Uncertainty Quantification (UQ) into their deployed C2 and CV systems. This concept paper explores the idea of synchronizing robust data operations and model fine-tuning driven by UQ all at the tactical edge. Specifically, curating datasets and training child models based on the residuals of predictions, using these child models to calculate prediction intervals (PI), and then using these PI to calibrate the deployed models. By incorporating UQ into the core operations surrounding C2 and CV systems at the tactical edge, we can help drive purposeful adaptability on the battlefield.",
    "authors": [
      "Hayden Moore"
    ],
    "url": "http://arxiv.org/abs/2312.00269v1",
    "timestamp": 1701392639,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c5029552-e08b-48b2-8e1d-8fea9d0ced99": {
    "pk": "c5029552-e08b-48b2-8e1d-8fea9d0ced99",
    "title": "Large Language Models Meet Computer Vision: A Brief Survey",
    "abstract": "Recently, the intersection of Large Language Models (LLMs) and Computer Vision (CV) has emerged as a pivotal area of research, driving significant advancements in the field of Artificial Intelligence (AI). As transformers have become the backbone of many state-of-the-art models in both Natural Language Processing (NLP) and CV, understanding their evolution and potential enhancements is crucial. This survey paper delves into the latest progressions in the domain of transformers and their subsequent successors, emphasizing their potential to revolutionize Vision Transformers (ViTs) and LLMs. This survey also presents a comparative analysis, juxtaposing the performance metrics of several leading paid and open-source LLMs, shedding light on their strengths and areas of improvement as well as a literature review on how LLMs are being used to tackle vision related tasks. Furthermore, the survey presents a comprehensive collection of datasets employed to train LLMs, offering insights into the diverse data available to achieve high performance in various pre-training and downstream tasks of LLMs. The survey is concluded by highlighting open directions in the field, suggesting potential venues for future research and development. This survey aims to underscores the profound intersection of LLMs on CV, leading to a new era of integrated and advanced AI models.",
    "authors": [
      "Raby Hamadi"
    ],
    "url": "http://arxiv.org/abs/2311.16673v1",
    "timestamp": 1701167959,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "34ffdbbb-5db7-45b7-b6d8-018332f0c0fa": {
    "pk": "34ffdbbb-5db7-45b7-b6d8-018332f0c0fa",
    "title": "Integration of Robotics, Computer Vision, and Algorithm Design: A Chinese Poker Self-Playing Robot",
    "abstract": "This paper presents Chinese Poker Self-Playing Robot, an integrated system enabling a TM5-900 robotic arm to independently play the four-person card game Chinese poker. The robot uses a custom sucker mechanism to pick up and play cards. An object detection model based on YOLOv5 is utilized to recognize the suit and number of 13 cards dealt to the robot. A greedy algorithm is developed to divide the 13 cards into optimal hands of 3, 5, and 5 cards to play. Experiments demonstrate that the robot can successfully obtain the cards, identify them using computer vision, strategically select hands to play using the algorithm, and physically play the selected cards in the game. The system showcases effective integration of mechanical design, computer vision, algorithm design, and robotic control to accomplish the complex task of independently playing cards.",
    "authors": [
      "Kuan-Huang Yu"
    ],
    "url": "http://arxiv.org/abs/2312.09455v1",
    "timestamp": 1701154414,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ca777a79-3252-497b-9a3e-a019037ccee9": {
    "pk": "ca777a79-3252-497b-9a3e-a019037ccee9",
    "title": "Computer Vision for Carriers: PATRIOT",
    "abstract": "Deck tracking performed on carriers currently involves a team of sailors manually identifying aircraft and updating a digital user interface called the Ouija Board. Improvements to the deck tracking process would result in increased Sortie Generation Rates, and therefore applying automation is seen as a critical method to improve deck tracking. However, the requirements on a carrier ship do not allow for the installation of hardware-based location sensing technologies like Global Positioning System (GPS) sensors. PATRIOT (Panoramic Asset Tracking of Real-Time Information for the Ouija Tabletop) is a research effort and proposed solution to performing deck tracking with passive sensing and without the need for GPS sensors. PATRIOT is a prototype system which takes existing camera feeds, calculates aircraft poses, and updates a virtual Ouija board interface with the current status of the assets. PATRIOT would allow for faster, more accurate, and less laborious asset tracking for aircraft, people, and support equipment. PATRIOT is anticipated to benefit the warfighter by reducing cognitive workload, reducing manning requirements, collecting data to improve logistics, and enabling an automation gateway for future efforts to improve efficiency and safety. The authors have developed and tested algorithms to perform pose estimations of assets in real-time including OpenPifPaf, High-Resolution Network (HRNet), HigherHRNet (HHRNet), Faster R-CNN, and in-house developed encoder-decoder network. The software was tested with synthetic and real-world data and was able to accurately extract the pose of assets. Fusion, tracking, and real-world generality are planned to be improved to ensure a successful transition to the fleet.",
    "authors": [
      "Ari Goodman",
      "Gurpreet Singh",
      "James Hing",
      "Ryan O'Shea"
    ],
    "url": "http://arxiv.org/abs/2311.15914v1",
    "timestamp": 1701098605,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "22fc465b-1259-4045-834c-6ed8d7f35010": {
    "pk": "22fc465b-1259-4045-834c-6ed8d7f35010",
    "title": "Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision",
    "abstract": "Computer vision models have been known to encode harmful biases, leading to the potentially unfair treatment of historically marginalized groups, such as people of color. However, there remains a lack of datasets balanced along demographic traits that can be used to evaluate the downstream fairness of these models. In this work, we demonstrate that diffusion models can be leveraged to create such a dataset. We first use a diffusion model to generate a large set of images depicting various occupations. Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race. Using this dataset, we benchmark several vision-language models on a multi-class occupation classification task. We find that images generated with non-Caucasian labels have a significantly higher occupation misclassification rate than images generated with Caucasian labels, and that several misclassifications are suggestive of racial biases. We measure a model's downstream fairness by computing the standard deviation in the probability of predicting the true occupation label across the different perceived identity groups. Using this fairness metric, we find significant disparities between the evaluated vision-and-language models. We hope that our work demonstrates the potential value of diffusion methods for fairness evaluations.",
    "authors": [
      "Nicholas Lui",
      "Bryan Chia",
      "William Berrios",
      "Candace Ross",
      "Douwe Kiela"
    ],
    "url": "http://arxiv.org/abs/2311.15108v2",
    "timestamp": 1700941213,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d0c653f3-cc00-4b32-8258-a31429d1e4c1": {
    "pk": "d0c653f3-cc00-4b32-8258-a31429d1e4c1",
    "title": "The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024",
    "abstract": "The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024 addresses maritime computer vision for Unmanned Aerial Vehicles (UAV) and Unmanned Surface Vehicles (USV). Three challenges categories are considered: (i) UAV-based Maritime Object Tracking with Re-identification, (ii) USV-based Maritime Obstacle Segmentation and Detection, (iii) USV-based Maritime Boat Tracking. The USV-based Maritime Obstacle Segmentation and Detection features three sub-challenges, including a new embedded challenge addressing efficicent inference on real-world embedded devices. This report offers a comprehensive overview of the findings from the challenges. We provide both statistical and qualitative analyses, evaluating trends from over 195 submissions. All datasets, evaluation code, and the leaderboard are available to the public at https://macvi.org/workshop/macvi24.",
    "authors": [
      "Benjamin Kiefer",
      "Lojze \u017dust",
      "Matej Kristan",
      "Janez Per\u0161",
      "Matija Ter\u0161ek",
      "Arnold Wiliem",
      "Martin Messmer",
      "Cheng-Yen Yang",
      "Hsiang-Wei Huang",
      "Zhongyu Jiang",
      "Heng-Cheng Kuo",
      "Jie Mei",
      "Jenq-Neng Hwang",
      "Daniel Stadler",
      "Lars Sommer",
      "Kaer Huang",
      "Aiguo Zheng",
      "Weitu Chong",
      "Kanokphan Lertniphonphan",
      "Jun Xie",
      "Feng Chen",
      "Jian Li",
      "Zhepeng Wang",
      "Luca Zedda",
      "Andrea Loddo",
      "Cecilia Di Ruberto",
      "Tuan-Anh Vu",
      "Hai Nguyen-Truong",
      "Tan-Sang Ha",
      "Quan-Dung Pham",
      "Sai-Kit Yeung",
      "Yuan Feng",
      "Nguyen Thanh Thien",
      "Lixin Tian",
      "Sheng-Yao Kuan",
      "Yuan-Hao Ho",
      "Angel Bueno Rodriguez",
      "Borja Carrillo-Perez",
      "Alexander Klein",
      "Antje Alex",
      "Yannik Steiniger",
      "Felix Sattler",
      "Edgardo Solano-Carrillo",
      "Matej Fabijani\u0107",
      "Magdalena \u0160umunec",
      "Nadir Kapetanovi\u0107",
      "Andreas Michel",
      "Wolfgang Gross",
      "Martin Weinmann"
    ],
    "url": "http://arxiv.org/abs/2311.14762v1",
    "timestamp": 1700773274,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "59526012-881d-4d92-aae6-c1a85f896ca7": {
    "pk": "59526012-881d-4d92-aae6-c1a85f896ca7",
    "title": "Exploring Lip Segmentation Techniques in Computer Vision: A Comparative Analysis",
    "abstract": "Lip segmentation is crucial in computer vision, especially for lip reading. Despite extensive face segmentation research, lip segmentation has received limited attention. The aim of this study is to compare state-of-the-art lip segmentation models using a standardized setting and a publicly available dataset. Five techniques, namely EHANet, Mask2Former, BiSeNet V2, PIDNet, and STDC1, are qualitatively selected based on their reported performance, inference time, code availability, recency, and popularity. The CelebAMask-HQ dataset, comprising manually annotated face images, is used to fairly assess the lip segmentation performance of the selected models. Inference experiments are conducted on a Raspberry Pi4 to emulate limited computational resources. The results show that Mask2Former and EHANet have the best performances in terms of mIoU score. BiSeNet V2 demonstrate competitive performance, while PIDNet excels in recall but has lower precision. Most models present inference time ranging from 1000 to around 3000 milliseconds on a Raspberry Pi4, with PIDNet having the lowest mean inference time. This study provides a comprehensive evaluation of lip segmentation models, highlighting their performance and inference times. The findings contribute to the development of lightweight techniques and establish benchmarks for future advances in lip segmentation, especially in IoT and edge computing scenarios.",
    "authors": [
      "Pietro B. S. Masur",
      "Francisco Braulio Oliveira",
      "Lucas Moreira Medino",
      "Emanuel Huber",
      "Milene Haraguchi Padilha",
      "Cassio de Alcantara",
      "Renata Sellaro"
    ],
    "url": "http://arxiv.org/abs/2311.11992v1",
    "timestamp": 1700504621,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1a751663-ca47-4883-9e03-b85e4a558b1e": {
    "pk": "1a751663-ca47-4883-9e03-b85e4a558b1e",
    "title": "Enhancing Radiology Diagnosis through Convolutional Neural Networks for Computer Vision in Healthcare",
    "abstract": "The transformative power of Convolutional Neural Networks (CNNs) in radiology diagnostics is examined in this study, with a focus on interpretability, effectiveness, and ethical issues. With an altered DenseNet architecture, the CNN performs admirably in terms of particularity, sensitivity, as well as accuracy. Its superiority over conventional methods is validated by comparative analyses, which highlight efficiency gains. Nonetheless, interpretability issues highlight the necessity of sophisticated methods in addition to continuous model improvement. Integration issues like interoperability and radiologists' training lead to suggestions for teamwork. Systematic consideration of the ethical implications is carried out, necessitating extensive frameworks. Refinement of architectures, interpretability, alongside ethical considerations need to be prioritized in future work for responsible CNN deployment in radiology diagnostics.",
    "authors": [
      "Keshav Kumar K.",
      "Dr N V S L Narasimham"
    ],
    "url": "http://arxiv.org/abs/2311.11234v1",
    "timestamp": 1700372132,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b1098639-0feb-4b28-a5ea-051c20ed8852": {
    "pk": "b1098639-0feb-4b28-a5ea-051c20ed8852",
    "title": "The possibility of making \\$138,000 from shredded banknote pieces using computer vision",
    "abstract": "Every country must dispose of old banknotes. At the Hong Kong Monetary Authority visitor center, visitors can buy a paperweight souvenir full of shredded banknotes. Even though the shredded banknotes are small, by using computer vision, it is possible to reconstruct the whole banknote like a jigsaw puzzle. Each paperweight souvenir costs \\$100 HKD, and it is claimed to contain shredded banknotes equivalent to 138 complete \\$1000 HKD banknotes. In theory, \\$138,000 HKD can be recovered by using computer vision. This paper discusses the technique of collecting shredded banknote pieces and applying a computer vision program.",
    "authors": [
      "Chung To Kong"
    ],
    "url": "http://arxiv.org/abs/2401.06133v1",
    "timestamp": 1700187931,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "161c289a-6cdd-4eea-b388-6f4319ec4c70": {
    "pk": "161c289a-6cdd-4eea-b388-6f4319ec4c70",
    "title": "Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions",
    "abstract": "Autonomous vehicle refers to a vehicle capable of perceiving its surrounding environment and driving with little or no human driver input. The perception system is a fundamental component which enables the autonomous vehicle to collect data and extract relevant information from the environment to drive safely. Benefit from the recent advances in computer vision, the perception task can be achieved by using sensors, such as camera, LiDAR, radar, and ultrasonic sensor. This paper reviews publications on computer vision and autonomous driving that are published during the last ten years. In particular, we first investigate the development of autonomous driving systems and summarize these systems that are developed by the major automotive manufacturers from different countries. Second, we investigate the sensors and benchmark data sets that are commonly utilized for autonomous driving. Then, a comprehensive overview of computer vision applications for autonomous driving such as depth estimation, object detection, lane detection, and traffic sign recognition are discussed. Additionally, we review public opinions and concerns on autonomous vehicles. Based on the discussion, we analyze the current technological challenges that autonomous vehicles meet with. Finally, we present our insights and point out some promising directions for future research. This paper will help the reader to understand autonomous vehicles from the perspectives of academia and industry.",
    "authors": [
      "Xingshuai Dong",
      "Massimiliano L. Cappuccio"
    ],
    "url": "http://arxiv.org/abs/2311.09093v2",
    "timestamp": 1700066478,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "935712e0-0ca6-410a-a48d-bec87540584f": {
    "pk": "935712e0-0ca6-410a-a48d-bec87540584f",
    "title": "Evaluating the Significance of Outdoor Advertising from Driver's Perspective Using Computer Vision",
    "abstract": "Outdoor advertising, such as roadside billboards, plays a significant role in marketing campaigns but can also be a distraction for drivers, potentially leading to accidents. In this study, we propose a pipeline for evaluating the significance of roadside billboards in videos captured from a driver's perspective. We have collected and annotated a new BillboardLamac dataset, comprising eight videos captured by drivers driving through a predefined path wearing eye-tracking devices. The dataset includes annotations of billboards, including 154 unique IDs and 155 thousand bounding boxes, as well as eye fixation data. We evaluate various object tracking methods in combination with a YOLOv8 detector to identify billboard advertisements with the best approach achieving 38.5 HOTA on BillboardLamac. Additionally, we train a random forest classifier to classify billboards into three classes based on the length of driver fixations achieving 75.8% test accuracy. An analysis of the trained classifier reveals that the duration of billboard visibility, its saliency, and size are the most influential features when assessing billboard significance.",
    "authors": [
      "Zuzana \u010cernekov\u00e1",
      "Zuzana Berger Haladov\u00e1",
      "J\u00e1n \u0160pirka",
      "Viktor Kocur"
    ],
    "url": "http://arxiv.org/abs/2311.07390v2",
    "timestamp": 1699888493,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cba5d46a-3b4d-4dcb-b371-7d950af508d4": {
    "pk": "cba5d46a-3b4d-4dcb-b371-7d950af508d4",
    "title": "CASTER: A Computer-Vision-Assisted Wireless Channel Simulator for Gesture Recognition",
    "abstract": "In this paper, a computer-vision-assisted simulation method is proposed to address the issue of training dataset acquisition for wireless hand gesture recognition. In the existing literature, in order to classify gestures via the wireless channel estimation, massive training samples should be measured in a consistent environment, consuming significant efforts. In the proposed CASTER simulator, however, the training dataset can be simulated via existing videos. Particularly, a gesture is represented by a sequence of snapshots, and the channel impulse response of each snapshot is calculated via tracing the rays scattered off a primitive-based hand model. Moreover, CASTER simulator relies on the existing videos to extract the motion data of gestures. Thus, the massive measurements of wireless channel can be eliminated. The experiments demonstrate a 90.8% average classification accuracy of simulation-to-reality inference.",
    "authors": [
      "Zhenyu Ren",
      "Guoliang Li",
      "Chenqing Ji",
      "Chao Yu",
      "Shuai Wang",
      "Rui Wang"
    ],
    "url": "http://arxiv.org/abs/2311.07169v2",
    "timestamp": 1699866277,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SP",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6c472ae5-9958-4d49-992e-29cc09db78d5": {
    "pk": "6c472ae5-9958-4d49-992e-29cc09db78d5",
    "title": "CD-COCO: A Versatile Complex Distorted COCO Database for Scene-Context-Aware Computer Vision",
    "abstract": "The recent development of deep learning methods applied to vision has enabled their increasing integration into real-world applications to perform complex Computer Vision (CV) tasks. However, image acquisition conditions have a major impact on the performance of high-level image processing. A possible solution to overcome these limitations is to artificially augment the training databases or to design deep learning models that are robust to signal distortions. We opt here for the first solution by enriching the database with complex and realistic distortions which were ignored until now in the existing databases. To this end, we built a new versatile database derived from the well-known MS-COCO database to which we applied local and global photo-realistic distortions. These new local distortions are generated by considering the scene context of the images that guarantees a high level of photo-realism. Distortions are generated by exploiting the depth information of the objects in the scene as well as their semantics. This guarantees a high level of photo-realism and allows to explore real scenarios ignored in conventional databases dedicated to various CV applications. Our versatile database offers an efficient solution to improve the robustness of various CV tasks such as Object Detection (OD), scene segmentation, and distortion-type classification methods. The image database, scene classification index, and distortion generation codes are publicly available \\footnote{\\url{https://github.com/Aymanbegh/CD-COCO}}",
    "authors": [
      "Ayman Beghdadi",
      "Azeddine Beghdadi",
      "Malik Mallem",
      "Lotfi Beji",
      "Faouzi Alaya Cheikh"
    ],
    "url": "http://arxiv.org/abs/2311.06976v1",
    "timestamp": 1699828099,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7e87d8d8-9cea-4715-967c-d49dbc0c96f0": {
    "pk": "7e87d8d8-9cea-4715-967c-d49dbc0c96f0",
    "title": "Adaptive recurrent vision performs zero-shot computation scaling to unseen difficulty levels",
    "abstract": "Humans solving algorithmic (or) reasoning problems typically exhibit solution times that grow as a function of problem difficulty. Adaptive recurrent neural networks have been shown to exhibit this property for various language-processing tasks. However, little work has been performed to assess whether such adaptive computation can also enable vision models to extrapolate solutions beyond their training distribution's difficulty level, with prior work focusing on very simple tasks. In this study, we investigate a critical functional role of such adaptive processing using recurrent neural networks: to dynamically scale computational resources conditional on input requirements that allow for zero-shot generalization to novel difficulty levels not seen during training using two challenging visual reasoning tasks: PathFinder and Mazes. We combine convolutional recurrent neural networks (ConvRNNs) with a learnable halting mechanism based on Graves (2016). We explore various implementations of such adaptive ConvRNNs (AdRNNs) ranging from tying weights across layers to more sophisticated biologically inspired recurrent networks that possess lateral connections and gating. We show that 1) AdRNNs learn to dynamically halt processing early (or late) to solve easier (or harder) problems, 2) these RNNs zero-shot generalize to more difficult problem settings not shown during training by dynamically increasing the number of recurrent iterations at test time. Our study provides modeling evidence supporting the hypothesis that recurrent processing enables the functional advantage of adaptively allocating compute resources conditional on input requirements and hence allowing generalization to harder difficulty levels of a visual reasoning problem without training.",
    "authors": [
      "Vijay Veerabadran",
      "Srinivas Ravishankar",
      "Yuan Tang",
      "Ritik Raina",
      "Virginia R. de Sa"
    ],
    "url": "http://arxiv.org/abs/2311.06964v1",
    "timestamp": 1699823224,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  }
}