{
  "b67f1a86-7484-4e60-95b1-62224ad2ae55": {
    "pk": "b67f1a86-7484-4e60-95b1-62224ad2ae55",
    "title": "Leveraging Large Language Models for Knowledge-free Weak Supervision in Clinical Natural Language Processing",
    "abstract": "The performance of deep learning-based natural language processing systems is based on large amounts of labeled training data which, in the clinical domain, are not easily available or affordable. Weak supervision and in-context learning offer partial solutions to this issue, particularly using large language models (LLMs), but their performance still trails traditional supervised methods with moderate amounts of gold-standard data. In particular, inferencing with LLMs is computationally heavy. We propose an approach leveraging fine-tuning LLMs and weak supervision with virtually no domain knowledge that still achieves consistently dominant performance. Using a prompt-based approach, the LLM is used to generate weakly-labeled data for training a downstream BERT model. The weakly supervised model is then further fine-tuned on small amounts of gold standard data. We evaluate this approach using Llama2 on three different n2c2 datasets. With no more than 10 gold standard notes, our final BERT models weakly supervised by fine-tuned Llama2-13B consistently outperformed out-of-the-box PubMedBERT by 4.7% to 47.9% in F1 scores. With only 50 gold standard notes, our models achieved close performance to fully fine-tuned systems.",
    "authors": [
      "Enshuo Hsu",
      "Kirk Roberts"
    ],
    "url": "http://arxiv.org/abs/2406.06723v1",
    "timestamp": 1718044488,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3926d81d-236d-40c4-92c9-db542241cef9": {
    "pk": "3926d81d-236d-40c4-92c9-db542241cef9",
    "title": "Investigation of the Impact of Economic and Social Factors on Energy Demand through Natural Language Processing",
    "abstract": "The relationship between energy demand and variables such as economic activity and weather is well established. However, this paper aims to explore the connection between energy demand and other social aspects, which receive little attention. Through the use of natural language processing on a large news corpus, we shed light on this important link. This study was carried out in five regions of the UK and Ireland and considers multiple horizons from 1 to 30 days. It also considers economic variables such as GDP, unemployment and inflation. We found that: 1) News about military conflicts, transportation, the global pandemic, regional economics, and the international energy market are related to electricity demand. 2) Economic indicators are more important in the East Midlands and Northern Ireland, while social indicators are more useful in the West Midlands and the South West of England. 3) The use of these indices improved forecasting performance by up to 9%.",
    "authors": [
      "Yun Bai",
      "Simon Camal",
      "Andrea Michiorri"
    ],
    "url": "http://arxiv.org/abs/2406.06641v1",
    "timestamp": 1717950314,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a7001fe4-492e-4051-868d-0923f93bbe3d": {
    "pk": "a7001fe4-492e-4051-868d-0923f93bbe3d",
    "title": "SMS Spam Detection and Classification to Combat Abuse in Telephone Networks Using Natural Language Processing",
    "abstract": "In the modern era, mobile phones have become ubiquitous, and Short Message Service (SMS) has grown to become a multi-million-dollar service due to the widespread adoption of mobile devices and the millions of people who use SMS daily. However, SMS spam has also become a pervasive problem that endangers users' privacy and security through phishing and fraud. Despite numerous spam filtering techniques, there is still a need for a more effective solution to address this problem [1]. This research addresses the pervasive issue of SMS spam, which poses threats to users' privacy and security. Despite existing spam filtering techniques, the high false-positive rate persists as a challenge. The study introduces a novel approach utilizing Natural Language Processing (NLP) and machine learning models, particularly BERT (Bidirectional Encoder Representations from Transformers), for SMS spam detection and classification. Data preprocessing techniques, such as stop word removal and tokenization, are applied, along with feature extraction using BERT. Machine learning models, including SVM, Logistic Regression, Naive Bayes, Gradient Boosting, and Random Forest, are integrated with BERT for differentiating spam from ham messages. Evaluation results revealed that the Na\\\"ive Bayes classifier + BERT model achieves the highest accuracy at 97.31% with the fastest execution time of 0.3 seconds on the test dataset. This approach demonstrates a notable enhancement in spam detection efficiency and a low false-positive rate. The developed model presents a valuable solution to combat SMS spam, ensuring faster and more accurate detection. This model not only safeguards users' privacy but also assists network providers in effectively identifying and blocking SMS spam messages.",
    "authors": [
      "Dare Azeez Oyeyemi",
      "Adebola K. Ojo"
    ],
    "url": "http://arxiv.org/abs/2406.06578v1",
    "timestamp": 1717508676,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0f2381ad-38a3-4aec-98ab-a4fd216113b9": {
    "pk": "0f2381ad-38a3-4aec-98ab-a4fd216113b9",
    "title": "Genshin: General Shield for Natural Language Processing with Large Language Models",
    "abstract": "Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been trending recently, demonstrating considerable advancement and generalizability power in countless domains. However, LLMs create an even bigger black box exacerbating opacity, with interpretability limited to few approaches. The uncertainty and opacity embedded in LLMs' nature restrict their application in high-stakes domains like financial fraud, phishing, etc. Current approaches mainly rely on traditional textual classification with posterior interpretable algorithms, suffering from attackers who may create versatile adversarial samples to break the system's defense, forcing users to make trade-offs between efficiency and robustness. To address this issue, we propose a novel cascading framework called Genshin (General Shield for Natural Language Processing with Large Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike most applications of LLMs that try to transform text into something new or structural, Genshin uses LLMs to recover text to its original state. Genshin aims to combine the generalizability of the LLM, the discrimination of the median model, and the interpretability of the simple model. Our experiments on the task of sentimental analysis and spam detection have shown fatal flaws of the current median models and exhilarating results on LLMs' recovery ability, demonstrating that Genshin is both effective and efficient. In our ablation study, we unearth several intriguing observations. Utilizing the LLM defender, a tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal mask rate results in the 3rd paradigm of NLP. Additionally, when employing the LLM as a potential adversarial tool, attackers are capable of executing effective attacks that are nearly semantically lossless.",
    "authors": [
      "Xiao Peng",
      "Tao Liu",
      "Ying Wang"
    ],
    "url": "http://arxiv.org/abs/2405.18741v2",
    "timestamp": 1716955445,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "483422ff-8e82-4b08-9235-4dd7bb685b18": {
    "pk": "483422ff-8e82-4b08-9235-4dd7bb685b18",
    "title": "Automatic detection of cognitive impairment in elderly people using an entertainment chatbot with Natural Language Processing capabilities",
    "abstract": "Previous researchers have proposed intelligent systems for therapeutic monitoring of cognitive impairments. However, most existing practical approaches for this purpose are based on manual tests. This raises issues such as excessive caretaking effort and the white-coat effect. To avoid these issues, we present an intelligent conversational system for entertaining elderly people with news of their interest that monitors cognitive impairment transparently. Automatic chatbot dialogue stages allow assessing content description skills and detecting cognitive impairment with Machine Learning algorithms. We create these dialogue flows automatically from updated news items using Natural Language Generation techniques. The system also infers the gold standard of the answers to the questions, so it can assess cognitive capabilities automatically by comparing these answers with the user responses. It employs a similarity metric with values in [0, 1], in increasing level of similarity. To evaluate the performance and usability of our approach, we have conducted field tests with a test group of 30 elderly people in the earliest stages of dementia, under the supervision of gerontologists. In the experiments, we have analysed the effect of stress and concentration in these users. Those without cognitive impairment performed up to five times better. In particular, the similarity metric varied between 0.03, for stressed and unfocused participants, and 0.36, for relaxed and focused users. Finally, we developed a Machine Learning algorithm based on textual analysis features for automatic cognitive impairment detection, which attained accuracy, F-measure and recall levels above 80%. We have thus validated the automatic approach to detect cognitive impairment in elderly people based on entertainment content.",
    "authors": [
      "Francisco de Arriba-P\u00e9rez",
      "Silvia Garc\u00eda-M\u00e9ndez",
      "Francisco J. Gonz\u00e1lez-Casta\u00f1o",
      "Enrique Costa-Montenegro"
    ],
    "url": "http://arxiv.org/abs/2405.18542v1",
    "timestamp": 1716923868,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0e04e923-0766-48fc-bab4-1dee58882c94": {
    "pk": "0e04e923-0766-48fc-bab4-1dee58882c94",
    "title": "Performance evaluation of Reddit Comments using Machine Learning and Natural Language Processing methods in Sentiment Analysis",
    "abstract": "Sentiment analysis, an increasingly vital field in both academia and industry, plays a pivotal role in machine learning applications, particularly on social media platforms like Reddit. However, the efficacy of sentiment analysis models is hindered by the lack of expansive and fine-grained emotion datasets. To address this gap, our study leverages the GoEmotions dataset, comprising a diverse range of emotions, to evaluate sentiment analysis methods across a substantial corpus of 58,000 comments. Distinguished from prior studies by the Google team, which limited their analysis to only two models, our research expands the scope by evaluating a diverse array of models. We investigate the performance of traditional classifiers such as Naive Bayes and Support Vector Machines (SVM), as well as state-of-the-art transformer-based models including BERT, RoBERTa, and GPT. Furthermore, our evaluation criteria extend beyond accuracy to encompass nuanced assessments, including hierarchical classification based on varying levels of granularity in emotion categorization. Additionally, considerations such as computational efficiency are incorporated to provide a comprehensive evaluation framework. Our findings reveal that the RoBERTa model consistently outperforms the baseline models, demonstrating superior accuracy in fine-grained sentiment classification tasks. This underscores the substantial potential and significance of the RoBERTa model in advancing sentiment analysis capabilities.",
    "authors": [
      "Xiaoxia Zhang",
      "Xiuyuan Qi",
      "Zixin Teng"
    ],
    "url": "http://arxiv.org/abs/2405.16810v2",
    "timestamp": 1716782368,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ae2b2cc9-49f8-4bce-bef3-56115a727c7a": {
    "pk": "ae2b2cc9-49f8-4bce-bef3-56115a727c7a",
    "title": "Explainable automatic industrial carbon footprint estimation from bank transaction classification using natural language processing",
    "abstract": "Concerns about the effect of greenhouse gases have motivated the development of certification protocols to quantify the industrial carbon footprint (CF). These protocols are manual, work-intensive, and expensive. All of the above have led to a shift towards automatic data-driven approaches to estimate the CF, including Machine Learning (ML) solutions. Unfortunately, the decision-making processes involved in these solutions lack transparency from the end user's point of view, who must blindly trust their outcomes compared to intelligible traditional manual approaches. In this research, manual and automatic methodologies for CF estimation were reviewed, taking into account their transparency limitations. This analysis led to the proposal of a new explainable ML solution for automatic CF calculations through bank transaction classification. Consideration should be given to the fact that no previous research has considered the explainability of bank transaction classification for this purpose. For classification, different ML models have been employed based on their promising performance in the literature, such as Support Vector Machine, Random Forest, and Recursive Neural Networks. The results obtained were in the 90 % range for accuracy, precision, and recall evaluation metrics. From their decision paths, the proposed solution estimates the CO2 emissions associated with bank transactions. The explainability methodology is based on an agnostic evaluation of the influence of the input terms extracted from the descriptions of transactions using locally interpretable models. The explainability terms were automatically validated using a similarity metric over the descriptions of the target categories. Conclusively, the explanation performance is satisfactory in terms of the proximity of the explanations to the associated activity sector descriptions.",
    "authors": [
      "Jaime Gonz\u00e1lez-Gonz\u00e1lez",
      "Silvia Garc\u00eda-M\u00e9ndez",
      "Francisco de Arriba-P\u00e9rez",
      "Francisco J. Gonz\u00e1lez-Casta\u00f1o",
      "\u00d3scar Barba-Seara"
    ],
    "url": "http://arxiv.org/abs/2405.14505v1",
    "timestamp": 1716468186,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c3e2fd80-a63b-48a3-a511-48aac0f8aafb": {
    "pk": "c3e2fd80-a63b-48a3-a511-48aac0f8aafb",
    "title": "Use of natural language processing to extract and classify papillary thyroid cancer features from surgical pathology reports",
    "abstract": "Background We aim to use Natural Language Processing (NLP) to automate the extraction and classification of thyroid cancer risk factors from pathology reports. Methods We analyzed 1,410 surgical pathology reports from adult papillary thyroid cancer patients at Mayo Clinic, Rochester, MN, from 2010 to 2019. Structured and non-structured reports were used to create a consensus-based ground truth dictionary and categorized them into modified recurrence risk levels. Non-structured reports were narrative, while structured reports followed standardized formats. We then developed ThyroPath, a rule-based NLP pipeline, to extract and classify thyroid cancer features into risk categories. Training involved 225 reports (150 structured, 75 unstructured), with testing on 170 reports (120 structured, 50 unstructured) for evaluation. The pipeline's performance was assessed using both strict and lenient criteria for accuracy, precision, recall, and F1-score. Results In extraction tasks, ThyroPath achieved overall strict F-1 scores of 93% for structured reports and 90 for unstructured reports, covering 18 thyroid cancer pathology features. In classification tasks, ThyroPath-extracted information demonstrated an overall accuracy of 93% in categorizing reports based on their corresponding guideline-based risk of recurrence: 76.9% for high-risk, 86.8% for intermediate risk, and 100% for both low and very low-risk cases. However, ThyroPath achieved 100% accuracy across all thyroid cancer risk categories with human-extracted pathology information. Conclusions ThyroPath shows promise in automating the extraction and risk recurrence classification of thyroid pathology reports at large scale. It offers a solution to laborious manual reviews and advancing virtual registries. However, it requires further validation before implementation.",
    "authors": [
      "Ricardo Loor-Torres",
      "Yuqi Wu",
      "Esteban Cabezas",
      "Mariana Borras",
      "David Toro-Tobon",
      "Mayra Duran",
      "Misk Al Zahidy",
      "Maria Mateo Chavez",
      "Cristian Soto Jacome",
      "Jungwei W. Fan",
      "Naykky M. Singh Ospina",
      "Yonghui Wu",
      "Juan P. Brito"
    ],
    "url": "http://arxiv.org/abs/2406.00015v1",
    "timestamp": 1716416832,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "58267e3a-c5d5-4b85-92bd-129e981c30c0": {
    "pk": "58267e3a-c5d5-4b85-92bd-129e981c30c0",
    "title": "LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language",
    "abstract": "Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode.",
    "authors": [
      "James Requeima",
      "John Bronskill",
      "Dami Choi",
      "Richard E. Turner",
      "David Duvenaud"
    ],
    "url": "http://arxiv.org/abs/2405.12856v2",
    "timestamp": 1716304392,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ML",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4b861ec7-8753-43de-b404-666313c3466e": {
    "pk": "4b861ec7-8753-43de-b404-666313c3466e",
    "title": "Unveiling factors influencing judgment variation in Sentiment Analysis with Natural Language Processing and Statistics",
    "abstract": "TripAdvisor reviews and comparable data sources play an important role in many tasks in Natural Language Processing (NLP), providing a data basis for the identification and classification of subjective judgments, such as hotel or restaurant reviews, into positive or negative polarities. This study explores three important factors influencing variation in crowdsourced polarity judgments, focusing on TripAdvisor reviews in Spanish. Three hypotheses are tested: the role of Part Of Speech (POS), the impact of sentiment words such as \"tasty\", and the influence of neutral words like \"ok\" on judgment variation. The study's methodology employs one-word titles, demonstrating their efficacy in studying polarity variation of words. Statistical tests on mean equality are performed on word groups of our interest. The results of this study reveal that adjectives in one-word titles tend to result in lower judgment variation compared to other word types or POS. Sentiment words contribute to lower judgment variation as well, emphasizing the significance of sentiment words in research on polarity judgments, and neutral words are associated with higher judgment variation as expected. However, these effects cannot be always reproduced in longer titles, which suggests that longer titles do not represent the best data source for testing the ambiguity of single words due to the influence on word polarity by other words like negation in longer titles. This empirical investigation contributes valuable insights into the factors influencing polarity variation of words, providing a foundation for NLP practitioners that aim to capture and predict polarity judgments in Spanish and for researchers that aim to understand factors influencing judgment variation.",
    "authors": [
      "Olga Kellert",
      "Carlos G\u00f3mez-Rodr\u00edguez",
      "Mahmud Uz Zaman"
    ],
    "url": "http://arxiv.org/abs/2405.12055v1",
    "timestamp": 1716215058,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "462030cc-3e5e-45f3-b549-cf3b27ad8cf4": {
    "pk": "462030cc-3e5e-45f3-b549-cf3b27ad8cf4",
    "title": "Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing",
    "abstract": "In this study, we explore the potential of using quantum natural language processing (QNLP) to inverse design metal-organic frameworks (MOFs) with targeted properties. Specifically, by analyzing 150 hypothetical MOF structures consisting of 10 metal nodes and 15 organic ligands, we categorize these structures into four distinct classes for pore volume and $H_{2}$ uptake values. We then compare various QNLP models (i.e. the bag-of-words, DisCoCat (Distributional Compositional Categorical), and sequence-based models) to identify the most effective approach to process the MOF dataset. Using a classical simulator provided by the IBM Qiskit, the bag-of-words model is identified to be the optimum model, achieving validation accuracies of 85.7% and 86.7% for binary classification tasks on pore volume and $H_{2}$ uptake, respectively. Further, we developed multi-class classification models tailored to the probabilistic nature of quantum circuits, with average test accuracies of 88.4% and 80.7% across different classes for pore volume and $H_{2}$ uptake datasets. Finally, the performance of generating MOF with target properties showed accuracies of 93.5% for pore volume and 89% for $H_{2}$ uptake, respectively. Although our investigation covers only a fraction of the vast MOF search space, it marks a promising first step towards using quantum computing for materials design, offering a new perspective through which to explore the complex landscape of MOFs.",
    "authors": [
      "Shinyoung Kang",
      "Jihan Kim"
    ],
    "url": "http://arxiv.org/abs/2405.11783v1",
    "timestamp": 1716181332,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6263b836-8b82-46ff-8f8f-22d4ad33e5a1": {
    "pk": "6263b836-8b82-46ff-8f8f-22d4ad33e5a1",
    "title": "Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks",
    "abstract": "The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.",
    "authors": [
      "Taiyuan Mei",
      "Yun Zi",
      "Xiaohan Cheng",
      "Zijun Gao",
      "Qi Wang",
      "Haowei Yang"
    ],
    "url": "http://arxiv.org/abs/2405.11704v1",
    "timestamp": 1716163800,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f10013a9-f146-4127-953b-300438517a83": {
    "pk": "f10013a9-f146-4127-953b-300438517a83",
    "title": "High-dimensional multiple imputation (HDMI) for partially observed confounders including natural language processing-derived auxiliary covariates",
    "abstract": "Multiple imputation (MI) models can be improved by including auxiliary covariates (AC), but their performance in high-dimensional data is not well understood. We aimed to develop and compare high-dimensional MI (HDMI) approaches using structured and natural language processing (NLP)-derived AC in studies with partially observed confounders. We conducted a plasmode simulation study using data from opioid vs. non-steroidal anti-inflammatory drug (NSAID) initiators (X) with observed serum creatinine labs (Z2) and time-to-acute kidney injury as outcome. We simulated 100 cohorts with a null treatment effect, including X, Z2, atrial fibrillation (U), and 13 other investigator-derived confounders (Z1) in the outcome generation. We then imposed missingness (MZ2) on 50% of Z2 measurements as a function of Z2 and U and created different HDMI candidate AC using structured and NLP-derived features. We mimicked scenarios where U was unobserved by omitting it from all AC candidate sets. Using LASSO, we data-adaptively selected HDMI covariates associated with Z2 and MZ2 for MI, and with U to include in propensity score models. The treatment effect was estimated following propensity score matching in MI datasets and we benchmarked HDMI approaches against a baseline imputation and complete case analysis with Z1 only. HDMI using claims data showed the lowest bias (0.072). Combining claims and sentence embeddings led to an improvement in the efficiency displaying the lowest root-mean-squared-error (0.173) and coverage (94%). NLP-derived AC alone did not perform better than baseline MI. HDMI approaches may decrease bias in studies with partially observed confounders where missingness depends on unobserved factors.",
    "authors": [
      "Janick Weberpals",
      "Pamela A. Shaw",
      "Kueiyu Joshua Lin",
      "Richard Wyss",
      "Joseph M Plasek",
      "Li Zhou",
      "Kerry Ngan",
      "Thomas DeRamus",
      "Sudha R. Raman",
      "Bradley G. Hammill",
      "Hana Lee",
      "Sengwee Toh",
      "John G. Connolly",
      "Kimberly J. Dandreo",
      "Fang Tian",
      "Wei Liu",
      "Jie Li",
      "Jos\u00e9 J. Hern\u00e1ndez-Mu\u00f1oz",
      "Sebastian Schneeweiss",
      "Rishi J. Desai"
    ],
    "url": "http://arxiv.org/abs/2405.10925v1",
    "timestamp": 1715966692,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ME",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "14368f33-08a8-40aa-8335-a230721efe29": {
    "pk": "14368f33-08a8-40aa-8335-a230721efe29",
    "title": "Natural Language Processing for Requirements Traceability",
    "abstract": "Traceability, the ability to trace relevant software artifacts to support reasoning about the quality of the software and its development process, plays a crucial role in requirements and software engineering, particularly for safety-critical systems. In this chapter, we provide a comprehensive overview of the representative tasks in requirement traceability for which natural language processing (NLP) and related techniques have made considerable progress in the past decade. We first present the definition of traceability in the context of requirements and the overall engineering process, as well as other important concepts related to traceability tasks. Then, we discuss two tasks in detail, including trace link recovery and trace link maintenance. We also introduce two other related tasks concerning when trace links are used in practical contexts. For each task, we explain the characteristics of the task, how it can be approached through NLP techniques, and how to design and conduct the experiment to demonstrate the performance of the NLP techniques. We further discuss practical considerations on how to effectively apply NLP techniques and assess their effectiveness regarding the data set collection, the metrics selection, and the role of humans when evaluating the NLP approaches. Overall, this chapter prepares the readers with the fundamental knowledge of designing automated traceability solutions enabled by NLP in practice.",
    "authors": [
      "Jin L. C. Guo",
      "Jan-Philipp Stegh\u00f6fer",
      "Andreas Vogelsang",
      "Jane Cleland-Huang"
    ],
    "url": "http://arxiv.org/abs/2405.10845v1",
    "timestamp": 1715959020,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2b79d100-3636-4527-b308-19bbf40725d5": {
    "pk": "2b79d100-3636-4527-b308-19bbf40725d5",
    "title": "On the relevance of pre-neural approaches in natural language processing pedagogy",
    "abstract": "While neural approaches using deep learning are the state-of-the-art for natural language processing (NLP) today, pre-neural algorithms and approaches still find a place in NLP textbooks and courses of recent years. In this paper, we compare two introductory NLP courses taught in Australia and India, and examine how Transformer and pre-neural approaches are balanced within the lecture plan and assessments of the courses. We also draw parallels with the objects-first and objects-later debate in CS1 education. We observe that pre-neural approaches add value to student learning by building an intuitive understanding of NLP problems, potential solutions and even Transformer-based models themselves. Despite pre-neural approaches not being state-of-the-art, the paper makes a case for their inclusion in NLP courses today.",
    "authors": [
      "Aditya Joshi",
      "Jake Renzella",
      "Pushpak Bhattacharyya",
      "Saurav Jha",
      "Xiangyu Zhang"
    ],
    "url": "http://arxiv.org/abs/2405.09854v1",
    "timestamp": 1715843653,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bf954747-b0ae-44ed-af41-c516caf287cf": {
    "pk": "bf954747-b0ae-44ed-af41-c516caf287cf",
    "title": "Is Less More? Quality, Quantity and Context in Idiom Processing with Natural Language Models",
    "abstract": "Compositionality in language models presents a problem when processing idiomatic expressions, as their meaning often cannot be directly derived from their individual parts. Although fine-tuning and other optimization strategies can be used to improve representations of idiomatic expressions, this depends on the availability of relevant data. We present the Noun Compound Synonym Substitution in Books - NCSSB - datasets, which are created by substitution of synonyms of potentially idiomatic English noun compounds in public domain book texts. We explore the trade-off between data quantity and quality when training models for idiomaticity detection, in conjunction with contextual information obtained locally (from the surrounding sentences) or externally (through language resources). Performance on an idiomaticity detection task indicates that dataset quality is a stronger factor for context-enriched models, but that quantity also plays a role in models without context inclusion strategies.",
    "authors": [
      "Agne Knietaite",
      "Adam Allsebrook",
      "Anton Minkov",
      "Adam Tomaszewski",
      "Norbert Slinko",
      "Richard Johnson",
      "Thomas Pickard",
      "Dylan Phelps",
      "Aline Villavicencio"
    ],
    "url": "http://arxiv.org/abs/2405.08497v1",
    "timestamp": 1715684060,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d6fbbe2c-bf94-46a7-88e7-fd6451b6cb65": {
    "pk": "d6fbbe2c-bf94-46a7-88e7-fd6451b6cb65",
    "title": "What Can Natural Language Processing Do for Peer Review?",
    "abstract": "The number of scientific articles produced every year is growing rapidly. Providing quality control over them is crucial for scientists and, ultimately, for the public good. In modern science, this process is largely delegated to peer review -- a distributed procedure in which each submission is evaluated by several independent experts in the field. Peer review is widely used, yet it is hard, time-consuming, and prone to error. Since the artifacts involved in peer review -- manuscripts, reviews, discussions -- are largely text-based, Natural Language Processing has great potential to improve reviewing. As the emergence of large language models (LLMs) has enabled NLP assistance for many new tasks, the discussion on machine-assisted peer review is picking up the pace. Yet, where exactly is help needed, where can NLP help, and where should it stand aside? The goal of our paper is to provide a foundation for the future efforts in NLP for peer-reviewing assistance. We discuss peer review as a general process, exemplified by reviewing at AI conferences. We detail each step of the process from manuscript submission to camera-ready revision, and discuss the associated challenges and opportunities for NLP assistance, illustrated by existing work. We then turn to the big challenges in NLP for peer review as a whole, including data acquisition and licensing, operationalization and experimentation, and ethical issues. To help consolidate community efforts, we create a companion repository that aggregates key datasets pertaining to peer review. Finally, we issue a detailed call for action for the scientific community, NLP and AI researchers, policymakers, and funding bodies to help bring the research in NLP for peer review forward. We hope that our work will help set the agenda for research in machine-assisted scientific quality control in the age of AI, within the NLP community and beyond.",
    "authors": [
      "Ilia Kuznetsov",
      "Osama Mohammed Afzal",
      "Koen Dercksen",
      "Nils Dycke",
      "Alexander Goldberg",
      "Tom Hope",
      "Dirk Hovy",
      "Jonathan K. Kummerfeld",
      "Anne Lauscher",
      "Kevin Leyton-Brown",
      "Sheng Lu",
      "Mausam",
      "Margot Mieskes",
      "Aur\u00e9lie N\u00e9v\u00e9ol",
      "Danish Pruthi",
      "Lizhen Qu",
      "Roy Schwartz",
      "Noah A. Smith",
      "Thamar Solorio",
      "Jingyan Wang",
      "Xiaodan Zhu",
      "Anna Rogers",
      "Nihar B. Shah",
      "Iryna Gurevych"
    ],
    "url": "http://arxiv.org/abs/2405.06563v1",
    "timestamp": 1715357203,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "dc6fd031-6070-4625-abd8-e551552a3855": {
    "pk": "dc6fd031-6070-4625-abd8-e551552a3855",
    "title": "Natural Language Processing RELIES on Linguistics",
    "abstract": "Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym $RELIES$ that encapsulates six major facets where linguistics contributes to NLP: $R$esources, $E$valuation, $L$ow-resource settings, $I$nterpretability, $E$xplanation, and the $S$tudy of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-a-vis systems of human language.",
    "authors": [
      "Juri Opitz",
      "Shira Wein",
      "Nathan Schneider"
    ],
    "url": "http://arxiv.org/abs/2405.05966v1",
    "timestamp": 1715277572,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a80ed9a6-75b6-419e-a631-8e8f20c86958": {
    "pk": "a80ed9a6-75b6-419e-a631-8e8f20c86958",
    "title": "Enhancing Holonic Architecture with Natural Language Processing for System of Systems",
    "abstract": "The complexity and dynamic nature of System of Systems (SoS) necessitate efficient communication mechanisms to ensure interoperability and collaborative functioning among constituent systems, termed holons. This paper proposes an innovative approach to enhance holon communication within SoS through the integration of Conversational Generative Intelligence (CGI) techniques. Our approach leverages advancements in CGI, specifically Large Language Models (LLMs), to enable holons to understand and act on natural language instructions. This fosters more intuitive human-holon interactions, improving social intelligence and ultimately leading to better coordination among diverse systems. This position paper outlines a conceptual framework for CGI-enhanced holon interaction, discusses the potential impact on SoS adaptability, usability and efficiency, and sets the stage for future exploration and prototype implementation.",
    "authors": [
      "Muhammad Ashfaq",
      "Ahmed R. Sadik",
      "Tommi Mikkonen",
      "Muhammad Waseem",
      "Niko M akitalo"
    ],
    "url": "http://arxiv.org/abs/2405.05365v1",
    "timestamp": 1715194072,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e97ae68e-c3cf-4f34-9759-24f44b09687c": {
    "pk": "e97ae68e-c3cf-4f34-9759-24f44b09687c",
    "title": "Conformal Prediction for Natural Language Processing: A Survey",
    "abstract": "The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.",
    "authors": [
      "Margarida M. Campos",
      "Ant\u00f3nio Farinhas",
      "Chrysoula Zerva",
      "M\u00e1rio A. T. Figueiredo",
      "Andr\u00e9 F. T. Martins"
    ],
    "url": "http://arxiv.org/abs/2405.01976v1",
    "timestamp": 1714730445,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "76cd8954-2896-45f8-9501-4c748f1093ad": {
    "pk": "76cd8954-2896-45f8-9501-4c748f1093ad",
    "title": "A Legal Framework for Natural Language Processing Model Training in Portugal",
    "abstract": "Recent advances in deep learning have promoted the advent of many computational systems capable of performing intelligent actions that, until then, were restricted to the human intellect. In the particular case of human languages, these advances allowed the introduction of applications like ChatGPT that are capable of generating coherent text without being explicitly programmed to do so. Instead, these models use large volumes of textual data to learn meaningful representations of human languages. Associated with these advances, concerns about copyright and data privacy infringements caused by these applications have emerged. Despite these concerns, the pace at which new natural language processing applications continued to be developed largely outperformed the introduction of new regulations. Today, communication barriers between legal experts and computer scientists motivate many unintentional legal infringements during the development of such applications. In this paper, a multidisciplinary team intends to bridge this communication gap and promote more compliant Portuguese NLP research by presenting a series of everyday NLP use cases, while highlighting the Portuguese legislation that may arise during its development.",
    "authors": [
      "R\u00faben Almeida",
      "Evelin Amorim"
    ],
    "url": "http://arxiv.org/abs/2405.00536v1",
    "timestamp": 1714573130,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "aa3c7209-cfc1-4b0d-aca1-b58f5562d14a": {
    "pk": "aa3c7209-cfc1-4b0d-aca1-b58f5562d14a",
    "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
    "abstract": "Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.",
    "authors": [
      "Yucheng Hu",
      "Yuxing Lu"
    ],
    "url": "http://arxiv.org/abs/2404.19543v1",
    "timestamp": 1714482891,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "32c99fbc-ccdc-429f-8ab0-802502873cca": {
    "pk": "32c99fbc-ccdc-429f-8ab0-802502873cca",
    "title": "Towards A Structured Overview of Use Cases for Natural Language Processing in the Legal Domain: A German Perspective",
    "abstract": "In recent years, the field of Legal Tech has risen in prevalence, as the Natural Language Processing (NLP) and legal disciplines have combined forces to digitalize legal processes. Amidst the steady flow of research solutions stemming from the NLP domain, the study of use cases has fallen behind, leading to a number of innovative technical methods without a place in practice. In this work, we aim to build a structured overview of Legal Tech use cases, grounded in NLP literature, but also supplemented by voices from legal practice in Germany. Based upon a Systematic Literature Review, we identify seven categories of NLP technologies for the legal domain, which are then studied in juxtaposition to 22 legal use cases. In the investigation of these use cases, we identify 15 ethical, legal, and social aspects (ELSA), shedding light on the potential concerns of digitally transforming the legal domain.",
    "authors": [
      "Juraj Vladika",
      "Stephen Meisenbacher",
      "Martina Preis",
      "Alexandra Klymenko",
      "Florian Matthes"
    ],
    "url": "http://arxiv.org/abs/2404.18759v2",
    "timestamp": 1714402607,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "22abe055-a93c-478e-b3e8-34f1eb1516af": {
    "pk": "22abe055-a93c-478e-b3e8-34f1eb1516af",
    "title": "Computational Job Market Analysis with Natural Language Processing",
    "abstract": "[Abridged Abstract]   Recent technological advances underscore labor market dynamics, yielding significant consequences for employment prospects and increasing job vacancy data across platforms and languages. Aggregating such data holds potential for valuable insights into labor market demands, new skills emergence, and facilitating job matching for various stakeholders. However, despite prevalent insights in the private sector, transparent language technology systems and data for this domain are lacking. This thesis investigates Natural Language Processing (NLP) technology for extracting relevant information from job descriptions, identifying challenges including scarcity of training data, lack of standardized annotation guidelines, and shortage of effective extraction methods from job ads. We frame the problem, obtaining annotated data, and introducing extraction methodologies. Our contributions include job description datasets, a de-identification dataset, and a novel active learning algorithm for efficient model training. We propose skill extraction using weak supervision, a taxonomy-aware pre-training methodology adapting multilingual language models to the job market domain, and a retrieval-augmented model leveraging multiple skill extraction datasets to enhance overall performance. Finally, we ground extracted information within a designated taxonomy.",
    "authors": [
      "Mike Zhang"
    ],
    "url": "http://arxiv.org/abs/2404.18977v1",
    "timestamp": 1714402358,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b8e62a9b-a78f-4427-a2d4-3b2e69af70d0": {
    "pk": "b8e62a9b-a78f-4427-a2d4-3b2e69af70d0",
    "title": "Quantitative Tools for Time Series Analysis in Natural Language Processing: A Practitioners Guide",
    "abstract": "Natural language processing tools have become frequently used in social sciences such as economics, political science, and sociology. Many publications apply topic modeling to elicit latent topics in text corpora and their development over time. Here, most publications rely on visual inspections and draw inference on changes, structural breaks, and developments over time. We suggest using univariate time series econometrics to introduce more quantitative rigor that can strengthen the analyses. In particular, we discuss the econometric topics of non-stationarity as well as structural breaks. This paper serves as a comprehensive practitioners guide to provide researchers in the social and life sciences as well as the humanities with concise advice on how to implement econometric time series methods to thoroughly investigate topic prevalences over time. We provide coding advice for the statistical software R throughout the paper. The application of the discussed tools to a sample dataset completes the analysis.",
    "authors": [
      "W. Benedikt Schmal"
    ],
    "url": "http://arxiv.org/abs/2404.18499v1",
    "timestamp": 1714380077,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "econ.GN",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b0b2dd9a-d8f6-49f8-b54d-7ececaa88f6e": {
    "pk": "b0b2dd9a-d8f6-49f8-b54d-7ececaa88f6e",
    "title": "Modeling the Sacred: Considerations when Using Considerations when Using Religious Texts in Natural Language Processing",
    "abstract": "This position paper concerns the use of religious texts in Natural Language Processing (NLP), which is of special interest to the Ethics of NLP. Religious texts are expressions of culturally important values, and machine learned models have a propensity to reproduce cultural values encoded in their training data. Furthermore, translations of religious texts are frequently used by NLP researchers when language data is scarce. This repurposes the translations from their original uses and motivations, which often involve attracting new followers. This paper argues that NLP's use of such texts raises considerations that go beyond model biases, including data provenance, cultural contexts, and their use in proselytism. We argue for more consideration of researcher positionality, and of the perspectives of marginalized linguistic and religious communities.",
    "authors": [
      "Ben Hutchinson"
    ],
    "url": "http://arxiv.org/abs/2404.14740v1",
    "timestamp": 1713847642,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "65d74f36-0d78-4030-8c4e-bb0604fcc7d6": {
    "pk": "65d74f36-0d78-4030-8c4e-bb0604fcc7d6",
    "title": "Improvement in Semantic Address Matching using Natural Language Processing",
    "abstract": "Address matching is an important task for many businesses especially delivery and take out companies which help them to take out a certain address from their data warehouse. Existing solution uses similarity of strings, and edit distance algorithms to find out the similar addresses from the address database, but these algorithms could not work effectively with redundant, unstructured, or incomplete address data. This paper discuss semantic Address matching technique, by which we can find out a particular address from a list of possible addresses. We have also reviewed existing practices and their shortcoming. Semantic address matching is an essentially NLP task in the field of deep learning. Through this technique We have the ability to triumph the drawbacks of existing methods like redundant or abbreviated data problems. The solution uses the OCR on invoices to extract the address and create the data pool of addresses. Then this data is fed to the algorithm BM-25 for scoring the best matching entries. Then to observe the best result, this will pass through BERT for giving the best possible result from the similar queries. Our investigation exhibits that our methodology enormously improves both accuracy and review of cutting-edge technology existing techniques.",
    "authors": [
      "Vansh Gupta",
      "Mohit Gupta",
      "Jai Garg",
      "Nitesh Garg"
    ],
    "url": "http://arxiv.org/abs/2404.11691v1",
    "timestamp": 1713379356,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "58ffbbe2-7f1c-4751-b5df-47826d71bbbd": {
    "pk": "58ffbbe2-7f1c-4751-b5df-47826d71bbbd",
    "title": "CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models",
    "abstract": "Effectively using Natural Language Processing (NLP) tools in under-resourced languages requires a thorough understanding of the language itself, familiarity with the latest models and training methodologies, and technical expertise to deploy these models. This could present a significant obstacle for language community members and linguists to use NLP tools. This paper introduces the CMU Linguistic Annotation Backend, an open-source framework that simplifies model deployment and continuous human-in-the-loop fine-tuning of NLP models. CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for speech recognition, OCR, translation, and syntactic analysis to new languages, even with limited training data. We describe various tools and APIs that are currently available and how developers can easily add new models/functionality to the framework. Code is available at https://github.com/neulab/cmulab along with a live demo at https://cmulab.dev",
    "authors": [
      "Zaid Sheikh",
      "Antonios Anastasopoulos",
      "Shruti Rijhwani",
      "Lindia Tjuatja",
      "Robbie Jimerson",
      "Graham Neubig"
    ],
    "url": "http://arxiv.org/abs/2404.02408v1",
    "timestamp": 1712110906,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "14c3e46d-4952-4ecc-860d-753c15e5d741": {
    "pk": "14c3e46d-4952-4ecc-860d-753c15e5d741",
    "title": "Enterprise Use Cases Combining Knowledge Graphs and Natural Language Processing",
    "abstract": "Knowledge management is a critical challenge for enterprises in today's digital world, as the volume and complexity of data being generated and collected continue to grow incessantly. Knowledge graphs (KG) emerged as a promising solution to this problem by providing a flexible, scalable, and semantically rich way to organize and make sense of data. This paper builds upon a recent survey of the research literature on combining KGs and Natural Language Processing (NLP). Based on selected application scenarios from enterprise context, we discuss synergies that result from such a combination. We cover various approaches from the three core areas of KG construction, reasoning as well as KG-based NLP tasks. In addition to explaining innovative enterprise use cases, we assess their maturity in terms of practical applicability and conclude with an outlook on emergent application areas for the future.",
    "authors": [
      "Phillip Schneider",
      "Tim Schopf",
      "Juraj Vladika",
      "Florian Matthes"
    ],
    "url": "http://arxiv.org/abs/2404.01443v1",
    "timestamp": 1711999732,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "97e21988-8dd6-4e48-b61c-43ca7afc8e23": {
    "pk": "97e21988-8dd6-4e48-b61c-43ca7afc8e23",
    "title": "Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning",
    "abstract": "Finance-related news such as Bloomberg News, CNN Business and Forbes are valuable sources of real data for market screening systems. In news, an expert shares opinions beyond plain technical analyses that include context such as political, sociological and cultural factors. In the same text, the expert often discusses the performance of different assets. Some key statements are mere descriptions of past events while others are predictions. Therefore, understanding the temporality of the key statements in a text is essential to separate context information from valuable predictions. We propose a novel system to detect the temporality of finance-related news at discourse level that combines Natural Language Processing and Machine Learning techniques, and exploits sophisticated features such as syntactic and semantic dependencies. More specifically, we seek to extract the dominant tenses of the main statements, which may be either explicit or implicit. We have tested our system on a labelled dataset of finance-related news annotated by researchers with knowledge in the field. Experimental results reveal a high detection precision compared to an alternative rule-based baseline approach. Ultimately, this research contributes to the state-of-the-art of market screening by identifying predictive knowledge for financial decision making.",
    "authors": [
      "Silvia Garc\u00eda-M\u00e9ndez",
      "Francisco de Arriba-P\u00e9rez",
      "Ana Barros-Vila",
      "Francisco J. Gonz\u00e1lez-Casta\u00f1o"
    ],
    "url": "http://arxiv.org/abs/2404.01337v1",
    "timestamp": 1711816810,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "622cdf1c-4f86-4f35-93e7-3817e517ae76": {
    "pk": "622cdf1c-4f86-4f35-93e7-3817e517ae76",
    "title": "Quantum Natural Language Processing",
    "abstract": "Language processing is at the heart of current developments in artificial intelligence, and quantum computers are becoming available at the same time. This has led to great interest in quantum natural language processing, and several early proposals and experiments.   This paper surveys the state of this area, showing how NLP-related techniques have been used in quantum language processing. We examine the art of word embeddings and sequential models, proposing some avenues for future investigation and discussing the tradeoffs present in these directions. We also highlight some recent methods to compute attention in transformer models, and perform grammatical parsing. We also introduce a new quantum design for the basic task of text encoding (representing a string of characters in memory), which has not been addressed in detail before.   Quantum theory has contributed toward quantifying uncertainty and explaining \"What is intelligence?\" In this context, we argue that \"hallucinations\" in modern artificial intelligence systems are a misunderstanding of the way facts are conceptualized: language can express many plausible hypotheses, of which only a few become actual.",
    "authors": [
      "Dominic Widdows",
      "Willie Aboumrad",
      "Dohun Kim",
      "Sayonee Ray",
      "Jonathan Mei"
    ],
    "url": "http://arxiv.org/abs/2403.19758v2",
    "timestamp": 1711649707,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1011b47a-ff44-46ba-a64e-8553e6adf689": {
    "pk": "1011b47a-ff44-46ba-a64e-8553e6adf689",
    "title": "From Explainable to Interpretable Deep Learning for Natural Language Processing in Healthcare: How Far from Reality?",
    "abstract": "Deep learning (DL) has substantially enhanced natural language processing (NLP) in healthcare research. However, the increasing complexity of DL-based NLP necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review of explainable and interpretable DL in healthcare NLP. The term \"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to distinguish XAI from IAI. Different models are further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms are the most prevalent emerging IAI technique. The use of IAI is growing, distinguishing it from XAI. The major challenges identified are that most XIAI does not explore \"global\" modelling processes, the lack of best practices, and the lack of systematic evaluation and benchmarks. One important opportunity is to use attention mechanisms to enhance multi-modal XIAI for personalized medicine. Additionally, combining DL with causal logic holds promise. Our discussion encourages the integration of XIAI in Large Language Models (LLMs) and domain-specific smaller models. In conclusion, XIAI adoption in healthcare requires dedicated in-house expertise. Collaboration with domain experts, end-users, and policymakers can lead to ready-to-use XIAI methods across NLP and medical tasks. While challenges exist, XIAI techniques offer a valuable foundation for interpretable NLP algorithms in healthcare.",
    "authors": [
      "Guangming Huang",
      "Yingya Li",
      "Shoaib Jameel",
      "Yunfei Long",
      "Giorgos Papanastasiou"
    ],
    "url": "http://arxiv.org/abs/2403.11894v3",
    "timestamp": 1710777213,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cfd1936a-f3c3-448d-b339-c7c0c41495fa": {
    "pk": "cfd1936a-f3c3-448d-b339-c7c0c41495fa",
    "title": "Identifying Health Risks from Family History: A Survey of Natural Language Processing Techniques",
    "abstract": "Electronic health records include information on patients' status and medical history, which could cover the history of diseases and disorders that could be hereditary. One important use of family history information is in precision health, where the goal is to keep the population healthy with preventative measures. Natural Language Processing (NLP) and machine learning techniques can assist with identifying information that could assist health professionals in identifying health risks before a condition is developed in their later years, saving lives and reducing healthcare costs.   We survey the literature on the techniques from the NLP field that have been developed to utilise digital health records to identify risks of familial diseases. We highlight that rule-based methods are heavily investigated and are still actively used for family history extraction. Still, more recent efforts have been put into building neural models based on large-scale pre-trained language models. In addition to the areas where NLP has successfully been utilised, we also identify the areas where more research is needed to unlock the value of patients' records regarding data collection, task formulation and downstream applications.",
    "authors": [
      "Xiang Dai",
      "Sarvnaz Karimi",
      "Nathan O'Callaghan"
    ],
    "url": "http://arxiv.org/abs/2403.09997v1",
    "timestamp": 1710474187,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cbe61e6e-b13c-4fd8-928d-0eabaa325c2d": {
    "pk": "cbe61e6e-b13c-4fd8-928d-0eabaa325c2d",
    "title": "Generalised Graph Grammars for Natural Language Processing",
    "abstract": "This seminal paper proposes a new query language for graph matching and rewriting overcoming {the declarative} limitation of Cypher while outperforming {Neo4j} on graph matching and rewriting by at least one order of magnitude. We exploited columnar databases (KnoBAB) to represent graphs using the Generalised Semistructured Model.",
    "authors": [
      "Oliver Robert Fox",
      "Giacomo Bergami"
    ],
    "url": "http://arxiv.org/abs/2403.07481v1",
    "timestamp": 1710238473,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DB",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d1abf719-019d-4799-b26d-f2fefaf8b96c": {
    "pk": "d1abf719-019d-4799-b26d-f2fefaf8b96c",
    "title": "Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation",
    "abstract": "Natural Language Processing (NLP) is an important branch of artificial intelligence that studies how to enable computers to understand, process, and generate human language. Text classification is a fundamental task in NLP, which aims to classify text into different predefined categories. Text classification is the most basic and classic task in natural language processing, and most of the tasks in natural language processing can be regarded as classification tasks. In recent years, deep learning has achieved great success in many research fields, and today, it has also become a standard technology in the field of NLP, which is widely integrated into text classification tasks. Unlike numbers and images, text processing emphasizes fine-grained processing ability. Traditional text classification methods generally require preprocessing the input model's text data. Additionally, they also need to obtain good sample features through manual annotation and then use classical machine learning algorithms for classification. Therefore, this paper analyzes the application status of deep learning in the three core tasks of NLP (including text representation, word order modeling, and knowledge representation). This content explores the improvement and synergy achieved through natural language processing in the context of text classification, while also taking into account the challenges posed by adversarial techniques in text generation, text classification, and semantic parsing. An empirical study on text classification tasks demonstrates the effectiveness of interactive integration training, particularly in conjunction with TextCNN, highlighting the significance of these advancements in text classification augmentation and enhancement.",
    "authors": [
      "Xiaonan Xu",
      "Zheng Xu",
      "Zhipeng Ling",
      "Zhengyu Jin",
      "ShuQian Du"
    ],
    "url": "http://arxiv.org/abs/2403.09718v1",
    "timestamp": 1710228353,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5a298ca3-3fe8-488a-8183-79a93b463227": {
    "pk": "5a298ca3-3fe8-488a-8183-79a93b463227",
    "title": "Institutional-Level Monitoring of Immune Checkpoint Inhibitor IrAEs Using a Novel Natural Language Processing Algorithmic Pipeline",
    "abstract": "Background: Immune checkpoint inhibitors (ICIs) have revolutionized cancer treatment but can result in severe immune-related adverse events (IrAEs). Monitoring IrAEs on a large scale is essential for personalized risk profiling and assisting in treatment decisions.   Methods: In this study, we conducted an analysis of clinical notes from patients who received ICIs at the Tel Aviv Sourasky Medical Center. By employing a Natural Language Processing algorithmic pipeline, we systematically identified seven common or severe IrAEs. We examined the utilization of corticosteroids, treatment discontinuation rates following IrAEs, and constructed survival curves to visualize the occurrence of adverse events during treatment.   Results: Our analysis encompassed 108,280 clinical notes associated with 1,635 patients who had undergone ICI therapy. The detected incidence of IrAEs was consistent with previous reports, exhibiting substantial variation across different ICIs. Treatment with corticosteroids varied depending on the specific IrAE, ranging from 17.3% for thyroiditis to 57.4% for myocarditis. Our algorithm demonstrated high accuracy in identifying IrAEs, as indicated by an area under the curve (AUC) of 0.89 for each suspected note and F1 scores of 0.87 or higher for five out of the seven IrAEs examined at the patient level.   Conclusions: This study presents a novel, large-scale monitoring approach utilizing deep neural networks for IrAEs. Our method provides accurate results, enhancing understanding of detrimental consequences experienced by ICI-treated patients. Moreover, it holds potential for monitoring other medications, enabling comprehensive post-marketing surveillance to identify susceptible populations and establish personalized drug safety profiles.",
    "authors": [
      "Michael Shapiro",
      "Herut Dor",
      "Anna Gurevich-Shapiro",
      "Tal Etan",
      "Ido Wolf"
    ],
    "url": "http://arxiv.org/abs/2403.09708v1",
    "timestamp": 1710011907,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6e5dcaf0-cecc-478a-9a86-67c63308ab96": {
    "pk": "6e5dcaf0-cecc-478a-9a86-67c63308ab96",
    "title": "Analysis of Systems' Performance in Natural Language Processing Competitions",
    "abstract": "Collaborative competitions have gained popularity in the scientific and technological fields. These competitions involve defining tasks, selecting evaluation scores, and devising result verification methods. In the standard scenario, participants receive a training set and are expected to provide a solution for a held-out dataset kept by organizers. An essential challenge for organizers arises when comparing algorithms' performance, assessing multiple participants, and ranking them. Statistical tools are often used for this purpose; however, traditional statistical methods often fail to capture decisive differences between systems' performance. This manuscript describes an evaluation methodology for statistically analyzing competition results and competition. The methodology is designed to be universally applicable; however, it is illustrated using eight natural language competitions as case studies involving classification and regression problems. The proposed methodology offers several advantages, including off-the-shell comparisons with correction mechanisms and the inclusion of confidence intervals. Furthermore, we introduce metrics that allow organizers to assess the difficulty of competitions. Our analysis shows the potential usefulness of our methodology for effectively evaluating competition results.",
    "authors": [
      "Sergio Nava-Mu\u00f1oz",
      "Mario Graff",
      "Hugo Jair Escalante"
    ],
    "url": "http://arxiv.org/abs/2403.04693v1",
    "timestamp": 1709833360,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a4ab5549-9796-40ad-81f7-71c0bf00ae32": {
    "pk": "a4ab5549-9796-40ad-81f7-71c0bf00ae32",
    "title": "Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing",
    "abstract": "Purpose: Our study explored the use of artificial intelligence (AI) to diagnose autism spectrum disorder (ASD). It focused on machine learning (ML) and deep learning (DL) to detect ASD from text inputs on social media, addressing challenges in traditional ASD diagnosis.   Methods: We used natural language processing (NLP), ML, and DL models (including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to analyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A subset of 90,000 tweets was used for model training and testing.   Results: Our AI models showed high accuracy, with an 88% success rate in identifying texts from individuals with ASD.   Conclusion: The study demonstrates AI's potential in improving ASD diagnosis, especially in children, highlighting the importance of early detection.",
    "authors": [
      "Sergio Rubio-Mart\u00edn",
      "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s",
      "Mart\u00edn Bay\u00f3n-Guti\u00e9rrez",
      "Natalia Prieto-Fern\u00e1ndez",
      "Jos\u00e9 Alberto Ben\u00edtez-Andrades"
    ],
    "url": "http://arxiv.org/abs/2403.03581v1",
    "timestamp": 1709719062,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e3615705-74fe-470a-a249-97c6cbde1dde": {
    "pk": "e3615705-74fe-470a-a249-97c6cbde1dde",
    "title": "A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing",
    "abstract": "The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets. The tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach.",
    "authors": [
      "Yu Wang"
    ],
    "url": "http://arxiv.org/abs/2403.02504v1",
    "timestamp": 1709589071,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1d6f9bd8-fb36-4b2e-be47-d522d4448613": {
    "pk": "1d6f9bd8-fb36-4b2e-be47-d522d4448613",
    "title": "Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing",
    "abstract": "We present a natural language processing pipeline that was used to extract polymer solar cell property data from the literature and simulate various active learning strategies. While data-driven methods have been well established to discover novel materials faster than Edisonian trial-and-error approaches, their benefits have not been quantified. Our approach demonstrates a potential reduction in discovery time by approximately 75 %, equivalent to a 15 year acceleration in material innovation. Our pipeline enables us to extract data from more than 3300 papers which is ~5 times larger than similar data sets reported by others. We also trained machine learning models to predict the power conversion efficiency and used our model to identify promising donor-acceptor combinations that are as yet unreported. We thus demonstrate a workflow that goes from published literature to extracted material property data which in turn is used to obtain data-driven insights. Our insights include active learning strategies that can simultaneously optimize the material system and train strong predictive models of material properties. This work provides a valuable framework for research in material science.",
    "authors": [
      "Pranav Shetty",
      "Aishat Adeboye",
      "Sonakshi Gupta",
      "Chao Zhang",
      "Rampi Ramprasad"
    ],
    "url": "http://arxiv.org/abs/2402.19462v1",
    "timestamp": 1709232886,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cond-mat.mtrl-sci",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ecd8a98b-6532-4f31-8d71-f6162eb4b702": {
    "pk": "ecd8a98b-6532-4f31-8d71-f6162eb4b702",
    "title": "Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey",
    "abstract": "Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP. This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes. We first propose an overview of representations of symbolic music adapted from natural language sequential representations. Such representations are designed by considering the specificities of symbolic music. These representations are then processed by models. Such models, possibly originally developed for text and adapted for symbolic music, are trained on various tasks. We describe these models, in particular deep learning models, through different prisms, highlighting music-specialized mechanisms. We finally present a discussion surrounding the effective use of NLP tools for symbolic music data. This includes technical issues regarding NLP methods and fundamental differences between text and music, which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR.",
    "authors": [
      "Dinh-Viet-Toan Le",
      "Louis Bigo",
      "Mikaela Keller",
      "Dorien Herremans"
    ],
    "url": "http://arxiv.org/abs/2402.17467v1",
    "timestamp": 1709038081,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6fbe25a4-c365-4ea7-baae-53c4ade3eafe": {
    "pk": "6fbe25a4-c365-4ea7-baae-53c4ade3eafe",
    "title": "Social Media as a Sensor: Analyzing Twitter Data for Breast Cancer Medication Effects Using Natural Language Processing",
    "abstract": "Breast cancer is a significant public health concern and is the leading cause of cancer-related deaths among women. Despite advances in breast cancer treatments, medication non-adherence remains a major problem. As electronic health records do not typically capture patient-reported outcomes that may reveal information about medication-related experiences, social media presents an attractive resource for enhancing our understanding of the patients' treatment experiences. In this paper, we developed natural language processing (NLP) based methodologies to study information posted by an automatically curated breast cancer cohort from social media. We employed a transformer-based classifier to identify breast cancer patients/survivors on X (Twitter) based on their self-reported information, and we collected longitudinal data from their profiles. We then designed a multi-layer rule-based model to develop a breast cancer therapy-associated side effect lexicon and detect patterns of medication usage and associated side effects among breast cancer patients. 1,454,637 posts were available from 583,962 unique users, of which 62,042 were detected as breast cancer members using our transformer-based model. 198 cohort members mentioned breast cancer medications with tamoxifen as the most common. Our side effect lexicon identified well-known side effects of hormone and chemotherapy. Furthermore, it discovered a subject feeling towards cancer and medications, which may suggest a pre-clinical phase of side effects or emotional distress. This analysis highlighted not only the utility of NLP techniques in unstructured social media data to identify self-reported breast cancer posts, medication usage patterns, and treatment side effects but also the richness of social data on such clinical questions.",
    "authors": [
      "Seibi Kobara",
      "Alireza Rafiei",
      "Masoud Nateghi",
      "Selen Bozkurt",
      "Rishikesan Kamaleswaran",
      "Abeed Sarker"
    ],
    "url": "http://arxiv.org/abs/2403.00821v1",
    "timestamp": 1708964239,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "117ecf82-c8b8-49ed-9db5-92626ab14616": {
    "pk": "117ecf82-c8b8-49ed-9db5-92626ab14616",
    "title": "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline",
    "abstract": "Large Language Models (LLMs) are the cornerstone in automating Requirements Engineering (RE) tasks, underpinning recent advancements in the field. Their pre-trained comprehension of natural language is pivotal for effectively tailoring them to specific RE tasks. However, selecting an appropriate LLM from a myriad of existing architectures and fine-tuning it to address the intricacies of a given task poses a significant challenge for researchers and practitioners in the RE domain. Utilizing LLMs effectively for NLP problems in RE necessitates a dual understanding: firstly, of the inner workings of LLMs, and secondly, of a systematic approach to selecting and adapting LLMs for NLP4RE tasks. This chapter aims to furnish readers with essential knowledge about LLMs in its initial segment. Subsequently, it provides a comprehensive guideline tailored for students, researchers, and practitioners on harnessing LLMs to address their specific objectives. By offering insights into the workings of LLMs and furnishing a practical guide, this chapter contributes towards improving future research and applications leveraging LLMs for solving RE challenges.",
    "authors": [
      "Andreas Vogelsang",
      "Jannik Fischbach"
    ],
    "url": "http://arxiv.org/abs/2402.13823v3",
    "timestamp": 1708524052,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d29f4542-787a-4de3-b5d5-7284808acdf0": {
    "pk": "d29f4542-787a-4de3-b5d5-7284808acdf0",
    "title": "Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research",
    "abstract": "Entity and relationship extraction is a crucial component in natural language processing tasks such as knowledge graph construction, question answering system design, and semantic analysis. Most of the information of the Yishui school of traditional Chinese Medicine (TCM) is stored in the form of unstructured classical Chinese text. The key information extraction of TCM texts plays an important role in mining and studying the academic schools of TCM. In order to solve these problems efficiently using artificial intelligence methods, this study constructs a word segmentation and entity relationship extraction model based on conditional random fields under the framework of natural language processing technology to identify and extract the entity relationship of traditional Chinese medicine texts, and uses the common weighting technology of TF-IDF information retrieval and data mining to extract important key entity information in different ancient books. The dependency syntactic parser based on neural network is used to analyze the grammatical relationship between entities in each ancient book article, and it is represented as a tree structure visualization, which lays the foundation for the next construction of the knowledge graph of Yishui school and the use of artificial intelligence methods to carry out the research of TCM academic schools.",
    "authors": [
      "Hanqing Zhao",
      "Yuehan Li"
    ],
    "url": "http://arxiv.org/abs/2402.10743v1",
    "timestamp": 1708095595,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "26e1b307-1be2-4d78-b728-48637ce1f864": {
    "pk": "26e1b307-1be2-4d78-b728-48637ce1f864",
    "title": "EEVEE: An Easy Annotation Tool for Natural Language Processing",
    "abstract": "Annotation tools are the starting point for creating Natural Language Processing (NLP) datasets. There is a wide variety of tools available; setting up these tools is however a hindrance. We propose EEVEE, an annotation tool focused on simplicity, efficiency, and ease of use. It can run directly in the browser (no setup required) and uses tab-separated files (as opposed to character offsets or task-specific formats) for annotation. It allows for annotation of multiple tasks on a single dataset and supports four task-types: sequence labeling, span labeling, text classification and seq2seq.",
    "authors": [
      "Axel Sorensen",
      "Siyao Peng",
      "Barbara Plank",
      "Rob van der Goot"
    ],
    "url": "http://arxiv.org/abs/2402.02864v1",
    "timestamp": 1707128680,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ec26c89b-f9fa-4948-ac6b-c7fed456e054": {
    "pk": "ec26c89b-f9fa-4948-ac6b-c7fed456e054",
    "title": "Predicting ATP binding sites in protein sequences using Deep Learning and Natural Language Processing",
    "abstract": "Predicting ATP-Protein Binding sites in genes is of great significance in the field of Biology and Medicine. The majority of research in this field has been conducted through time- and resource-intensive 'wet experiments' in laboratories. Over the years, researchers have been investigating computational methods computational methods to accomplish the same goals, utilising the strength of advanced Deep Learning and NLP algorithms. In this paper, we propose to develop methods to classify ATP-Protein binding sites. We conducted various experiments mainly using PSSMs and several word embeddings as features. We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms. The MP3Vec and BERT models have also been subjected to testing in our study. The outcomes of our experiments demonstrated improvement over the state-of-the-art benchmarks.",
    "authors": [
      "Shreyas V",
      "Swati Agarwal"
    ],
    "url": "http://arxiv.org/abs/2402.01829v1",
    "timestamp": 1706899359,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "q-bio.BM",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "074060e6-395c-47b5-a55f-fe1dd1c8ef81": {
    "pk": "074060e6-395c-47b5-a55f-fe1dd1c8ef81",
    "title": "SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural Networks",
    "abstract": "As spiking neural networks receive more attention, we look toward applications of this computing paradigm in fields other than computer vision and signal processing. One major field, underexplored in the neuromorphic setting, is Natural Language Processing (NLP), where most state-of-the-art solutions still heavily rely on resource-consuming and power-hungry traditional deep learning architectures. Therefore, it is compelling to design NLP models for neuromorphic architectures due to their low energy requirements, with the additional benefit of a more human-brain-like operating model for processing information. However, one of the biggest issues with bringing NLP to the neuromorphic setting is in properly encoding text into a spike train so that it can be seamlessly handled by both current and future SNN architectures. In this paper, we compare various methods of encoding text as spikes and assess each method's performance in an associated SNN on a downstream NLP task, namely, sentiment analysis. Furthermore, we go on to propose a new method of encoding text as spikes that outperforms a widely-used rate-coding technique, Poisson rate-coding, by around 13\\% on our benchmark NLP tasks. Subsequently, we demonstrate the energy efficiency of SNNs implemented in hardware for the sentiment analysis task compared to traditional deep neural networks, observing an energy efficiency increase of more than 32x during inference and 60x during training while incurring the expected energy-performance tradeoff.",
    "authors": [
      "R. Alexander Knipper",
      "Kaniz Mishty",
      "Mehdi Sadi",
      "Shubhra Kanti Karmaker Santu"
    ],
    "url": "http://arxiv.org/abs/2401.17911v1",
    "timestamp": 1706714185,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ae04c749-6e37-4a0e-9cf6-023c10a1f9dc": {
    "pk": "ae04c749-6e37-4a0e-9cf6-023c10a1f9dc",
    "title": "Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing",
    "abstract": "The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP). While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks. Meanwhile, the symbolic methods excel in representing knowledge-rich data. However, they struggle to adapt dynamic data and generalize the knowledge. Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths. Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks. In this paper, we present an overview of hybrid approaches used for NLP. Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generation, and reasoning. Furthermore, we discuss the existing resources available for hybrid approaches for NLP along with the challenges and future directions, offering a roadmap for future research avenues.",
    "authors": [
      "Rrubaa Panchendrarajan",
      "Arkaitz Zubiaga"
    ],
    "url": "http://arxiv.org/abs/2401.11972v2",
    "timestamp": 1705933443,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4b76627c-caba-4506-aae4-503c522cd86f": {
    "pk": "4b76627c-caba-4506-aae4-503c522cd86f",
    "title": "Advancements in eHealth Data Analytics through Natural Language Processing and Deep Learning",
    "abstract": "The healthcare environment is commonly referred to as \"information-rich\" but also \"knowledge poor\". Healthcare systems collect huge amounts of data from various sources: lab reports, medical letters, logs of medical tools or programs, medical prescriptions, etc. These massive sets of data can provide great knowledge and information that can improve the medical services, and overall the healthcare domain, such as disease prediction by analyzing the patient's symptoms or disease prevention, by facilitating the discovery of behavioral factors for diseases. Unfortunately, only a relatively small volume of the textual eHealth data is processed and interpreted, an important factor being the difficulty in efficiently performing Big Data operations. In the medical field, detecting domain-specific multi-word terms is a crucial task as they can define an entire concept with a few words. A term can be defined as a linguistic structure or a concept, and it is composed of one or more words with a specific meaning to a domain. All the terms of a domain create its terminology. This chapter offers a critical study of the current, most performant solutions for analyzing unstructured (image and textual) eHealth data. This study also provides a comparison of the current Natural Language Processing and Deep Learning techniques in the eHealth context. Finally, we examine and discuss some of the current issues, and we define a set of research directions in this area.",
    "authors": [
      "Elena-Simona Apostol",
      "Ciprian-Octavian Truic\u0103"
    ],
    "url": "http://arxiv.org/abs/2401.10850v1",
    "timestamp": 1705686671,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "684da0ea-43cc-4e0d-a422-cdb3ff5f08fc": {
    "pk": "684da0ea-43cc-4e0d-a422-cdb3ff5f08fc",
    "title": "The \"Colonial Impulse\" of Natural Language Processing: An Audit of Bengali Sentiment Analysis Tools and Their Identity-based Biases",
    "abstract": "While colonization has sociohistorically impacted people's identities across various dimensions, those colonial values and biases continue to be perpetuated by sociotechnical systems. One category of sociotechnical systems--sentiment analysis tools--can also perpetuate colonial values and bias, yet less attention has been paid to how such tools may be complicit in perpetuating coloniality, although they are often used to guide various practices (e.g., content moderation). In this paper, we explore potential bias in sentiment analysis tools in the context of Bengali communities that have experienced and continue to experience the impacts of colonialism. Drawing on identity categories most impacted by colonialism amongst local Bengali communities, we focused our analytic attention on gender, religion, and nationality. We conducted an algorithmic audit of all sentiment analysis tools for Bengali, available on the Python package index (PyPI) and GitHub. Despite similar semantic content and structure, our analyses showed that in addition to inconsistencies in output from different tools, Bengali sentiment analysis tools exhibit bias between different identity categories and respond differently to different ways of identity expression. Connecting our findings with colonially shaped sociocultural structures of Bengali communities, we discuss the implications of downstream bias of sentiment analysis tools.",
    "authors": [
      "Dipto Das",
      "Shion Guha",
      "Jed Brubaker",
      "Bryan Semaan"
    ],
    "url": "http://arxiv.org/abs/2401.10535v1",
    "timestamp": 1705648905,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e7ea0619-0a58-4417-87e1-823e701eac7d": {
    "pk": "e7ea0619-0a58-4417-87e1-823e701eac7d",
    "title": "A Comparison of Veterans with Problematic Opioid Use Identified through Natural Language Processing of Clinical Notes versus Using Diagnostic Codes",
    "abstract": "Background: Electronic health records (EHRs) are a data source for opioid research. Opioid use disorder is known to be under-coded as a diagnosis, yet problematic opioid use can be documented in clinical notes.   Objectives: Our goals were 1) to identify problematic opioid use from a full range of clinical notes; and 2) to compare the characteristics of patients identified as having problematic opioid use, exclusively documented in clinical notes, to those having documented ICD opioid use disorder diagnostic codes.   Materials and Methods: We developed and applied a natural language processing (NLP) tool to the clinical notes of a patient cohort (n=222,371) from two Veteran Affairs service regions to identify patients with problematic opioid use. We also used a set of ICD diagnostic codes to identify patients with opioid use disorder from the same cohort. We compared the demographic and clinical characteristics of patients identified only through NLP, to those of patients identified through ICD codes.   Results: NLP exclusively identified 57,331 patients; 6,997 patients had positive ICD code identifications. Patients exclusively identified through NLP were more likely to be women. Those identified through ICD codes were more likely to be male, younger, have concurrent benzodiazepine prescriptions, more comorbidities, more care encounters, and less likely to be married. Patients in the NLP and ICD groups had substantially elevated comorbidity levels compared to patients not documented as experiencing problematic opioid use.   Conclusions: NLP is a feasible approach for identifying problematic opioid use not otherwise recorded by ICD codes. Clinicians may be reluctant to code for opioid use disorder. It is therefore incumbent on the healthcare team to search for documentation of opioid concerns within clinical notes.",
    "authors": [
      "Terri Elizabeth Workman",
      "Joel Kupersmith",
      "Phillip Ma",
      "Christopher Spevak",
      "Friedhelm Sandbrink",
      "Yan Cheng Qing Zeng-Treitler"
    ],
    "url": "http://arxiv.org/abs/2401.12996v1",
    "timestamp": 1705601296,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1b4ccbd3-3605-42fc-b7ab-de5fb5c1e5cc": {
    "pk": "1b4ccbd3-3605-42fc-b7ab-de5fb5c1e5cc",
    "title": "Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends",
    "abstract": "Natural Language Processing (NLP) aims to analyze text or speech via techniques in the computer science field. It serves the applications in domains of healthcare, commerce, education and so on. Particularly, NLP has been widely applied to the education domain and its applications have enormous potential to help teaching and learning. In this survey, we review recent advances in NLP with the focus on solving problems relevant to the education domain. In detail, we begin with introducing the related background and the real-world scenarios in education where NLP techniques could contribute. Then, we present a taxonomy of NLP in the education domain and highlight typical NLP applications including question answering, question construction, automated assessment, and error correction. Next, we illustrate the task definition, challenges, and corresponding cutting-edge techniques based on the above taxonomy. In particular, LLM-involved methods are included for discussion due to the wide usage of LLMs in diverse NLP applications. After that, we showcase some off-the-shelf demonstrations in this domain. At last, we conclude with six promising directions for future research, including more datasets in education domain, controllable usage of LLMs, intervention of difficulty-level control, interpretable educational NLP, methods with adaptive learning, and integrated systems for education. We organize all relevant datasets and papers in the open-available Github Link for better review~\\url{https://github.com/LiXinyuan1015/NLP-for-Education}.",
    "authors": [
      "Yunshi Lan",
      "Xinyuan Li",
      "Hanyue Du",
      "Xuesong Lu",
      "Ming Gao",
      "Weining Qian",
      "Aoying Zhou"
    ],
    "url": "http://arxiv.org/abs/2401.07518v3",
    "timestamp": 1705304922,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "27451680-fad5-4d1a-a47c-4a117b13e2b0": {
    "pk": "27451680-fad5-4d1a-a47c-4a117b13e2b0",
    "title": "Analyzing Regional Impacts of Climate Change using Natural Language Processing Techniques",
    "abstract": "Understanding the multifaceted effects of climate change across diverse geographic locations is crucial for timely adaptation and the development of effective mitigation strategies. As the volume of scientific literature on this topic continues to grow exponentially, manually reviewing these documents has become an immensely challenging task. Utilizing Natural Language Processing (NLP) techniques to analyze this wealth of information presents an efficient and scalable solution. By gathering extensive amounts of peer-reviewed articles and studies, we can extract and process critical information about the effects of climate change in specific regions. We employ BERT (Bidirectional Encoder Representations from Transformers) for Named Entity Recognition (NER), which enables us to efficiently identify specific geographies within the climate literature. This, in turn, facilitates location-specific analyses. We conduct region-specific climate trend analyses to pinpoint the predominant themes or concerns related to climate change within a particular area, trace the temporal progression of these identified issues, and evaluate their frequency, severity, and potential development over time. These in-depth examinations of location-specific climate data enable the creation of more customized policy-making, adaptation, and mitigation strategies, addressing each region's unique challenges and providing more effective solutions rooted in data-driven insights. This approach, founded on a thorough exploration of scientific texts, offers actionable insights to a wide range of stakeholders, from policymakers to engineers to environmentalists. By proactively understanding these impacts, societies are better positioned to prepare, allocate resources wisely, and design tailored strategies to cope with future climate conditions, ensuring a more resilient future for all.",
    "authors": [
      "Tanwi Mallick",
      "John Murphy",
      "Joshua David Bergerson",
      "Duane R. Verner",
      "John K Hutchison",
      "Leslie-Anne Levy"
    ],
    "url": "http://arxiv.org/abs/2401.06817v1",
    "timestamp": 1704991499,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b442524b-d1b8-4fcb-8914-85a7ba1a553d": {
    "pk": "b442524b-d1b8-4fcb-8914-85a7ba1a553d",
    "title": "Natural Language Processing for Dialects of a Language: A Survey",
    "abstract": "State-of-the-art natural language processing (NLP) models are trained on massive training corpora, and report a superlative performance on evaluation datasets. This survey delves into an important attribute of these datasets: the dialect of a language. Motivated by the performance degradation of NLP models for dialectic datasets and its implications for the equity of language technologies, we survey past research in NLP for dialects in terms of datasets, and approaches. We describe a wide range of NLP tasks in terms of two categories: natural language understanding (NLU) (for tasks such as dialect classification, sentiment analysis, parsing, and NLU benchmarks) and natural language generation (NLG) (for summarisation, machine translation, and dialogue systems). The survey is also broad in its coverage of languages which include English, Arabic, German among others. We observe that past work in NLP concerning dialects goes deeper than mere dialect classification, and . This includes early approaches that used sentence transduction that lead to the recent approaches that integrate hypernetworks into LoRA. We expect that this survey will be useful to NLP researchers interested in building equitable language technologies by rethinking LLM benchmarks and model architectures.",
    "authors": [
      "Aditya Joshi",
      "Raj Dabre",
      "Diptesh Kanojia",
      "Zhuang Li",
      "Haolan Zhan",
      "Gholamreza Haffari",
      "Doris Dippold"
    ],
    "url": "http://arxiv.org/abs/2401.05632v2",
    "timestamp": 1704942278,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c2af3946-c8c2-4f95-83a1-0a8e6052546b": {
    "pk": "c2af3946-c8c2-4f95-83a1-0a8e6052546b",
    "title": "CADgpt: Harnessing Natural Language Processing for 3D Modelling to Enhance Computer-Aided Design Workflows",
    "abstract": "This paper introduces CADgpt, an innovative plugin integrating Natural Language Processing (NLP) with Rhino3D for enhancing 3D modelling in computer-aided design (CAD) environments. Leveraging OpenAI's GPT-4, CADgpt simplifies the CAD interface, enabling users, particularly beginners, to perform complex 3D modelling tasks through intuitive natural language commands. This approach significantly reduces the learning curve associated with traditional CAD software, fostering a more inclusive and engaging educational environment. The paper discusses CADgpt's technical architecture, including its integration within Rhino3D and the adaptation of GPT-4 capabilities for CAD tasks. It presents case studies demonstrating CADgpt's efficacy in various design scenarios, highlighting its potential to democratise design education by making sophisticated design tools accessible to a broader range of students. The discussion further explores CADgpt's implications for pedagogy and curriculum development, emphasising its role in enhancing creative exploration and conceptual thinking in design education.   Keywords: Natural Language Processing, Computer-Aided Design, 3D Modelling, Design Automation, Design Education, Architectural Education",
    "authors": [
      "Timo Kapsalis"
    ],
    "url": "http://arxiv.org/abs/2401.05476v1",
    "timestamp": 1704907952,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.HC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c7804fff-faac-4059-8a83-f08897ae0561": {
    "pk": "c7804fff-faac-4059-8a83-f08897ae0561",
    "title": "Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches",
    "abstract": "Effective collaboration requires groups to strategically regulate themselves to overcome challenges. Research has shown that groups may fail to regulate due to differences in members' perceptions of challenges which may benefit from external support. In this study, we investigated the potential of leveraging three distinct natural language processing models: an expert knowledge rule-based model, a supervised machine learning (ML) model and a Large Language model (LLM), in challenge detection and challenge dimension identification (cognitive, metacognitive, emotional and technical/other challenges) from student discourse, was investigated. The results show that the supervised ML and the LLM approaches performed considerably well in both tasks, in contrast to the rule-based approach, whose efficacy heavily relies on the engineered features by experts. The paper provides an extensive discussion of the three approaches' performance for automated detection and support of students' challenge moments in collaborative learning activities. It argues that, although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation. We conclude the paper with a discussion on additional considerations, including model transparency to explore feasible and meaningful analytical feedback for students and educators using LLMs.",
    "authors": [
      "Wannapon Suraworachet",
      "Jennifer Seon",
      "Mutlu Cukurova"
    ],
    "url": "http://arxiv.org/abs/2401.01692v1",
    "timestamp": 1704282870,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2de324e2-b726-458f-b11b-2017a77a31e2": {
    "pk": "2de324e2-b726-458f-b11b-2017a77a31e2",
    "title": "Practical Guidelines for the Selection and Evaluation of Natural Language Processing Techniques in Requirements Engineering",
    "abstract": "[Context and Motivation] Natural Language Processing (NLP) is now a cornerstone of requirements automation. One compelling factor behind the growing adoption of NLP in Requirements Engineering (RE) is the prevalent use of natural language (NL) for specifying requirements in industry. NLP techniques are commonly used for automatically classifying requirements, extracting important information, e.g., domain models and glossary terms, and performing quality assurance tasks, such as ambiguity handling and completeness checking. With so many different NLP solution strategies available and the possibility of applying machine learning alongside, it can be challenging to choose the right strategy for a specific RE task and to evaluate the resulting solution in an empirically rigorous manner. [Content] In this chapter, we present guidelines for the selection of NLP techniques as well as for their evaluation in the context of RE. In particular, we discuss how to choose among different strategies such as traditional NLP, feature-based machine learning, and language-model-based methods. [Contribution] Our ultimate hope for this chapter is to serve as a stepping stone, assisting newcomers to NLP4RE in quickly initiating themselves into the NLP technologies most pertinent to the RE field.",
    "authors": [
      "Mehrdad Sabetzadeh",
      "Chetan Arora"
    ],
    "url": "http://arxiv.org/abs/2401.01508v2",
    "timestamp": 1704248675,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "11650d20-7a27-478c-9e1f-01d4f0838d79": {
    "pk": "11650d20-7a27-478c-9e1f-01d4f0838d79",
    "title": "Natural Language Processing and Multimodal Stock Price Prediction",
    "abstract": "In the realm of financial decision-making, predicting stock prices is pivotal. Artificial intelligence techniques such as long short-term memory networks (LSTMs), support-vector machines (SVMs), and natural language processing (NLP) models are commonly employed to predict said prices. This paper utilizes stock percentage change as training data, in contrast to the traditional use of raw currency values, with a focus on analyzing publicly released news articles. The choice of percentage change aims to provide models with context regarding the significance of price fluctuations and overall price change impact on a given stock. The study employs specialized BERT natural language processing models to predict stock price trends, with a particular emphasis on various data modalities. The results showcase the capabilities of such strategies with a small natural language processing model to accurately predict overall stock trends, and highlight the effectiveness of certain data features and sector-specific data.",
    "authors": [
      "Kevin Taylor",
      "Jerry Ng"
    ],
    "url": "http://arxiv.org/abs/2401.01487v1",
    "timestamp": 1704244890,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c634a3ec-2037-438f-8e5e-33931d0e5331": {
    "pk": "c634a3ec-2037-438f-8e5e-33931d0e5331",
    "title": "Fairness Certification for Natural Language Processing and Large Language Models",
    "abstract": "Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes to certify fairness, both from the perspective of the auditor and the audited organization.",
    "authors": [
      "Vincent Freiberger",
      "Erik Buchmann"
    ],
    "url": "http://arxiv.org/abs/2401.01262v2",
    "timestamp": 1704211776,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c9f995f8-5fc9-4ed8-bd1f-5a65fc75f487": {
    "pk": "c9f995f8-5fc9-4ed8-bd1f-5a65fc75f487",
    "title": "Uncovering Regulatory Affairs Complexity in Medical Products: A Qualitative Assessment Utilizing Open Coding and Natural Language Processing (NLP)",
    "abstract": "This study investigates the complexity of regulatory affairs in the medical device industry, a critical factor influencing market access and patient care. Through qualitative research, we sought expert insights to understand the factors contributing to this complexity. The study involved semi-structured interviews with 28 professionals from medical device companies, specializing in various aspects of regulatory affairs. These interviews were analyzed using open coding and Natural Language Processing (NLP) techniques. The findings reveal key sources of complexity within the regulatory landscape, divided into five domains: (A) Regulatory language complexity, (B) Intricacies within the regulatory process, (C) Global-level complexities, (D) Database-related considerations, and (E) Product-level issues. The participants highlighted the need for strategies to streamline regulatory compliance, enhance interactions between regulatory bodies and industry players, and develop adaptable frameworks for rapid technological advancements. Emphasizing interdisciplinary collaboration and increased transparency, the study concludes that these elements are vital for establishing coherent and effective regulatory procedures in the medical device sector.",
    "authors": [
      "Yu Han",
      "Aaron Ceross",
      "Jeroen H. M. Bergmann"
    ],
    "url": "http://arxiv.org/abs/2401.02975v1",
    "timestamp": 1703907597,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "458711af-6e4d-4200-a142-8befdd834ee0": {
    "pk": "458711af-6e4d-4200-a142-8befdd834ee0",
    "title": "From Dialogue to Diagram: Task and Relationship Extraction from Natural Language for Accelerated Business Process Prototyping",
    "abstract": "The automatic transformation of verbose, natural language descriptions into structured process models remains a challenge of significant complexity - This paper introduces a contemporary solution, where central to our approach, is the use of dependency parsing and Named Entity Recognition (NER) for extracting key elements from textual descriptions. Additionally, we utilize Subject-Verb-Object (SVO) constructs for identifying action relationships and integrate semantic analysis tools, including WordNet, for enriched contextual understanding. A novel aspect of our system is the application of neural coreference resolution, integrated with the SpaCy framework, enhancing the precision of entity linkage and anaphoric references. Furthermore, the system adeptly handles data transformation and visualization, converting extracted information into BPMN (Business Process Model and Notation) diagrams. This methodology not only streamlines the process of capturing and representing business workflows but also significantly reduces the manual effort and potential for error inherent in traditional modeling approaches.",
    "authors": [
      "Sara Qayyum",
      "Muhammad Moiz Asghar",
      "Muhammad Fouzan Yaseen"
    ],
    "url": "http://arxiv.org/abs/2312.10432v1",
    "timestamp": 1702730128,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8bf0cc9b-74b4-45fa-856e-5c2282f9699e": {
    "pk": "8bf0cc9b-74b4-45fa-856e-5c2282f9699e",
    "title": "A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors",
    "abstract": "This research delves into Musculoskeletal Disorder (MSD) risk factors, using a blend of Natural Language Processing (NLP) and mode-based ranking. The aim is to refine understanding, classification, and prioritization for focused prevention and treatment. Eight NLP models are evaluated, combining pre-trained transformers, cosine similarity, and distance metrics to categorize factors into personal, biomechanical, workplace, psychological, and organizational classes. BERT with cosine similarity achieves 28% accuracy; sentence transformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%. With 10-fold cross-validation, statistical tests ensure robust results. Survey data and mode-based ranking determine severity hierarchy, aligning with the literature. \"Working posture\" is the most severe, highlighting posture's role. Survey insights emphasize \"Job insecurity,\" \"Effort reward imbalance,\" and \"Poor employee facility\" as significant contributors. Rankings offer actionable insights for MSD prevention. The study suggests targeted interventions, workplace improvements, and future research directions. This integrated NLP and ranking approach enhances MSD comprehension and informs occupational health strategies.",
    "authors": [
      "Md Abrar Jahin",
      "Subrata Talapatra"
    ],
    "url": "http://arxiv.org/abs/2312.11517v3",
    "timestamp": 1702409663,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6a6794c0-d3dc-41d2-b981-291a2ec6fa19": {
    "pk": "6a6794c0-d3dc-41d2-b981-291a2ec6fa19",
    "title": "A Review of Hybrid and Ensemble in Deep Learning for Natural Language Processing",
    "abstract": "This review presents a comprehensive exploration of hybrid and ensemble deep learning models within Natural Language Processing (NLP), shedding light on their transformative potential across diverse tasks such as Sentiment Analysis, Named Entity Recognition, Machine Translation, Question Answering, Text Classification, Generation, Speech Recognition, Summarization, and Language Modeling. The paper systematically introduces each task, delineates key architectures from Recurrent Neural Networks (RNNs) to Transformer-based models like BERT, and evaluates their performance, challenges, and computational demands. The adaptability of ensemble techniques is emphasized, highlighting their capacity to enhance various NLP applications. Challenges in implementation, including computational overhead, overfitting, and model interpretation complexities, are addressed alongside the trade-off between interpretability and performance. Serving as a concise yet invaluable guide, this review synthesizes insights into tasks, architectures, and challenges, offering a holistic perspective for researchers and practitioners aiming to advance language-driven applications through ensemble deep learning in NLP.",
    "authors": [
      "Jianguo Jia",
      "Wen Liang",
      "Youzhi Liang"
    ],
    "url": "http://arxiv.org/abs/2312.05589v1",
    "timestamp": 1702133374,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "61efe5bc-6d65-468d-a2a6-845648efa8e4": {
    "pk": "61efe5bc-6d65-468d-a2a6-845648efa8e4",
    "title": "Illicit Darkweb Classification via Natural-language Processing: Classifying Illicit Content of Webpages based on Textual Information",
    "abstract": "This work aims at expanding previous works done in the context of illegal activities classification, performing three different steps. First, we created a heterogeneous dataset of 113995 onion sites and dark marketplaces. Then, we compared pre-trained transferable models, i.e., ULMFit (Universal Language Model Fine-tuning), Bert (Bidirectional Encoder Representations from Transformers), and RoBERTa (Robustly optimized BERT approach) with a traditional text classification approach like LSTM (Long short-term memory) neural networks. Finally, we developed two illegal activities classification approaches, one for illicit content on the Dark Web and one for identifying the specific types of drugs. Results show that Bert obtained the best approach, classifying the dark web's general content and the types of Drugs with 96.08% and 91.98% of accuracy.",
    "authors": [
      "Giuseppe Cascavilla",
      "Gemma Catolino",
      "Mirella Sangiovanni"
    ],
    "url": "http://arxiv.org/abs/2312.04944v1",
    "timestamp": 1702030788,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1c71f166-55b2-4df7-8257-b65570b0a0d5": {
    "pk": "1c71f166-55b2-4df7-8257-b65570b0a0d5",
    "title": "PyThaiNLP: Thai Natural Language Processing in Python",
    "abstract": "We present PyThaiNLP, a free and open-source natural language processing (NLP) library for Thai language implemented in Python. It provides a wide range of software, models, and datasets for Thai language. We first provide a brief historical context of tools for Thai language prior to the development of PyThaiNLP. We then outline the functionalities it provided as well as datasets and pre-trained language models. We later summarize its development milestones and discuss our experience during its development. We conclude by demonstrating how industrial and research communities utilize PyThaiNLP in their work. The library is freely available at https://github.com/pythainlp/pythainlp.",
    "authors": [
      "Wannaphong Phatthiyaphaibun",
      "Korakot Chaovavanich",
      "Charin Polpanumas",
      "Arthit Suriyawongkul",
      "Lalita Lowphansirikul",
      "Pattarawat Chormai",
      "Peerat Limkonchotiwat",
      "Thanathip Suntorntip",
      "Can Udomcharoenchaikit"
    ],
    "url": "http://arxiv.org/abs/2312.04649v1",
    "timestamp": 1701976783,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "43b06550-d5ce-41c7-aab8-8b2322a098c5": {
    "pk": "43b06550-d5ce-41c7-aab8-8b2322a098c5",
    "title": "Enabling Quantum Natural Language Processing for Hindi Language",
    "abstract": "Quantum Natural Language Processing (QNLP) is taking huge leaps in solving the shortcomings of classical Natural Language Processing (NLP) techniques and moving towards a more \"Explainable\" NLP system. The current literature around QNLP focuses primarily on implementing QNLP techniques in sentences in the English language. In this paper, we propose to enable the QNLP approach to HINDI, which is the third most spoken language in South Asia. We present the process of building the parameterized quantum circuits required to undertake QNLP on Hindi sentences. We use the pregroup representation of Hindi and the DisCoCat framework to draw sentence diagrams. Later, we translate these diagrams to Parameterised Quantum Circuits based on Instantaneous Quantum Polynomial (IQP) style ansatz. Using these parameterized quantum circuits allows one to train grammar and topic-aware sentence classifiers for the Hindi Language.",
    "authors": [
      "Naman Srivastava",
      "Gaurang Belekar",
      "Sunil Saumya",
      "Aswath Babu H"
    ],
    "url": "http://arxiv.org/abs/2312.01221v1",
    "timestamp": 1701548351,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "66e3090a-7dc9-4bad-aa89-c9b264369127": {
    "pk": "66e3090a-7dc9-4bad-aa89-c9b264369127",
    "title": "A natural language processing-based approach: mapping human perception by understanding deep semantic features in street view images",
    "abstract": "In the past decade, using Street View images and machine learning to measure human perception has become a mainstream research approach in urban science. However, this approach using only image-shallow information makes it difficult to comprehensively understand the deep semantic features of human perception of a scene. In this study, we proposed a new framework based on a pre-train natural language model to understand the relationship between human perception and the sense of a scene. Firstly, Place Pulse 2.0 was used as our base dataset, which contains a variety of human-perceived labels, namely, beautiful, safe, wealthy, depressing, boring, and lively. An image captioning network was used to extract the description information of each street view image. Secondly, a pre-trained BERT model was finetuning and added a regression function for six human perceptual dimensions. Furthermore, we compared the performance of five traditional regression methods with our approach and conducted a migration experiment in Hong Kong. Our results show that human perception scoring by deep semantic features performed better than previous studies by machine learning methods with shallow features. The use of deep scene semantic features provides new ideas for subsequent human perception research, as well as better explanatory power in the face of spatial heterogeneity.",
    "authors": [
      "Haoran Ma",
      "Dongdong Wu"
    ],
    "url": "http://arxiv.org/abs/2311.17354v1",
    "timestamp": 1701234043,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ce1bfec1-f6f2-4d76-9eca-c61781195e8f": {
    "pk": "ce1bfec1-f6f2-4d76-9eca-c61781195e8f",
    "title": "Natural Language Processing Through Transfer Learning: A Case Study on Sentiment Analysis",
    "abstract": "Artificial intelligence and machine learning have significantly bolstered the technological world. This paper explores the potential of transfer learning in natural language processing focusing mainly on sentiment analysis. The models trained on the big data can also be used where data are scarce. The claim is that, compared to training models from scratch, transfer learning, using pre-trained BERT models, can increase sentiment classification accuracy. The study adopts a sophisticated experimental design that uses the IMDb dataset of sentimentally labelled movie reviews. Pre-processing includes tokenization and encoding of text data, making it suitable for NLP models. The dataset is used on a BERT based model, measuring its performance using accuracy. The result comes out to be 100 per cent accurate. Although the complete accuracy could appear impressive, it might be the result of overfitting or a lack of generalization. Further analysis is required to ensure the model's ability to handle diverse and unseen data. The findings underscore the effectiveness of transfer learning in NLP, showcasing its potential to excel in sentiment analysis tasks. However, the research calls for a cautious interpretation of perfect accuracy and emphasizes the need for additional measures to validate the model's generalization.",
    "authors": [
      "Aman Yadav",
      "Abhishek Vichare"
    ],
    "url": "http://arxiv.org/abs/2311.16965v1",
    "timestamp": 1701191526,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a49545c4-2e71-4f78-b725-02eaa8c53af9": {
    "pk": "a49545c4-2e71-4f78-b725-02eaa8c53af9",
    "title": "De-identification of clinical free text using natural language processing: A systematic review of current approaches",
    "abstract": "Background: Electronic health records (EHRs) are a valuable resource for data-driven medical research. However, the presence of protected health information (PHI) makes EHRs unsuitable to be shared for research purposes. De-identification, i.e. the process of removing PHI is a critical step in making EHR data accessible. Natural language processing has repeatedly demonstrated its feasibility in automating the de-identification process. Objectives: Our study aims to provide systematic evidence on how the de-identification of clinical free text has evolved in the last thirteen years, and to report on the performances and limitations of the current state-of-the-art systems. In addition, we aim to identify challenges and potential research opportunities in this field. Methods: A systematic search in PubMed, Web of Science and the DBLP was conducted for studies published between January 2010 and February 2023. Titles and abstracts were examined to identify the relevant studies. Selected studies were then analysed in-depth, and information was collected on de-identification methodologies, data sources, and measured performance. Results: A total of 2125 publications were identified for the title and abstract screening. 69 studies were found to be relevant. Machine learning (37 studies) and hybrid (26 studies) approaches are predominant, while six studies relied only on rules. Majority of the approaches were trained and evaluated on public corpora. The 2014 i2b2/UTHealth corpus is the most frequently used (36 studies), followed by the 2006 i2b2 (18 studies) and 2016 CEGS N-GRID (10 studies) corpora.",
    "authors": [
      "Aleksandar Kova\u010devi\u0107",
      "Bojana Ba\u0161aragin",
      "Nikola Milo\u0161evi\u0107",
      "Goran Nenadi\u0107"
    ],
    "url": "http://arxiv.org/abs/2312.03736v1",
    "timestamp": 1701177641,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9cfe2e40-0ceb-4bb2-8bea-437c5cb0f926": {
    "pk": "9cfe2e40-0ceb-4bb2-8bea-437c5cb0f926",
    "title": "Ascle: A Python Natural Language Processing Toolkit for Medical Text Generation",
    "abstract": "This study introduces Ascle, a pioneering natural language processing (NLP) toolkit designed for medical text generation. Ascle is tailored for biomedical researchers and healthcare professionals with an easy-to-use, all-in-one solution that requires minimal programming expertise. For the first time, Ascle evaluates and provides interfaces for the latest pre-trained language models, encompassing four advanced and challenging generative functions: question-answering, text summarization, text simplification, and machine translation. In addition, Ascle integrates 12 essential NLP functions, along with query and search capabilities for clinical databases. The toolkit, its models, and associated data are publicly available via https://github.com/Yale-LILY/MedGen.",
    "authors": [
      "Rui Yang",
      "Qingcheng Zeng",
      "Keen You",
      "Yujie Qiao",
      "Lucas Huang",
      "Chia-Chun Hsieh",
      "Benjamin Rosand",
      "Jeremy Goldwasser",
      "Amisha D Dave",
      "Tiarnan D. L. Keenan",
      "Emily Y Chew",
      "Dragomir Radev",
      "Zhiyong Lu",
      "Hua Xu",
      "Qingyu Chen",
      "Irene Li"
    ],
    "url": "http://arxiv.org/abs/2311.16588v2",
    "timestamp": 1701159209,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fd6165ec-97af-4f27-bc9c-55afc379237d": {
    "pk": "fd6165ec-97af-4f27-bc9c-55afc379237d",
    "title": "Combatting Human Trafficking in the Cyberspace: A Natural Language Processing-Based Methodology to Analyze the Language in Online Advertisements",
    "abstract": "This project tackles the pressing issue of human trafficking in online C2C marketplaces through advanced Natural Language Processing (NLP) techniques. We introduce a novel methodology for generating pseudo-labeled datasets with minimal supervision, serving as a rich resource for training state-of-the-art NLP models. Focusing on tasks like Human Trafficking Risk Prediction (HTRP) and Organized Activity Detection (OAD), we employ cutting-edge Transformer models for analysis. A key contribution is the implementation of an interpretability framework using Integrated Gradients, providing explainable insights crucial for law enforcement. This work not only fills a critical gap in the literature but also offers a scalable, machine learning-driven approach to combat human exploitation online. It serves as a foundation for future research and practical applications, emphasizing the role of machine learning in addressing complex social issues.",
    "authors": [
      "Alejandro Rodriguez Perez",
      "Pablo Rivas"
    ],
    "url": "http://arxiv.org/abs/2311.13118v1",
    "timestamp": 1700621101,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ff950aba-841c-4647-89ef-41b81b8ff6c1": {
    "pk": "ff950aba-841c-4647-89ef-41b81b8ff6c1",
    "title": "Natural Language Processing for Financial Regulation",
    "abstract": "This article provides an understanding of Natural Language Processing techniques in the framework of financial regulation, more specifically in order to perform semantic matching search between rules and policy when no dataset is available for supervised learning. We outline how to outperform simple pre-trained sentences-transformer models using freely available resources and explain the mathematical concepts behind the key building blocks of Natural Language Processing.",
    "authors": [
      "Ixandra Achitouv",
      "Dragos Gorduza",
      "Antoine Jacquier"
    ],
    "url": "http://arxiv.org/abs/2311.08533v1",
    "timestamp": 1699995501,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d60a88ca-4232-4287-b584-2f0f7b0b74f0": {
    "pk": "d60a88ca-4232-4287-b584-2f0f7b0b74f0",
    "title": "calamanCy: A Tagalog Natural Language Processing Toolkit",
    "abstract": "We introduce calamanCy, an open-source toolkit for constructing natural language processing (NLP) pipelines for Tagalog. It is built on top of spaCy, enabling easy experimentation and integration with other frameworks. calamanCy addresses the development gap by providing a consistent API for building NLP applications and offering general-purpose multitask models with out-of-the-box support for dependency parsing, parts-of-speech (POS) tagging, and named entity recognition (NER). calamanCy aims to accelerate the progress of Tagalog NLP by consolidating disjointed resources in a unified framework. The calamanCy toolkit is available on GitHub: https://github.com/ljvmiranda921/calamanCy.",
    "authors": [
      "Lester James V. Miranda"
    ],
    "url": "http://arxiv.org/abs/2311.07171v1",
    "timestamp": 1699866403,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3d0a9c5b-0670-412c-8151-9b9c08c17699": {
    "pk": "3d0a9c5b-0670-412c-8151-9b9c08c17699",
    "title": "Performance Prediction of Data-Driven Knowledge summarization of High Entropy Alloys (HEAs) literature implementing Natural Language Processing algorithms",
    "abstract": "The ability to interpret spoken language is connected to natural language processing. It involves teaching the AI how words relate to one another, how they are meant to be used, and in what settings. The goal of natural language processing (NLP) is to get a machine intelligence to process words the same way a human brain does. This enables machine intelligence to interpret, arrange, and comprehend textual data by processing the natural language. The technology can comprehend what is communicated, whether it be through speech or writing because AI pro-cesses language more quickly than humans can. In the present study, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic Analysis (LSA), and Kull-back-Liebler (KL) al-gorithm, are implemented for the first time for the knowledge summarization purpose of the High Entropy Alloys (HEAs). The performance prediction of these algorithms is made by using the BLEU score and ROUGE score. The results showed that the Luhn algorithm has the highest accuracy score for the knowledge summarization tasks compared to the other used algorithms.",
    "authors": [
      "Akshansh Mishra",
      "Vijaykumar S Jatti",
      "Vaishnavi More",
      "Anish Dasgupta",
      "Devarrishi Dixit",
      "Eyob Messele Sefene"
    ],
    "url": "http://arxiv.org/abs/2311.07584v1",
    "timestamp": 1699287752,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "351fe475-bf08-4d02-9611-b4c1a3930b2c": {
    "pk": "351fe475-bf08-4d02-9611-b4c1a3930b2c",
    "title": "mahaNLP: A Marathi Natural Language Processing Library",
    "abstract": "We present mahaNLP, an open-source natural language processing (NLP) library specifically built for the Marathi language. It aims to enhance the support for the low-resource Indian language Marathi in the field of NLP. It is an easy-to-use, extensible, and modular toolkit for Marathi text analysis built on state-of-the-art MahaBERT-based transformer models. Our work holds significant importance as other existing Indic NLP libraries provide basic Marathi processing support and rely on older models with restricted performance. Our toolkit stands out by offering a comprehensive array of NLP tasks, encompassing both fundamental preprocessing tasks and advanced NLP tasks like sentiment analysis, NER, hate speech detection, and sentence completion. This paper focuses on an overview of the mahaNLP framework, its features, and its usage. This work is a part of the L3Cube MahaNLP initiative, more information about it can be found at https://github.com/l3cube-pune/MarathiNLP .",
    "authors": [
      "Vidula Magdum",
      "Omkar Dhekane",
      "Sharayu Hiwarkhedkar",
      "Saloni Mittal",
      "Raviraj Joshi"
    ],
    "url": "http://arxiv.org/abs/2311.02579v1",
    "timestamp": 1699167599,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4467bf92-9bb7-4193-9cf2-53c03acc523b": {
    "pk": "4467bf92-9bb7-4193-9cf2-53c03acc523b",
    "title": "An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology",
    "abstract": "Natural Language Processing (NLP) is a key technique for developing Medical Artificial Intelligence (AI) systems that leverage Electronic Health Record (EHR) data to build diagnostic and prognostic models. NLP enables the conversion of unstructured clinical text into structured data that can be fed into AI algorithms. The emergence of the transformer architecture and large language models (LLMs) has led to remarkable advances in NLP for various healthcare tasks, such as entity recognition, relation extraction, sentence similarity, text summarization, and question answering. In this article, we review the major technical innovations that underpin modern NLP models and present state-of-the-art NLP applications that employ LLMs in radiation oncology research. However, these LLMs are prone to many errors such as hallucinations, biases, and ethical violations, which necessitate rigorous evaluation and validation before clinical deployment. As such, we propose a comprehensive framework for assessing the NLP models based on their purpose and clinical fit, technical performance, bias and trust, legal and ethical implications, and quality assurance, prior to implementation in clinical radiation oncology. Our article aims to provide guidance and insights for researchers and clinicians who are interested in developing and using NLP models in clinical radiation oncology.",
    "authors": [
      "Reza Khanmohammadi",
      "Mohammad M. Ghassemi",
      "Kyle Verdecchia",
      "Ahmed I. Ghanem",
      "Luo Bing",
      "Indrin J. Chetty",
      "Hassan Bagher-Ebadian",
      "Farzan Siddiqui",
      "Mohamed Elshaikh",
      "Benjamin Movsas",
      "Kundan Thind"
    ],
    "url": "http://arxiv.org/abs/2311.02205v2",
    "timestamp": 1699039955,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b1975cfe-076b-49fe-97a8-15f0744a7021": {
    "pk": "b1975cfe-076b-49fe-97a8-15f0744a7021",
    "title": "Partial Tensorized Transformers for Natural Language Processing",
    "abstract": "The transformer architecture has revolutionized Natural Language Processing (NLP) and other machine-learning tasks, due to its unprecedented accuracy. However, their extensive memory and parameter requirements often hinder their practical applications. In this work, we study the effect of tensor-train decomposition to improve the accuracy and compress transformer vision-language neural networks, namely BERT and ViT. We focus both on embedding-layer compression and partial tensorization of neural networks (PTNN) through an algorithmic approach. Our novel PTNN approach significantly improves the accuracy of existing models by up to 5%, all without the need for post-training adjustments, breaking new ground in the field of tensor decomposition.",
    "authors": [
      "Subhadra Vadlamannati",
      "Ryan Solgi"
    ],
    "url": "http://arxiv.org/abs/2310.20077v1",
    "timestamp": 1698707946,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b9b45db9-96ad-4c7e-9a66-cf3fd5640170": {
    "pk": "b9b45db9-96ad-4c7e-9a66-cf3fd5640170",
    "title": "BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing",
    "abstract": "To enhance the performance of large language models (LLMs) in biomedical natural language processing (BioNLP) by introducing a domain-specific instruction dataset and examining its impact when combined with multi-task learning principles. We created the BioInstruct, comprising 25,005 instructions to instruction-tune LLMs(LLaMA 1 & 2, 7B & 13B version). The instructions were created by prompting the GPT-4 language model with three-seed samples randomly drawn from an 80 human curated instructions. We employed Low-Rank Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three major categories: question answering(QA), information extraction(IE), and text generation(GEN). We also examined whether categories(e.g., QA, IE, and generation) of instructions impact model performance. Comparing with LLMs without instruction-tuned, our instruction-tuned LLMs demonstrated marked performance gains: 17.3% in QA, 5.7% in IE, and 96% in Generation tasks. Our 7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with vast domain-specific data or a variety of tasks. Our results also show that the performance gain is significantly higher when instruction fine-tuning is conducted with closely related tasks. Our findings align with the observations of multi-task learning, suggesting the synergies between two tasks. The BioInstruct dataset serves as a valuable resource and instruction tuned LLMs lead to the best performing BioNLP applications.",
    "authors": [
      "Hieu Tran",
      "Zhichao Yang",
      "Zonghai Yao",
      "Hong Yu"
    ],
    "url": "http://arxiv.org/abs/2310.19975v3",
    "timestamp": 1698694730,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2db021d9-8038-42f7-ab3d-31a4ffef500c": {
    "pk": "2db021d9-8038-42f7-ab3d-31a4ffef500c",
    "title": "This Reads Like That: Deep Learning for Interpretable Natural Language Processing",
    "abstract": "Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained sentence embeddings. Additionally, we propose a post-hoc explainability mechanism that extracts prediction-relevant words from both the prototype and input sentences. Finally, we empirically demonstrate that our proposed method not only improves predictive performance on the AG News and RT Polarity datasets over a previous prototype-based approach, but also improves the faithfulness of explanations compared to rationale-based recurrent convolutions.",
    "authors": [
      "Claudio Fanconi",
      "Moritz Vandenhirtz",
      "Severin Husmann",
      "Julia E. Vogt"
    ],
    "url": "http://arxiv.org/abs/2310.17010v1",
    "timestamp": 1698268715,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6f327a2b-88c4-43ce-9c71-041ba80228b7": {
    "pk": "6f327a2b-88c4-43ce-9c71-041ba80228b7",
    "title": "Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls",
    "abstract": "Building and analysing knowledge graphs (KGs) to aid drug discovery is a topical area of research. A salient feature of KGs is their ability to combine many heterogeneous data sources in a format that facilitates discovering connections. The utility of KGs has been exemplified in areas such as drug repurposing, with insights made through manual exploration and modelling of the data. In this article, we discuss promises and pitfalls of using natural language processing (NLP) to mine unstructured text typically from scientific literature as a data source for KGs. This draws on our experience of initially parsing structured data sources such as ChEMBL as the basis for data within a KG, and then enriching or expanding upon them using NLP. The fundamental promise of NLP for KGs is the automated extraction of data from millions of documents a task practically impossible to do via human curation alone. However, there are many potential pitfalls in NLP-KG pipelines such as incorrect named entity recognition and ontology linking all of which could ultimately lead to erroneous inferences and conclusions.",
    "authors": [
      "J. Charles G. Jeynes",
      "Tim James",
      "Matthew Corney"
    ],
    "url": "http://arxiv.org/abs/2310.15572v1",
    "timestamp": 1698132924,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9810f5ba-7adf-4897-a7aa-a4a0a59f1adc": {
    "pk": "9810f5ba-7adf-4897-a7aa-a4a0a59f1adc",
    "title": "A Review of Reinforcement Learning for Natural Language Processing, and Applications in Healthcare",
    "abstract": "Reinforcement learning (RL) has emerged as a powerful approach for tackling complex medical decision-making problems such as treatment planning, personalized medicine, and optimizing the scheduling of surgeries and appointments. It has gained significant attention in the field of Natural Language Processing (NLP) due to its ability to learn optimal strategies for tasks such as dialogue systems, machine translation, and question-answering. This paper presents a review of the RL techniques in NLP, highlighting key advancements, challenges, and applications in healthcare. The review begins by visualizing a roadmap of machine learning and its applications in healthcare. And then it explores the integration of RL with NLP tasks. We examined dialogue systems where RL enables the learning of conversational strategies, RL-based machine translation models, question-answering systems, text summarization, and information extraction. Additionally, ethical considerations and biases in RL-NLP systems are addressed.",
    "authors": [
      "Ying Liu",
      "Haozhu Wang",
      "Huixue Zhou",
      "Mingchen Li",
      "Yu Hou",
      "Sicheng Zhou",
      "Fang Wang",
      "Rama Hoetzlein",
      "Rui Zhang"
    ],
    "url": "http://arxiv.org/abs/2310.18354v1",
    "timestamp": 1698092775,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c20fa7de-2c57-418a-a554-322d7279df72": {
    "pk": "c20fa7de-2c57-418a-a554-322d7279df72",
    "title": "From Proprietary to High-Level Trigger-Action Programming Rules: A Natural Language Processing Approach",
    "abstract": "With the rise of popular task automation or IoT platforms such as 'If This Then That (IFTTT)', users can define rules to enable interactions between smart devices in their environment and thereby improve their daily lives. However, the rules authored via these platforms are usually tied to the platforms and sometimes even to the specific devices for which they have been defined. Therefore, when a user wishes to move to a different environment controlled by a different platform and/or devices, they need to recreate their rules for the new environment. The rise in the number of smart devices further adds to the complexity of rule authoring since users will have to navigate an ever-changing landscape of IoT devices. In order to address this problem, we need human-computer interaction that works across the boundaries of specific IoT platforms and devices. A step towards this human-computer interaction across platforms and devices is the introduction of a high-level semantic model for end-user IoT development, enabling users to create rules at a higher level of abstraction. However, many users who already got used to the rule representation in their favourite tool might be unwilling to learn and adapt to a new representation. We present a method for translating proprietary rules to a high-level semantic model by using natural language processing techniques. Our translation enables users to work with their familiar rule representation language and tool, and at the same time apply their rules across different IoT platforms and devices.",
    "authors": [
      "Ekene Attoh",
      "Beat Signer"
    ],
    "url": "http://arxiv.org/abs/2310.15024v1",
    "timestamp": 1698074605,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.HC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4f03c780-8d44-4221-817f-576730425a0e": {
    "pk": "4f03c780-8d44-4221-817f-576730425a0e",
    "title": "We are Who We Cite: Bridges of Influence Between Natural Language Processing and Other Academic Fields",
    "abstract": "Natural Language Processing (NLP) is poised to substantially influence the world. However, significant progress comes hand-in-hand with substantial risks. Addressing them requires broad engagement with various fields of study. Yet, little empirical work examines the state of such engagement (past or current). In this paper, we quantify the degree of influence between 23 fields of study and NLP (on each other). We analyzed ~77k NLP papers, ~3.1m citations from NLP papers to other papers, and ~1.8m citations from other papers to NLP papers. We show that, unlike most fields, the cross-field engagement of NLP, measured by our proposed Citation Field Diversity Index (CFDI), has declined from 0.58 in 1980 to 0.31 in 2022 (an all-time low). In addition, we find that NLP has grown more insular -- citing increasingly more NLP papers and having fewer papers that act as bridges between fields. NLP citations are dominated by computer science; Less than 8% of NLP citations are to linguistics, and less than 3% are to math and psychology. These findings underscore NLP's urgent need to reflect on its engagement with various fields.",
    "authors": [
      "Jan Philip Wahle",
      "Terry Ruas",
      "Mohamed Abdalla",
      "Bela Gipp",
      "Saif M. Mohammad"
    ],
    "url": "http://arxiv.org/abs/2310.14870v1",
    "timestamp": 1698064926,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d47326de-9c6a-4fcf-8f06-4e86aa4c6d1e": {
    "pk": "d47326de-9c6a-4fcf-8f06-4e86aa4c6d1e",
    "title": "Social Media Perceptions of 51% Attacks on Proof-of-Work Cryptocurrencies: A Natural Language Processing Approach",
    "abstract": "This work is the first study on the effects of attacks on cryptocurrencies as expressed in the sentiments and emotions of social media users. Our goals are to design the methodologies for the study including data collection, conduct volumetric and temporal analyses of the data, and profile the sentiments and emotions that emerge from the data. As a first step, we have created a first-of-its-kind comprehensive list of 31 events of 51% attacks on various PoW cryptocurrencies, showing that these events are quite common contrary to the general perception. We have gathered Twitter data on the events as well as benchmark data during normal times for comparison. We have defined parameters for profiling the datasets based on their sentiments and emotions. We have studied the variation of these sentiment and emotion profiles when a cryptocurrency is under attack and the benchmark otherwise, between multiple attack events of the same cryptocurrency, and between different cryptocurrencies. Our results confirm some expected overall behaviour and reactions while providing nuanced insights that may not be obvious or may even be considered surprising. Our code and datasets are publicly accessible.",
    "authors": [
      "Zsofia Baruwa",
      "Sanjay Bhattacherjee",
      "Sahil Rey Chandnani",
      "Zhen Zhu"
    ],
    "url": "http://arxiv.org/abs/2310.14307v2",
    "timestamp": 1697983516,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "44e8a84a-46bc-44dd-bac1-016fa19018a3": {
    "pk": "44e8a84a-46bc-44dd-bac1-016fa19018a3",
    "title": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing",
    "abstract": "The emergence of Large Language Models (LLMs), such as ChatGPT, has revolutionized general natural language preprocessing (NLP) tasks. However, their expertise in the financial domain lacks a comprehensive evaluation. To assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval, a framework for Financial Language Model Evaluation, comprising nine datasets designed to evaluate the performance of language models. This study compares the performance of encoder-only language models and the decoder-only language models. Our findings reveal that while some decoder-only LLMs demonstrate notable performance across most financial tasks via zero-shot prompting, they generally lag behind the fine-tuned expert models, especially when dealing with proprietary datasets. We hope this study provides foundation evaluations for continuing efforts to build more advanced LLMs in the financial domain.",
    "authors": [
      "Yue Guo",
      "Zian Xu",
      "Yi Yang"
    ],
    "url": "http://arxiv.org/abs/2310.12664v1",
    "timestamp": 1697715795,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "628bc2ac-f13f-4375-a4ed-29433a6be534": {
    "pk": "628bc2ac-f13f-4375-a4ed-29433a6be534",
    "title": "Field-testing items using artificial intelligence: Natural language processing with transformers",
    "abstract": "Five thousand variations of the RoBERTa model, an artificially intelligent \"transformer\" that can understand text language, completed an English literacy exam with 29 multiple-choice questions. Data were used to calculate the psychometric properties of the items, which showed some degree of agreement to those obtained from human examinee data.",
    "authors": [
      "Hotaka Maeda"
    ],
    "url": "http://arxiv.org/abs/2310.11655v1",
    "timestamp": 1697594176,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f655bf38-b60e-434e-b749-e339c90a9ffb": {
    "pk": "f655bf38-b60e-434e-b749-e339c90a9ffb",
    "title": "Enhanced Transformer Architecture for Natural Language Processing",
    "abstract": "Transformer is a state-of-the-art model in the field of natural language processing (NLP). Current NLP models primarily increase the number of transformers to improve processing performance. However, this technique requires a lot of training resources such as computing capacity. In this paper, a novel structure of Transformer is proposed. It is featured by full layer normalization, weighted residual connection, positional encoding exploiting reinforcement learning, and zero masked self-attention. The proposed Transformer model, which is called Enhanced Transformer, is validated by the bilingual evaluation understudy (BLEU) score obtained with the Multi30k translation dataset. As a result, the Enhanced Transformer achieves 202.96% higher BLEU score as compared to the original transformer with the translation dataset.",
    "authors": [
      "Woohyeon Moon",
      "Taeyoung Kim",
      "Bumgeun Park",
      "Dongsoo Har"
    ],
    "url": "http://arxiv.org/abs/2310.10930v1",
    "timestamp": 1697507947,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "aa4b59f7-4ca2-4d52-b622-78f96680f77c": {
    "pk": "aa4b59f7-4ca2-4d52-b622-78f96680f77c",
    "title": "ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing",
    "abstract": "Climate change's impact on human health poses unprecedented and diverse challenges. Unless proactive measures based on solid evidence are implemented, these threats will likely escalate and continue to endanger human well-being. The escalating advancements in information and communication technologies have facilitated the widespread availability and utilization of social media platforms. Individuals utilize platforms such as Twitter and Facebook to express their opinions, thoughts, and critiques on diverse subjects, encompassing the pressing issue of climate change. The proliferation of climate change-related content on social media necessitates comprehensive analysis to glean meaningful insights. This paper employs natural language processing (NLP) techniques to analyze climate change discourse and quantify the sentiment of climate change-related tweets. We use ClimateBERT, a pretrained model fine-tuned specifically for the climate change domain. The objective is to discern the sentiment individuals express and uncover patterns in public opinion concerning climate change. Analyzing tweet sentiments allows a deeper comprehension of public perceptions, concerns, and emotions about this critical global challenge. The findings from this experiment unearth valuable insights into public sentiment and the entities associated with climate change discourse. Policymakers, researchers, and organizations can leverage such analyses to understand public perceptions, identify influential actors, and devise informed strategies to address climate change challenges.",
    "authors": [
      "Ajay Krishnan",
      "V. S. Anoop"
    ],
    "url": "http://arxiv.org/abs/2310.08099v2",
    "timestamp": 1697096930,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "335d8370-ad57-4aa1-bf38-3724b28ba2d1": {
    "pk": "335d8370-ad57-4aa1-bf38-3724b28ba2d1",
    "title": "To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing",
    "abstract": "NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.",
    "authors": [
      "Sireesh Gururaja",
      "Amanda Bertsch",
      "Clara Na",
      "David Gray Widder",
      "Emma Strubell"
    ],
    "url": "http://arxiv.org/abs/2310.07715v1",
    "timestamp": 1697047176,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2cdedaf7-9920-4529-901b-a8cc817ca74d": {
    "pk": "2cdedaf7-9920-4529-901b-a8cc817ca74d",
    "title": "Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining",
    "abstract": "Estimates of energy usage in layers of computing from devices to algorithms have been determined and analyzed. Building on the previous analysis [3], energy needed from single devices and systems including three large-scale computing applications such as Artificial Intelligence (AI)/Machine Learning for Natural Language Processing, Scientific Simulations, and Cryptocurrency Mining have been estimated. In contrast to the bit-level switching, in which transistors achieved energy efficiency due to geometrical scaling, higher energy is expended both at the at the instructions and simulations levels of an application. Additionally, the analysis based on AI/ML Accelerators indicate that changes in architectures using an older semiconductor technology node have comparable energy efficiency with a different architecture using a newer technology. Further comparisons of the energy in computing systems with the thermodynamic and biological limits, indicate that there is a 27-36 orders of magnitude higher energy requirements for total simulation of an application. These energy estimates underscore the need for serious considerations of energy efficiency in computing by including energy as a design parameter, enabling growing needs of compute-intensive applications in a digital world.",
    "authors": [
      "Sadasivan Shankar"
    ],
    "url": "http://arxiv.org/abs/2310.07516v1",
    "timestamp": 1697033645,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2c649fd9-1304-485f-8d04-02b99e2a656a": {
    "pk": "2c649fd9-1304-485f-8d04-02b99e2a656a",
    "title": "Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI",
    "abstract": "Since the invention of computers, communication through natural language (actual human language) has been a dream technology. However, natural language is extremely difficult to mathematically formulate, making it difficult to realize as an algorithm without considering programming. While there have been numerous technological developments, one cannot say that any results allowing free utilization have been achieved thus far. In the case of language learning in humans, for instance when learning one's mother tongue or foreign language, one must admit that this process is similar to the adage \"practice makes perfect\" in principle, even though the learning method is significant up to a point. Deep learning has played a central role in contemporary AI technology in recent years. When applied to natural language processing (NLP), this produced unprecedented results. Achievements exceeding the initial predictions have been reported from the results of learning vast amounts of textual data using deep learning. For instance, four arithmetic operations could be performed without explicit learning, thereby enabling the explanation of complex images and the generation of images from corresponding explanatory texts. It is an accurate example of the learner embodying the concept of \"practice makes perfect\" by using vast amounts of textual data. This report provides a technological explanation of how cutting-edge NLP has made it possible to realize the \"practice makes perfect\" principle. Additionally, examples of how this can be applied to business are provided. We reported in June 2022 in Japanese on the NLP movement from late 2021 to early 2022. We would like to summarize this as a memorandum since this is just the initial movement leading to the current large language models (LLMs).",
    "authors": [
      "Masahiro Yamamoto"
    ],
    "url": "http://arxiv.org/abs/2310.06228v1",
    "timestamp": 1696898498,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6a5f41dd-ab8f-4cfe-8e9d-b8554547ecfb": {
    "pk": "6a5f41dd-ab8f-4cfe-8e9d-b8554547ecfb",
    "title": "A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education",
    "abstract": "Natural Language Processing (NLP) plays a significant role in our daily lives and has become an essential part of Artificial Intelligence (AI) education in K-12. As children grow up with NLP-powered applications, it is crucial to introduce NLP concepts to them, fostering their understanding of language processing, language generation, and ethical implications of AI and NLP. This paper presents a comprehensive review of digital learning environments for teaching NLP in K-12. Specifically, it explores existing digital learning tools, discusses how they support specific NLP tasks and procedures, and investigates their explainability and evaluation results in educational contexts. By examining the strengths and limitations of these tools, this literature review sheds light on the current state of NLP learning tools in K-12 education. It aims to guide future research efforts to refine existing tools, develop new ones, and explore more effective and inclusive strategies for integrating NLP into K-12 educational contexts.",
    "authors": [
      "Xiaoyi Tian",
      "Kristy Elizabeth Boyer"
    ],
    "url": "http://arxiv.org/abs/2310.01603v1",
    "timestamp": 1696276470,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "77c55025-f240-42bf-b329-270176209782": {
    "pk": "77c55025-f240-42bf-b329-270176209782",
    "title": "Multilingual Natural Language Processing Model for Radiology Reports -- The Summary is all you need!",
    "abstract": "The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality matched or exceeded the corresponding human-written summaries, suggesting substantial clinical reliability. Furthermore, this study showed that the multilingual model outperformed other models that specialized in summarizing radiology reports in only one language, as well as models that were not specifically designed for summarizing radiology reports, such as ChatGPT.",
    "authors": [
      "Mariana Lindo",
      "Ana Sofia Santos",
      "Andr\u00e9 Ferreira",
      "Jianning Li",
      "Gijs Luijten",
      "Gustavo Correia",
      "Moon Kim",
      "Benedikt Michael Schaarschmidt",
      "Cornelius Deuschl",
      "Johannes Haubold",
      "Jens Kleesiek",
      "Jan Egger",
      "Victor Alves"
    ],
    "url": "http://arxiv.org/abs/2310.00100v4",
    "timestamp": 1696015227,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2de8c684-5baa-4112-8bcc-b7b8ecdb0640": {
    "pk": "2de8c684-5baa-4112-8bcc-b7b8ecdb0640",
    "title": "DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task",
    "abstract": "This paper presents an approach for adapting the DebertaV3 XSmall model pre-trained in English for Brazilian Portuguese natural language processing (NLP) tasks. A key aspect of the methodology involves a multistep training process to ensure the model is effectively tuned for the Portuguese language. Initial datasets from Carolina and BrWac are preprocessed to address issues like emojis, HTML tags, and encodings. A Portuguese-specific vocabulary of 50,000 tokens is created using SentencePiece. Rather than training from scratch, the weights of the pre-trained English model are used to initialize most of the network, with random embeddings, recognizing the expensive cost of training from scratch. The model is fine-tuned using the replaced token detection task in the same format of DebertaV3 training. The adapted model, called DeBERTinha, demonstrates effectiveness on downstream tasks like named entity recognition, sentiment analysis, and determining sentence relatedness, outperforming BERTimbau-Large in two tasks despite having only 40M parameters.",
    "authors": [
      "Israel Campiotti",
      "Matheus Rodrigues",
      "Yuri Albuquerque",
      "Rafael Azevedo",
      "Alyson Andrade"
    ],
    "url": "http://arxiv.org/abs/2309.16844v2",
    "timestamp": 1695934405,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8b2c11c0-42e2-4d6f-8af1-9cd4649f99b2": {
    "pk": "8b2c11c0-42e2-4d6f-8af1-9cd4649f99b2",
    "title": "Natural Language Processing for Requirements Formalization: How to Derive New Approaches?",
    "abstract": "It is a long-standing desire of industry and research to automate the software development and testing process as much as possible. In this process, requirements engineering (RE) plays a fundamental role for all other steps that build on it. Model-based design and testing methods have been developed to handle the growing complexity and variability of software systems. However, major effort is still required to create specification models from a large set of functional requirements provided in natural language. Numerous approaches based on natural language processing (NLP) have been proposed in the literature to generate requirements models using mainly syntactic properties. Recent advances in NLP show that semantic quantities can also be identified and used to provide better assistance in the requirements formalization process. In this work, we present and discuss principal ideas and state-of-the-art methodologies from the field of NLP in order to guide the readers on how to create a set of rules and methods for the semi-automated formalization of requirements according to their specific use case and needs. We discuss two different approaches in detail and highlight the iterative development of rule sets. The requirements models are represented in a human- and machine-readable format in the form of pseudocode. The presented methods are demonstrated on two industrial use cases from the automotive and railway domains. It shows that using current pre-trained NLP models requires less effort to create a set of rules and can be easily adapted to specific use cases and domains. In addition, findings and shortcomings of this research area are highlighted and an outlook on possible future developments is given.",
    "authors": [
      "Viju Sudhi",
      "Libin Kutty",
      "Robin Gr\u00f6pler"
    ],
    "url": "http://arxiv.org/abs/2309.13272v1",
    "timestamp": 1695447919,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ba988b5d-764b-44cc-a09e-ba338f2e87df": {
    "pk": "ba988b5d-764b-44cc-a09e-ba338f2e87df",
    "title": "Classifying Organizations for Food System Ontologies using Natural Language Processing",
    "abstract": "Our research explores the use of natural language processing (NLP) methods to automatically classify entities for the purpose of knowledge graph population and integration with food system ontologies. We have created NLP models that can automatically classify organizations with respect to categories associated with environmental issues as well as Standard Industrial Classification (SIC) codes, which are used by the U.S. government to characterize business activities. As input, the NLP models are provided with text snippets retrieved by the Google search engine for each organization, which serves as a textual description of the organization that is used for learning. Our experimental results show that NLP models can achieve reasonably good performance for these two classification tasks, and they rely on a general framework that could be applied to many other classification problems as well. We believe that NLP models represent a promising approach for automatically harvesting information to populate knowledge graphs and aligning the information with existing ontologies through shared categories and concepts.",
    "authors": [
      "Tianyu Jiang",
      "Sonia Vinogradova",
      "Nathan Stringham",
      "E. Louise Earl",
      "Allan D. Hollander",
      "Patrick R. Huber",
      "Ellen Riloff",
      "R. Sandra Schillo",
      "Giorgio A. Ubbiali",
      "Matthew Lange"
    ],
    "url": "http://arxiv.org/abs/2309.10880v1",
    "timestamp": 1695150468,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a2d5afc0-337a-4138-8f19-3da5a2b7fd3c": {
    "pk": "a2d5afc0-337a-4138-8f19-3da5a2b7fd3c",
    "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
    "authors": [
      "Sonish Sivarajkumar",
      "Mark Kelley",
      "Alyssa Samolyk-Mazzanti",
      "Shyam Visweswaran",
      "Yanshan Wang"
    ],
    "url": "http://arxiv.org/abs/2309.08008v1",
    "timestamp": 1694720100,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "caa226e3-c8b6-43a5-8dee-a62c5c76c305": {
    "pk": "caa226e3-c8b6-43a5-8dee-a62c5c76c305",
    "title": "Electricity Demand Forecasting through Natural Language Processing with Long Short-Term Memory Networks",
    "abstract": "Electricity demand forecasting is a well established research field. Usually this task is performed considering historical loads, weather forecasts, calendar information and known major events. Recently attention has been given on the possible use of new sources of information from textual news in order to improve the performance of these predictions. This paper proposes a Long and Short-Term Memory (LSTM) network incorporating textual news features that successfully predicts the deterministic and probabilistic tasks of the UK national electricity demand. The study finds that public sentiment and word vector representations related to transport and geopolitics have time-continuity effects on electricity demand. The experimental results show that the LSTM with textual features improves by more than 3% compared to the pure LSTM benchmark and by close to 10% over the official benchmark. Furthermore, the proposed model effectively reduces forecasting uncertainty by narrowing the confidence interval and bringing the forecast distribution closer to the truth.",
    "authors": [
      "Yun Bai",
      "Simon Camal",
      "Andrea Michiorri"
    ],
    "url": "http://arxiv.org/abs/2309.06793v1",
    "timestamp": 1694593696,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3b6a5dc5-28c1-478e-bf6a-f7dc6cfa7bdf": {
    "pk": "3b6a5dc5-28c1-478e-bf6a-f7dc6cfa7bdf",
    "title": "Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review",
    "abstract": "Applicating third-party data and models has become a new paradigm for language modeling in NLP, which also introduces some potential security vulnerabilities because attackers can manipulate the training process and data source. In this case, backdoor attacks can induce the model to exhibit expected behaviors through specific triggers and have little inferior influence on primitive tasks. Hence, it could have dire consequences, especially considering that the backdoor attack surfaces are broad.   However, there is still no systematic and comprehensive review to reflect the security challenges, attacker's capabilities, and purposes according to the attack surface. Moreover, there is a shortage of analysis and comparison of the diverse emerging backdoor countermeasures in this context. In this paper, we conduct a timely review of backdoor attacks and countermeasures to sound the red alarm for the NLP security community. According to the affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into three categorizations: attacking pre-trained model with fine-tuning (APMF) or parameter-efficient tuning (APMP), and attacking final model with training (AFMT). Thus, attacks under each categorization are combed. The countermeasures are categorized into two general classes: sample inspection and model inspection. Overall, the research on the defense side is far behind the attack side, and there is no single defense that can prevent all types of backdoor attacks. An attacker can intelligently bypass existing defenses with a more invisible attack. Drawing the insights from the systematic review, we also present crucial areas for future research on the backdoor, such as empirical security evaluations on large language models, and in particular, more efficient and practical countermeasures are solicited.",
    "authors": [
      "Pengzhou Cheng",
      "Zongru Wu",
      "Wei Du",
      "Haodong Zhao",
      "Wei Lu",
      "Gongshen Liu"
    ],
    "url": "http://arxiv.org/abs/2309.06055v4",
    "timestamp": 1694508518,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "85e9262b-5bbd-4f48-9bdc-286c000ac994": {
    "pk": "85e9262b-5bbd-4f48-9bdc-286c000ac994",
    "title": "Machine Learning for Tangible Effects: Natural Language Processing for Uncovering the Illicit Massage Industry & Computer Vision for Tactile Sensing",
    "abstract": "I explore two questions in this thesis: how can computer science be used to fight human trafficking? And how can computer vision create a sense of touch?   I use natural language processing (NLP) to monitor the United States illicit massage industry (IMI), a multi-billion dollar industry that offers not just therapeutic massages but also commercial sexual services. Employees of this industry are often immigrant women with few job opportunities, leaving them vulnerable to fraud, coercion, and other facets of human trafficking. Monitoring spatiotemporal trends helps prevent trafficking in the IMI. By creating datasets with three publicly-accessible websites: Google Places, Rubmaps, and AMPReviews, combined with NLP techniques such as bag-of-words and Word2Vec, I show how to derive insights into the labor pressures and language barriers that employees face, as well as the income, demographics, and societal pressures affecting sex buyers. I include a call-to-action to other researchers given these datasets. I also consider how to creating synthetic financial data, which can aid with counter-trafficking in the banking sector. I use an agent-based model to create both tabular and payee-recipient graph data.   I then consider the role of computer vision in making tactile sensors. I report on a novel sensor, the Digger Finger, that adapts the Gelsight sensor to finding objects in granular media. Changes include using a wedge shape to facilitate digging, replacing the internal lighting LEDs with fluorescent paint, and adding a vibrator motor to counteract jamming. Finally, I also show how to use a webcam and a printed reference marker, or fiducial, to create a low-cost six-axis force-torque sensor. This sensor is up to a hundred times less expensive than commercial sensors, allowing for a wider range of applications. For this and earlier chapters I release design files and code as open source.",
    "authors": [
      "Rui Ouyang"
    ],
    "url": "http://arxiv.org/abs/2309.03470v1",
    "timestamp": 1694059441,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  }
}