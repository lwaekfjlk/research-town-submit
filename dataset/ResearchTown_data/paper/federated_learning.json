{
  "7b967c26-cccc-4ee5-9679-584a03ae9e5b": {
    "pk": "7b967c26-cccc-4ee5-9679-584a03ae9e5b",
    "title": "Decentralized Personalized Federated Learning",
    "abstract": "This work tackles the challenges of data heterogeneity and communication limitations in decentralized federated learning. We focus on creating a collaboration graph that guides each client in selecting suitable collaborators for training personalized models that leverage their local data effectively. Our approach addresses these issues through a novel, communication-efficient strategy that enhances resource efficiency. Unlike traditional methods, our formulation identifies collaborators at a granular level by considering combinatorial relations of clients, enhancing personalization while minimizing communication overhead. We achieve this through a bi-level optimization framework that employs a constrained greedy algorithm, resulting in a resource-efficient collaboration graph for personalized learning. Extensive evaluation against various baselines across diverse datasets demonstrates the superiority of our method, named DPFL. DPFL consistently outperforms other approaches, showcasing its effectiveness in handling real-world data heterogeneity, minimizing communication overhead, enhancing resource efficiency, and building personalized models in decentralized federated learning scenarios.",
    "authors": [
      "Salma Kharrat",
      "Marco Canini",
      "Samuel Horvath"
    ],
    "url": "http://arxiv.org/abs/2406.06520v1",
    "timestamp": 1718042328,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f9a11ae9-dfb2-47d0-935a-db5f14ba0efe": {
    "pk": "f9a11ae9-dfb2-47d0-935a-db5f14ba0efe",
    "title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
    "abstract": "Copious amounts of relevance judgments are necessary for the effective training and accurate evaluation of retrieval systems. Conventionally, these judgments are made by human assessors, rendering this process expensive and laborious. A recent study by Thomas et al. from Microsoft Bing suggested that large language models (LLMs) can accurately perform the relevance assessment task and provide human-quality judgments, but unfortunately their study did not yield any reusable software artifacts. Our work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper. Across Deep Learning Tracks from TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate highly with rankings generated by effective multi-stage retrieval systems. Our toolkit is designed to be easily extensible and can be integrated into existing multi-stage retrieval and evaluation pipelines, offering researchers a valuable resource for studying retrieval evaluation methodologies. UMBRELA will be used in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our toolkit becoming a foundation for further innovation in the field. UMBRELA is available at https://github.com/castorini/umbrela.",
    "authors": [
      "Shivani Upadhyay",
      "Ronak Pradeep",
      "Nandan Thakur",
      "Nick Craswell",
      "Jimmy Lin"
    ],
    "url": "http://arxiv.org/abs/2406.06519v1",
    "timestamp": 1718042309,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "43118eb4-15f5-47f0-8da8-ca5ad6e996f9": {
    "pk": "43118eb4-15f5-47f0-8da8-ca5ad6e996f9",
    "title": "Data Augmentation for Multivariate Time Series Classification: An Experimental Study",
    "abstract": "Our study investigates the impact of data augmentation on the performance of multivariate time series models, focusing on datasets from the UCR archive. Despite the limited size of these datasets, we achieved classification accuracy improvements in 10 out of 13 datasets using the Rocket and InceptionTime models. This highlights the essential role of sufficient data in training effective models, paralleling the advancements seen in computer vision. Our work delves into adapting and applying existing methods in innovative ways to the domain of multivariate time series classification. Our comprehensive exploration of these techniques sets a new standard for addressing data scarcity in time series analysis, emphasizing that diverse augmentation strategies are crucial for unlocking the potential of both traditional and deep learning models. Moreover, by meticulously analyzing and applying a variety of augmentation techniques, we demonstrate that strategic data enrichment can enhance model accuracy. This not only establishes a benchmark for future research in time series analysis but also underscores the importance of adopting varied augmentation approaches to improve model performance in the face of limited data availability.",
    "authors": [
      "Romain Ilbert",
      "Thai V. Hoang",
      "Zonghua Zhang"
    ],
    "url": "http://arxiv.org/abs/2406.06518v1",
    "timestamp": 1718042282,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f3de70e4-841a-4c8b-bf91-c519fc33e7c7": {
    "pk": "f3de70e4-841a-4c8b-bf91-c519fc33e7c7",
    "title": "Genomics-guided Representation Learning for Pathologic Pan-cancer Tumor Microenvironment Subtype Prediction",
    "abstract": "The characterization of Tumor MicroEnvironment (TME) is challenging due to its complexity and heterogeneity. Relatively consistent TME characteristics embedded within highly specific tissue features, render them difficult to predict. The capability to accurately classify TME subtypes is of critical significance for clinical tumor diagnosis and precision medicine. Based on the observation that tumors with different origins share similar microenvironment patterns, we propose PathoTME, a genomics-guided Siamese representation learning framework employing Whole Slide Image (WSI) for pan-cancer TME subtypes prediction. Specifically, we utilize Siamese network to leverage genomic information as a regularization factor to assist WSI embeddings learning during the training phase. Additionally, we employ Domain Adversarial Neural Network (DANN) to mitigate the impact of tissue type variations. To eliminate domain bias, a dynamic WSI prompt is designed to further unleash the model's capabilities. Our model achieves better performance than other state-of-the-art methods across 23 cancer types on TCGA dataset. Our code is available at https://github.com/Mengflz/PathoTME.",
    "authors": [
      "Fangliangzi Meng",
      "Hongrun Zhang",
      "Ruodan Yan",
      "Guohui Chuai",
      "Chao Li",
      "Qi Liu"
    ],
    "url": "http://arxiv.org/abs/2406.06517v1",
    "timestamp": 1718042181,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f9495125-d163-49c4-925d-303c11ec9195": {
    "pk": "f9495125-d163-49c4-925d-303c11ec9195",
    "title": "Distribution-Free Predictive Inference under Unknown Temporal Drift",
    "abstract": "Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex statistical models. Their validity hinges on reliable calibration data, which may not be readily available as real-world environments often undergo unknown changes over time. In this paper, we propose a strategy for choosing an adaptive window and use the data therein to construct prediction sets. The window is selected by optimizing an estimated bias-variance tradeoff. We provide sharp coverage guarantees for our method, showing its adaptivity to the underlying temporal drift. We also illustrate its efficacy through numerical experiments on synthetic and real data.",
    "authors": [
      "Elise Han",
      "Chengpiao Huang",
      "Kaizheng Wang"
    ],
    "url": "http://arxiv.org/abs/2406.06516v1",
    "timestamp": 1718042143,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ME",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5bcfa452-f7f7-4c25-b882-df954e9f64b4": {
    "pk": "5bcfa452-f7f7-4c25-b882-df954e9f64b4",
    "title": "Random Features Approximation for Control-Affine Systems",
    "abstract": "Modern data-driven control applications call for flexible nonlinear models that are amenable to principled controller synthesis and realtime feedback. Many nonlinear dynamical systems of interest are control affine. We propose two novel classes of nonlinear feature representations which capture control affine structure while allowing for arbitrary complexity in the state dependence. Our methods make use of random features (RF) approximations, inheriting the expressiveness of kernel methods at a lower computational cost. We formalize the representational capabilities of our methods by showing their relationship to the Affine Dot Product (ADP) kernel proposed by Casta\\~neda et al. (2021) and a novel Affine Dense (AD) kernel that we introduce. We further illustrate the utility by presenting a case study of data-driven optimization-based control using control certificate functions (CCF). Simulation experiments on a double pendulum empirically demonstrate the advantages of our methods.",
    "authors": [
      "Kimia Kazemian",
      "Yahya Sattar",
      "Sarah Dean"
    ],
    "url": "http://arxiv.org/abs/2406.06514v1",
    "timestamp": 1718042097,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d3d38dbd-4fad-447f-9be1-feab0880b7f1": {
    "pk": "d3d38dbd-4fad-447f-9be1-feab0880b7f1",
    "title": "Robust Distribution Learning with Local and Global Adversarial Corruptions",
    "abstract": "We consider learning in an adversarial environment, where an $\\varepsilon$-fraction of samples from a distribution $P$ are arbitrarily modified (*global* corruptions) and the remaining perturbations have average magnitude bounded by $\\rho$ (*local* corruptions). Given access to $n$ such corrupted samples, we seek a computationally efficient estimator $\\hat{P}_n$ that minimizes the Wasserstein distance $\\mathsf{W}_1(\\hat{P}_n,P)$. In fact, we attack the fine-grained task of minimizing $\\mathsf{W}_1(\\Pi_\\# \\hat{P}_n, \\Pi_\\# P)$ for all orthogonal projections $\\Pi \\in \\mathbb{R}^{d \\times d}$, with performance scaling with $\\mathrm{rank}(\\Pi) = k$. This allows us to account simultaneously for mean estimation ($k=1$), distribution estimation ($k=d$), as well as the settings interpolating between these two extremes. We characterize the optimal population-limit risk for this task and then develop an efficient finite-sample algorithm with error bounded by $\\sqrt{\\varepsilon k} + \\rho + d^{O(1)}\\tilde{O}(n^{-1/k})$ when $P$ has bounded moments of order $2+\\delta$, for constant $\\delta > 0$. For data distributions with bounded covariance, our finite-sample bounds match the minimax population-level optimum for large sample sizes. Our efficient procedure relies on a novel trace norm approximation of an ideal yet intractable 2-Wasserstein projection estimator. We apply this algorithm to robust stochastic optimization, and, in the process, uncover a new method for overcoming the curse of dimensionality in Wasserstein distributionally robust optimization.",
    "authors": [
      "Sloan Nietert",
      "Ziv Goldfeld",
      "Soroosh Shafiee"
    ],
    "url": "http://arxiv.org/abs/2406.06509v1",
    "timestamp": 1718041716,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "16e92fd7-4407-49ac-a263-b7697c2ee121": {
    "pk": "16e92fd7-4407-49ac-a263-b7697c2ee121",
    "title": "Verification-Guided Shielding for Deep Reinforcement Learning",
    "abstract": "In recent years, Deep Reinforcement Learning (DRL) has emerged as an effective approach to solving real-world tasks. However, despite their successes, DRL-based policies suffer from poor reliability, which limits their deployment in safety-critical domains. As a result, various methods have been put forth to address this issue by providing formal safety guarantees. Two main approaches include shielding and verification. While shielding ensures the safe behavior of the policy by employing an external online component (i.e., a ``shield'') that overruns potentially dangerous actions, this approach has a significant computational cost as the shield must be invoked at runtime to validate every decision. On the other hand, verification is an offline process that can identify policies that are unsafe, prior to their deployment, yet, without providing alternative actions when such a policy is deemed unsafe. In this work, we present verification-guided shielding -- a novel approach that bridges the DRL reliability gap by integrating these two methods. Our approach combines both formal and probabilistic verification tools to partition the input domain into safe and unsafe regions. In addition, we employ clustering and symbolic representation procedures that compress the unsafe regions into a compact representation. This, in turn, allows to temporarily activate the shield solely in (potentially) unsafe regions, in an efficient manner. Our novel approach allows to significantly reduce runtime overhead while still preserving formal safety guarantees. We extensively evaluate our approach on two benchmarks from the robotic navigation domain, as well as provide an in-depth analysis of its scalability and completeness.",
    "authors": [
      "Davide Corsi",
      "Guy Amir",
      "Andoni Rodriguez",
      "Cesar Sanchez",
      "Guy Katz",
      "Roy Fox"
    ],
    "url": "http://arxiv.org/abs/2406.06507v1",
    "timestamp": 1718041499,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "be5d8061-b4a9-4e15-b0d4-43777c25afff": {
    "pk": "be5d8061-b4a9-4e15-b0d4-43777c25afff",
    "title": "Online Newton Method for Bandit Convex Optimisation",
    "abstract": "We introduce a computationally efficient algorithm for zeroth-order bandit convex optimisation and prove that in the adversarial setting its regret is at most $d^{3.5} \\sqrt{n} \\mathrm{polylog}(n, d)$ with high probability where $d$ is the dimension and $n$ is the time horizon. In the stochastic setting the bound improves to $M d^{2} \\sqrt{n} \\mathrm{polylog}(n, d)$ where $M \\in [d^{-1/2}, d^{-1 / 4}]$ is a constant that depends on the geometry of the constraint set and the desired computational properties.",
    "authors": [
      "Hidde Fokkema",
      "Dirk van der Hoeven",
      "Tor Lattimore",
      "Jack J. Mayo"
    ],
    "url": "http://arxiv.org/abs/2406.06506v1",
    "timestamp": 1718041451,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.OC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f45b49df-ad51-42cf-84b0-86bb58d1063a": {
    "pk": "f45b49df-ad51-42cf-84b0-86bb58d1063a",
    "title": "Equivariant Neural Tangent Kernels",
    "abstract": "Equivariant neural networks have in recent years become an important technique for guiding architecture selection for neural networks with many applications in domains ranging from medical image analysis to quantum chemistry. In particular, as the most general linear equivariant layers with respect to the regular representation, group convolutions have been highly impactful in numerous applications. Although equivariant architectures have been studied extensively, much less is known about the training dynamics of equivariant neural networks. Concurrently, neural tangent kernels (NTKs) have emerged as a powerful tool to analytically understand the training dynamics of wide neural networks. In this work, we combine these two fields for the first time by giving explicit expressions for NTKs of group convolutional neural networks. In numerical experiments, we demonstrate superior performance for equivariant NTKs over non-equivariant NTKs on a classification task for medical images.",
    "authors": [
      "Philipp Misof",
      "Pan Kessel",
      "Jan E. Gerken"
    ],
    "url": "http://arxiv.org/abs/2406.06504v1",
    "timestamp": 1718041393,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2bfc828f-b298-4e8b-b3de-029d1b9aa9fb": {
    "pk": "2bfc828f-b298-4e8b-b3de-029d1b9aa9fb",
    "title": "Adaptive Opponent Policy Detection in Multi-Agent MDPs: Real-Time Strategy Switch Identification Using Running Error Estimation",
    "abstract": "In Multi-agent Reinforcement Learning (MARL), accurately perceiving opponents' strategies is essential for both cooperative and adversarial contexts, particularly within dynamic environments. While Proximal Policy Optimization (PPO) and related algorithms such as Actor-Critic with Experience Replay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradient (DDPG) perform well in single-agent, stationary environments, they suffer from high variance in MARL due to non-stationary and hidden policies of opponents, leading to diminished reward performance. Additionally, existing methods in MARL face significant challenges, including the need for inter-agent communication, reliance on explicit reward information, high computational demands, and sampling inefficiencies. These issues render them less effective in continuous environments where opponents may abruptly change their policies without prior notice. Against this background, we present OPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that employs dynamic error decay to detect changes in opponents' policies. OPS-DeMo continuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank and selects corresponding responses from a pre-trained Response Policy Bank. Each response policy is trained against consistently strategizing opponents, reducing training uncertainty and enabling the effective use of algorithms like PPO in multi-agent environments. Comparative assessments show that our approach outperforms PPO-trained models in dynamic scenarios like the Predator-Prey setting, providing greater robustness to sudden policy shifts and enabling more informed decision-making through precise opponent policy insights.",
    "authors": [
      "Mohidul Haque Mridul",
      "Mohammad Foysal Khan",
      "Redwan Ahmed Rizvee",
      "Md Mosaddek Khan"
    ],
    "url": "http://arxiv.org/abs/2406.06500v1",
    "timestamp": 1718040884,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e3ae0a8a-ab95-4378-9a80-23a64475d408": {
    "pk": "e3ae0a8a-ab95-4378-9a80-23a64475d408",
    "title": "NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative",
    "abstract": "Existing video captioning benchmarks and models lack coherent representations of causal-temporal narrative, which is sequences of events linked through cause and effect, unfolding over time and driven by characters or agents. This lack of narrative restricts models' ability to generate text descriptions that capture the causal and temporal dynamics inherent in video content. To address this gap, we propose NarrativeBridge, an approach comprising of: (1) a novel Causal-Temporal Narrative (CTN) captions benchmark generated using a large language model and few-shot prompting, explicitly encoding cause-effect temporal relationships in video descriptions, evaluated automatically to ensure caption quality and relevance; and (2) a dedicated Cause-Effect Network (CEN) architecture with separate encoders for capturing cause and effect dynamics independently, enabling effective learning and generation of captions with causal-temporal narrative. Extensive experiments demonstrate that CEN is more accurate in articulating the causal and temporal aspects of video content than the second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT datasets, respectively. The proposed framework understands and generates nuanced text descriptions with intricate causal-temporal narrative structures present in videos, addressing a critical limitation in video captioning. For project details, visit https://narrativebridge.github.io/.",
    "authors": [
      "Asmar Nadeem",
      "Faegheh Sardari",
      "Robert Dawes",
      "Syed Sameed Husain",
      "Adrian Hilton",
      "Armin Mustafa"
    ],
    "url": "http://arxiv.org/abs/2406.06499v1",
    "timestamp": 1718040864,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5a0b207b-1f5a-4556-a479-592143b1f702": {
    "pk": "5a0b207b-1f5a-4556-a479-592143b1f702",
    "title": "Direct Preference Optimization for Suppressing Hallucinated Prior Exams in Radiology Report Generation",
    "abstract": "Recent advances in generative vision-language models (VLMs) have exciting potential implications for AI in radiology, yet VLMs are also known to produce hallucinations, nonsensical text, and other unwanted behaviors that can waste clinicians' time and cause patient harm. Drawing on recent work on direct preference optimization (DPO), we propose a simple method for modifying the behavior of pretrained VLMs performing radiology report generation by suppressing unwanted types of generations. We apply our method to the prevention of hallucinations of prior exams, addressing a long-established problem behavior in models performing chest X-ray report generation. Across our experiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining model performance on clinical accuracy metrics. Our work is, to the best of our knowledge, the first work to apply DPO to medical VLMs, providing a data- and compute- efficient way to suppress problem behaviors while maintaining overall clinical accuracy.",
    "authors": [
      "Oishi Banerjee",
      "Hong-Yu Zhou",
      "Subathra Adithan",
      "Stephen Kwak",
      "Kay Wu",
      "Pranav Rajpurkar"
    ],
    "url": "http://arxiv.org/abs/2406.06496v1",
    "timestamp": 1718040696,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3109fc4b-a74a-4ba6-a77d-1b84eb28fc34": {
    "pk": "3109fc4b-a74a-4ba6-a77d-1b84eb28fc34",
    "title": "Boosting Robustness in Preference-Based Reinforcement Learning with Dynamic Sparsity",
    "abstract": "For autonomous agents to successfully integrate into human-centered environments, agents should be able to learn from and adapt to humans in their native settings. Preference-based reinforcement learning (PbRL) is a promising approach that learns reward functions from human preferences. This enables RL agents to adapt their behavior based on human desires. However, humans live in a world full of diverse information, most of which is not relevant to completing a particular task. It becomes essential that agents learn to focus on the subset of task-relevant environment features. Unfortunately, prior work has largely ignored this aspect; primarily focusing on improving PbRL algorithms in standard RL environments that are carefully constructed to contain only task-relevant features. This can result in algorithms that may not effectively transfer to a more noisy real-world setting. To that end, this work proposes R2N (Robust-to-Noise), the first PbRL algorithm that leverages principles of dynamic sparse training to learn robust reward models that can focus on task-relevant features. We study the effectiveness of R2N in the Extremely Noisy Environment setting, an RL problem setting where up to 95% of the state features are irrelevant distractions. In experiments with a simulated teacher, we demonstrate that R2N can adapt the sparse connectivity of its neural networks to focus on task-relevant features, enabling R2N to significantly outperform several state-of-the-art PbRL algorithms in multiple locomotion and control environments.",
    "authors": [
      "Calarina Muslimani",
      "Bram Grooten",
      "Deepak Ranganatha Sastry Mamillapalli",
      "Mykola Pechenizkiy",
      "Decebal Constantin Mocanu",
      "Matthew E. Taylor"
    ],
    "url": "http://arxiv.org/abs/2406.06495v1",
    "timestamp": 1718040667,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cfa02f63-6593-4a3a-8c46-44c046ac3ae1": {
    "pk": "cfa02f63-6593-4a3a-8c46-44c046ac3ae1",
    "title": "Scaling Continuous Latent Variable Models as Probabilistic Integral Circuits",
    "abstract": "Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs). PICs are symbolic computational graphs defining continuous LV models as hierarchies of functions that are summed and multiplied together, or integrated over some LVs. They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.   So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale. In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training. In extensive experiments, we showcase the effectiveness of functional sharing and the superiority of QPCs over traditional PCs.",
    "authors": [
      "Gennaro Gala",
      "Cassio de Campos",
      "Antonio Vergari",
      "Erik Quaeghebeur"
    ],
    "url": "http://arxiv.org/abs/2406.06494v1",
    "timestamp": 1718040617,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "81ac1950-d874-40b3-81af-0df025c11ef6": {
    "pk": "81ac1950-d874-40b3-81af-0df025c11ef6",
    "title": "Probing out-of-distribution generalization in machine learning for materials",
    "abstract": "Scientific machine learning (ML) endeavors to develop generalizable models with broad applicability. However, the assessment of generalizability is often based on heuristics. Here, we demonstrate in the materials science setting that heuristics based evaluations lead to substantially biased conclusions of ML generalizability and benefits of neural scaling. We evaluate generalization performance in over 700 out-of-distribution tasks that features new chemistry or structural symmetry not present in the training data. Surprisingly, good performance is found in most tasks and across various ML models including simple boosted trees. Analysis of the materials representation space reveals that most tasks contain test data that lie in regions well covered by training data, while poorly-performing tasks contain mainly test data outside the training domain. For the latter case, increasing training set size or training time has marginal or even adverse effects on the generalization performance, contrary to what the neural scaling paradigm assumes. Our findings show that most heuristically-defined out-of-distribution tests are not genuinely difficult and evaluate only the ability to interpolate. Evaluating on such tasks rather than the truly challenging ones can lead to an overestimation of generalizability and benefits of scaling.",
    "authors": [
      "Kangming Li",
      "Andre Niyongabo Rubungo",
      "Xiangyun Lei",
      "Daniel Persaud",
      "Kamal Choudhary",
      "Brian DeCost",
      "Adji Bousso Dieng",
      "Jason Hattrick-Simpers"
    ],
    "url": "http://arxiv.org/abs/2406.06489v1",
    "timestamp": 1718040432,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cond-mat.mtrl-sci",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ff96487a-a4c2-468e-935a-09fa231e1e98": {
    "pk": "ff96487a-a4c2-468e-935a-09fa231e1e98",
    "title": "When is Multicalibration Post-Processing Necessary?",
    "abstract": "Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates. Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income). We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs. Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly. More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts.",
    "authors": [
      "Dutch Hansen",
      "Siddartha Devic",
      "Preetum Nakkiran",
      "Vatsal Sharan"
    ],
    "url": "http://arxiv.org/abs/2406.06487v1",
    "timestamp": 1718040399,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1bb8e851-4481-4910-9918-962529d53a71": {
    "pk": "1bb8e851-4481-4910-9918-962529d53a71",
    "title": "Continuum Attention for Neural Operators",
    "abstract": "Transformers, and the attention mechanism in particular, have become ubiquitous in machine learning. Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time-series problems. Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators. Motivated by this, we study transformers in the function space setting. We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator. The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces, for which we prove a universal approximation result. The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures. For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators. Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators.",
    "authors": [
      "Edoardo Calvello",
      "Nikola B. Kovachki",
      "Matthew E. Levine",
      "Andrew M. Stuart"
    ],
    "url": "http://arxiv.org/abs/2406.06486v1",
    "timestamp": 1718040346,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "44326232-b8ba-4b01-ace3-701ac9986270": {
    "pk": "44326232-b8ba-4b01-ace3-701ac9986270",
    "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length",
    "abstract": "Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive outer-product update in linear transformers with the delta rule have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks (including on tasks that focus on recall). We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrid models outperform strong transformer baselines.",
    "authors": [
      "Songlin Yang",
      "Bailin Wang",
      "Yu Zhang",
      "Yikang Shen",
      "Yoon Kim"
    ],
    "url": "http://arxiv.org/abs/2406.06484v1",
    "timestamp": 1718040282,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d5321766-e345-4b72-adfd-099d9f832b53": {
    "pk": "d5321766-e345-4b72-adfd-099d9f832b53",
    "title": "Quantum Equilibrium Propagation for efficient training of quantum systems based on Onsager reciprocity",
    "abstract": "The widespread adoption of machine learning and artificial intelligence in all branches of science and technology has created a need for energy-efficient, alternative hardware platforms. While such neuromorphic approaches have been proposed and realised for a wide range of platforms, physically extracting the gradients required for training remains challenging as generic approaches only exist in certain cases. Equilibrium propagation (EP) is such a procedure that has been introduced and applied to classical energy-based models which relax to an equilibrium. Here, we show a direct connection between EP and Onsager reciprocity and exploit this to derive a quantum version of EP. This can be used to optimize loss functions that depend on the expectation values of observables of an arbitrary quantum system. Specifically, we illustrate this new concept with supervised and unsupervised learning examples in which the input or the solvable task is of quantum mechanical nature, e.g., the recognition of quantum many-body ground states, quantum phase exploration, sensing and phase boundary exploration. We propose that in the future quantum EP may be used to solve tasks such as quantum phase discovery with a quantum simulator even for Hamiltonians which are numerically hard to simulate or even partially unknown. Our scheme is relevant for a variety of quantum simulation platforms such as ion chains, superconducting qubit arrays, neutral atom Rydberg tweezer arrays and strongly interacting atoms in optical lattices.",
    "authors": [
      "Clara C. Wanjura",
      "Florian Marquardt"
    ],
    "url": "http://arxiv.org/abs/2406.06482v1",
    "timestamp": 1718040129,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "058d8e4f-a71f-4aac-b2f4-5ca715cf43b5": {
    "pk": "058d8e4f-a71f-4aac-b2f4-5ca715cf43b5",
    "title": "Graph-Based Bidirectional Transformer Decision Threshold Adjustment Algorithm for Class-Imbalanced Molecular Data",
    "abstract": "Data sets with imbalanced class sizes, often where one class size is much smaller than that of others, occur extremely often in various applications, including those with biological foundations, such as drug discovery and disease diagnosis. Thus, it is extremely important to be able to identify data elements of classes of various sizes, as a failure to detect can result in heavy costs. However, many data classification algorithms do not perform well on imbalanced data sets as they often fail to detect elements belonging to underrepresented classes. In this paper, we propose the BTDT-MBO algorithm, incorporating Merriman-Bence-Osher (MBO) techniques and a bidirectional transformer, as well as distance correlation and decision threshold adjustments, for data classification problems on highly imbalanced molecular data sets, where the sizes of the classes vary greatly. The proposed method not only integrates adjustments in the classification threshold for the MBO algorithm in order to help deal with the class imbalance, but also uses a bidirectional transformer model based on an attention mechanism for self-supervised learning. Additionally, the method implements distance correlation as a weight function for the similarity graph-based framework on which the adjusted MBO algorithm operates. The proposed model is validated using six molecular data sets, and we also provide a thorough comparison to other competing algorithms. The computational experiments show that the proposed method performs better than competing techniques even when the class imbalance ratio is very high.",
    "authors": [
      "Nicole Hayes",
      "Ekaterina Merkurjev",
      "Guo-Wei Wei"
    ],
    "url": "http://arxiv.org/abs/2406.06479v1",
    "timestamp": 1718040013,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ab786581-f7f7-4fa1-bb14-a2bffda6977e": {
    "pk": "ab786581-f7f7-4fa1-bb14-a2bffda6977e",
    "title": "GKAN: Graph Kolmogorov-Arnold Networks",
    "abstract": "We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural network architecture that extends the principles of the recently proposed Kolmogorov-Arnold Networks (KAN) to graph-structured data. By adopting the unique characteristics of KANs, notably the use of learnable univariate functions instead of fixed linear weights, we develop a powerful model for graph-based learning tasks. Unlike traditional Graph Convolutional Networks (GCNs) that rely on a fixed convolutional architecture, GKANs implement learnable spline-based functions between layers, transforming the way information is processed across the graph structure. We present two different ways to incorporate KAN layers into GKAN: architecture 1 -- where the learnable functions are applied to input features after aggregation and architecture 2 -- where the learnable functions are applied to input features before aggregation. We evaluate GKAN empirically using a semi-supervised graph learning task on a real-world dataset (Cora). We find that architecture generally performs better. We find that GKANs achieve higher accuracy in semi-supervised learning tasks on graphs compared to the traditional GCN model. For example, when considering 100 features, GCN provides an accuracy of 53.5 while a GKAN with a comparable number of parameters gives an accuracy of 61.76; with 200 features, GCN provides an accuracy of 61.24 while a GKAN with a comparable number of parameters gives an accuracy of 67.66. We also present results on the impact of various parameters such as the number of hidden nodes, grid-size, and the polynomial-degree of the spline on the performance of GKAN.",
    "authors": [
      "Mehrdad Kiamari",
      "Mohammad Kiamari",
      "Bhaskar Krishnamachari"
    ],
    "url": "http://arxiv.org/abs/2406.06470v1",
    "timestamp": 1718039378,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3501bb64-6643-423c-9285-eedebaacce9a": {
    "pk": "3501bb64-6643-423c-9285-eedebaacce9a",
    "title": "Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning",
    "abstract": "Language agents perform complex tasks by using tools to execute each step precisely. However, most existing agents are based on proprietary models or designed to target specific tasks, such as mathematics or multi-hop question answering. We introduce Husky, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning. Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state. We identify a thorough ontology of actions for addressing complex tasks and curate high-quality data to train expert models for executing these actions. Our experiments show that Husky outperforms prior language agents across 14 evaluation datasets. Moreover, we introduce HuskyQA, a new evaluation set which stress tests language agents for mixed-tool reasoning, with a focus on retrieving missing knowledge and performing numerical reasoning. Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of our holistic approach in addressing complex reasoning problems. Our code and models are available at https://github.com/agent-husky/Husky-v1.",
    "authors": [
      "Joongwon Kim",
      "Bhargavi Paranjape",
      "Tushar Khot",
      "Hannaneh Hajishirzi"
    ],
    "url": "http://arxiv.org/abs/2406.06469v1",
    "timestamp": 1718039245,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "497d94c0-1062-40ab-a046-8adcc0ef5906": {
    "pk": "497d94c0-1062-40ab-a046-8adcc0ef5906",
    "title": "How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad",
    "abstract": "Can Transformers predict new syllogisms by composing established ones? More generally, what type of targets can be learned by such models from scratch? Recent works show that Transformers can be Turing-complete in terms of expressivity, but this does not address the learnability objective. This paper puts forward the notion of 'distribution locality' to capture when weak learning is efficiently achievable by regular Transformers, where the locality measures the least number of tokens required in addition to the tokens histogram to correlate nontrivially with the target. As shown experimentally and theoretically under additional assumptions, distributions with high locality cannot be learned efficiently. In particular, syllogisms cannot be composed on long chains. Furthermore, we show that (i) an agnostic scratchpad cannot help to break the locality barrier, (ii) an educated scratchpad can help if it breaks the locality at each step, (iii) a notion of 'inductive scratchpad' can both break the locality and improve the out-of-distribution generalization, e.g., generalizing to almost double input size for some arithmetic tasks.",
    "authors": [
      "Emmanuel Abbe",
      "Samy Bengio",
      "Aryo Lotfi",
      "Colin Sandon",
      "Omid Saremi"
    ],
    "url": "http://arxiv.org/abs/2406.06467v1",
    "timestamp": 1718039112,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d810ac05-7668-471d-87f3-730428a271fd": {
    "pk": "d810ac05-7668-471d-87f3-730428a271fd",
    "title": "AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction",
    "abstract": "Text-guided video prediction (TVP) involves predicting the motion of future frames from the initial frame according to an instruction, which has wide applications in virtual reality, robotics, and content creation. Previous TVP methods make significant breakthroughs by adapting Stable Diffusion for this task. However, they struggle with frame consistency and temporal stability primarily due to the limited scale of video datasets. We observe that pretrained Image2Video diffusion models possess good priors for video dynamics but they lack textual control. Hence, transferring Image2Video models to leverage their video dynamic priors while injecting instruction control to generate controllable videos is both a meaningful and challenging task. To achieve this, we introduce the Multi-Modal Large Language Model (MLLM) to predict future video states based on initial frames and text instructions. More specifically, we design a dual query transformer (DQFormer) architecture, which integrates the instructions and frames into the conditional embeddings for future frame prediction. Additionally, we develop Long-Short Term Temporal Adapters and Spatial Adapters that can quickly transfer general video diffusion models to specific scenarios with minimal training costs. Experimental results show that our method significantly outperforms state-of-the-art techniques on four datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and UCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and SSv2 respectively, demonstrating its effectiveness in various domains. More examples can be found at our website https://chenhsing.github.io/AID.",
    "authors": [
      "Zhen Xing",
      "Qi Dai",
      "Zejia Weng",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "url": "http://arxiv.org/abs/2406.06465v1",
    "timestamp": 1718038928,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b7e18b28-589a-4d54-8ce1-660fecd95873": {
    "pk": "b7e18b28-589a-4d54-8ce1-660fecd95873",
    "title": "VCR: Visual Caption Restoration",
    "abstract": "We introduce Visual Caption Restoration (VCR), a novel vision-language task that challenges models to accurately restore partially obscured texts using pixel-level hints within images. This task stems from the observation that text embedded in images is intrinsically different from common visual elements and natural language due to the need to align the modalities of vision, text, and text embedded in images. While numerous works have integrated text embedded in images into visual question-answering tasks, approaches to these tasks generally rely on optical character recognition or masked language modeling, thus reducing the task to mainly text-based processing. However, text-based processing becomes ineffective in VCR as accurate text restoration depends on the combined information from provided images, context, and subtle cues from the tiny exposed areas of masked texts. We develop a pipeline to generate synthetic images for the VCR task using image-caption pairs, with adjustable caption visibility to control the task difficulty. With this pipeline, we construct a dataset for VCR called VCR-Wiki using images with captions from Wikipedia, comprising 2.11M English and 346K Chinese entities in both easy and hard split variants. Our results reveal that current vision language models significantly lag behind human performance in the VCR task, and merely fine-tuning the models on our dataset does not lead to notable improvements. We release VCR-Wiki and the data construction code to facilitate future research.",
    "authors": [
      "Tianyu Zhang",
      "Suyuchen Wang",
      "Lu Li",
      "Ge Zhang",
      "Perouz Taslakian",
      "Sai Rajeswar",
      "Jie Fu",
      "Bang Liu",
      "Yoshua Bengio"
    ],
    "url": "http://arxiv.org/abs/2406.06462v1",
    "timestamp": 1718038728,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "615ba55b-8c80-4473-92b1-c0d7267c1910": {
    "pk": "615ba55b-8c80-4473-92b1-c0d7267c1910",
    "title": "Towards Real-World Efficiency: Domain Randomization in Reinforcement Learning for Pre-Capture of Free-Floating Moving Targets by Autonomous Robots",
    "abstract": "In this research, we introduce a deep reinforcement learning-based control approach to address the intricate challenge of the robotic pre-grasping phase under microgravity conditions. Leveraging reinforcement learning eliminates the necessity for manual feature design, therefore simplifying the problem and empowering the robot to learn pre-grasping policies through trial and error. Our methodology incorporates an off-policy reinforcement learning framework, employing the soft actor-critic technique to enable the gripper to proficiently approach a free-floating moving object, ensuring optimal pre-grasp success. For effective learning of the pre-grasping approach task, we developed a reward function that offers the agent clear and insightful feedback. Our case study examines a pre-grasping task where a Robotiq 3F gripper is required to navigate towards a free-floating moving target, pursue it, and subsequently position itself at the desired pre-grasp location. We assessed our approach through a series of experiments in both simulated and real-world environments. The source code, along with recordings of real-world robot grasping, is available at Fanuc_Robotiq_Grasp.",
    "authors": [
      "Bahador Beigomi",
      "Zheng H. Zhu"
    ],
    "url": "http://arxiv.org/abs/2406.06460v1",
    "timestamp": 1718038491,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "511b3c1b-2914-457d-bb3e-7e33c4c25969": {
    "pk": "511b3c1b-2914-457d-bb3e-7e33c4c25969",
    "title": "How Useful is Intermittent, Asynchronous Expert Feedback for Bayesian Optimization?",
    "abstract": "Bayesian optimization (BO) is an integral part of automated scientific discovery -- the so-called self-driving lab -- where human inputs are ideally minimal or at least non-blocking. However, scientists often have strong intuition, and thus human feedback is still useful. Nevertheless, prior works in enhancing BO with expert feedback, such as by incorporating it in an offline or online but blocking (arrives at each BO iteration) manner, are incompatible with the spirit of self-driving labs. In this work, we study whether a small amount of randomly arriving expert feedback that is being incorporated in a non-blocking manner can improve a BO campaign. To this end, we run an additional, independent computing thread on top of the BO loop to handle the feedback-gathering process. The gathered feedback is used to learn a Bayesian preference model that can readily be incorporated into the BO thread, to steer its exploration-exploitation process. Experiments on toy and chemistry datasets suggest that even just a few intermittent, asynchronous expert feedback can be useful for improving or constraining BO. This can especially be useful for its implication in improving self-driving labs, e.g. making them more data-efficient and less costly.",
    "authors": [
      "Agustinus Kristiadi",
      "Felix Strieth-Kalthoff",
      "Sriram Ganapathi Subramanian",
      "Vincent Fortuin",
      "Pascal Poupart",
      "Geoff Pleiss"
    ],
    "url": "http://arxiv.org/abs/2406.06459v1",
    "timestamp": 1718038438,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ce52771b-1f87-478d-ba8b-db832ab0fa96": {
    "pk": "ce52771b-1f87-478d-ba8b-db832ab0fa96",
    "title": "Time Series Analysis: yesterday, today, tomorrow",
    "abstract": "Forecasts of various processes have always been a sophisticated problem for statistics and data science. Over the past decades the solution procedures were updated by deep learning and kernel methods. According to many specialists, these approaches are much more precise, stable, and suitable compared to the classical statistical linear time series methods. Here we investigate how true this point of view is.",
    "authors": [
      "Igor Mackarov"
    ],
    "url": "http://arxiv.org/abs/2406.06453v1",
    "timestamp": 1718037764,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "725a2b8c-1e49-4f6a-9672-ffcbeea6802d": {
    "pk": "725a2b8c-1e49-4f6a-9672-ffcbeea6802d",
    "title": "Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data",
    "abstract": "Accurately predicting conditional average treatment effects (CATEs) is crucial in personalized medicine and digital platform analytics. Since often the treatments of interest cannot be directly randomized, observational data is leveraged to learn CATEs, but this approach can incur significant bias from unobserved confounding. One strategy to overcome these limitations is to seek latent quasi-experiments in instrumental variables (IVs) for the treatment, for example, a randomized intent to treat or a randomized product recommendation. This approach, on the other hand, can suffer from low compliance, i.e., IV weakness. Some subgroups may even exhibit zero compliance meaning we cannot instrument for their CATEs at all. In this paper we develop a novel approach to combine IV and observational data to enable reliable CATE estimation in the presence of unobserved confounding in the observational data and low compliance in the IV data, including no compliance for some subgroups. We propose a two-stage framework that first learns biased CATEs from the observational data, and then applies a compliance-weighted correction using IV data, effectively leveraging IV strength variability across covariates. We characterize the convergence rates of our method and validate its effectiveness through a simulation study. Additionally, we demonstrate its utility with real data by analyzing the heterogeneous effects of 401(k) plan participation on wealth.",
    "authors": [
      "Miruna Oprescu",
      "Nathan Kallus"
    ],
    "url": "http://arxiv.org/abs/2406.06452v1",
    "timestamp": 1718037655,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ME",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6d921cb4-0fd5-4c28-b73a-1f9e0b0b3f7b": {
    "pk": "6d921cb4-0fd5-4c28-b73a-1f9e0b0b3f7b",
    "title": "Cometh: A continuous-time discrete-state graph diffusion model",
    "abstract": "Discrete-state denoising diffusion models led to state-of-the-art performance in graph generation, especially in the molecular domain. Recently, they have been transposed to continuous time, allowing more flexibility in the reverse process and a better trade-off between sampling efficiency and quality. Here, to leverage the benefits of both approaches, we propose Cometh, a continuous-time discrete-state graph diffusion model, integrating graph data into a continuous-time diffusion model framework. Empirically, we show that integrating continuous time leads to significant improvements across various metrics over state-of-the-art discrete-state diffusion models on a large set of molecular and non-molecular benchmark datasets.",
    "authors": [
      "Antoine Siraudin",
      "Fragkiskos D. Malliaros",
      "Christopher Morris"
    ],
    "url": "http://arxiv.org/abs/2406.06449v1",
    "timestamp": 1718037579,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f35d8edb-2338-4e6a-a05d-fb5163f20aa0": {
    "pk": "f35d8edb-2338-4e6a-a05d-fb5163f20aa0",
    "title": "How is the Pilot Doing: VTOL Pilot Workload Estimation by Multimodal Machine Learning on Psycho-physiological Signals",
    "abstract": "Vertical take-off and landing (VTOL) aircraft do not require a prolonged runway, thus allowing them to land almost anywhere. In recent years, their flexibility has made them popular in development, research, and operation. When compared to traditional fixed-wing aircraft and rotorcraft, VTOLs bring unique challenges as they combine many maneuvers from both types of aircraft. Pilot workload is a critical factor for safe and efficient operation of VTOLs. In this work, we conduct a user study to collect multimodal data from 28 pilots while they perform a variety of VTOL flight tasks. We analyze and interpolate behavioral patterns related to their performance and perceived workload. Finally, we build machine learning models to estimate their workload from the collected data. Our results are promising, suggesting that quantitative and accurate VTOL pilot workload monitoring is viable. Such assistive tools would help the research field understand VTOL operations and serve as a stepping stone for the industry to ensure VTOL safe operations and further remote operations.",
    "authors": [
      "Jong Hoon Park",
      "Lawrence Chen",
      "Ian Higgins",
      "Zhaobo Zheng",
      "Shashank Mehrotra",
      "Kevin Salubre",
      "Mohammadreza Mousaei",
      "Steven Willits",
      "Blain Levedahl",
      "Timothy Buker",
      "Eliot Xing",
      "Teruhisa Misu",
      "Sebastian Scherer",
      "Jean Oh"
    ],
    "url": "http://arxiv.org/abs/2406.06448v1",
    "timestamp": 1718037561,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.HC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "58af7d52-1c9a-4807-8fcf-4364cc0a4e70": {
    "pk": "58af7d52-1c9a-4807-8fcf-4364cc0a4e70",
    "title": "Deep Generative Modeling Reshapes Compression and Transmission: From Efficiency to Resiliency",
    "abstract": "Information theory and machine learning are inextricably linked and have even been referred to as \"two sides of the same coin\". One particularly elegant connection is the essential equivalence between probabilistic generative modeling and data compression or transmission. In this article, we reveal the dual-functionality of deep generative models that reshapes both data compression for efficiency and transmission error concealment for resiliency. We present how the contextual predictive capabilities of powerful generative models can be well positioned to be strong compressors and estimators. In this sense, we advocate for viewing the deep generative modeling problem through the lens of end-to-end communications, and evaluate the compression and error restoration capabilities of foundation generative models. We show that the kernel of many large generative models is powerful predictor that can capture complex relationships among semantic latent variables, and the communication viewpoints provide novel insights into semantic feature tokenization, contextual learning, and usage of deep generative models. In summary, our article highlights the essential connections of generative AI to source and channel coding techniques, and motivates researchers to make further explorations in this emerging topic.",
    "authors": [
      "Jincheng Dai",
      "Xiaoqi Qin",
      "Sixian Wang",
      "Lexi Xu",
      "Kai Niu",
      "Ping Zhang"
    ],
    "url": "http://arxiv.org/abs/2406.06446v1",
    "timestamp": 1718037362,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IT",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f0831857-b02f-4ef1-8138-008677310109": {
    "pk": "f0831857-b02f-4ef1-8138-008677310109",
    "title": "LLM Dataset Inference: Did you train on my dataset?",
    "abstract": "The proliferation of large language models (LLMs) in the real world has come with a rise in copyright cases against companies for training their models on unlicensed data from the internet. Recent works have presented methods to identify if individual text sequences were members of the model's training data, known as membership inference attacks (MIAs). We demonstrate that the apparent success of these MIAs is confounded by selecting non-members (text sequences not used for training) belonging to a different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared with ones used to train the model). This distribution shift makes membership inference appear successful. However, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time). Even when MIAs work, we find that different MIAs succeed at inferring membership of samples from different distributions. Instead, we propose a new dataset inference method to accurately identify the datasets used to train large language models. This paradigm sits realistically in the modern-day copyright landscape, where authors claim that an LLM is trained over multiple documents (such as a book) written by them, rather than one particular paragraph. While dataset inference shares many of the challenges of membership inference, we solve it by selectively combining the MIAs that provide positive signal for a given distribution, and aggregating them to perform a statistical test on a given dataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile with statistically significant p-values < 0.1, without any false positives.",
    "authors": [
      "Pratyush Maini",
      "Hengrui Jia",
      "Nicolas Papernot",
      "Adam Dziedzic"
    ],
    "url": "http://arxiv.org/abs/2406.06443v1",
    "timestamp": 1718037283,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5aef8615-6766-4457-83f5-750ed522a3fc": {
    "pk": "5aef8615-6766-4457-83f5-750ed522a3fc",
    "title": "Interpretability of Language Models via Task Spaces",
    "abstract": "The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs.",
    "authors": [
      "Lucas Weber",
      "Jaap Jumelet",
      "Elia Bruni",
      "Dieuwke Hupkes"
    ],
    "url": "http://arxiv.org/abs/2406.06441v1",
    "timestamp": 1718037270,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b5de8858-cfa6-426a-a81e-ca63ceed86a6": {
    "pk": "b5de8858-cfa6-426a-a81e-ca63ceed86a6",
    "title": "Multimodal Contextualized Semantic Parsing from Speech",
    "abstract": "We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents' contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent's knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available.",
    "authors": [
      "Jordan Voas",
      "Raymond Mooney",
      "David Harwath"
    ],
    "url": "http://arxiv.org/abs/2406.06438v1",
    "timestamp": 1718037094,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "85f74682-e1a5-4813-bfdf-a30ae76689c8": {
    "pk": "85f74682-e1a5-4813-bfdf-a30ae76689c8",
    "title": "Spatiotemporal Graph Neural Network Modelling Perfusion MRI",
    "abstract": "Perfusion MRI (pMRI) offers valuable insights into tumor vascularity and promises to predict tumor genotypes, thus benefiting prognosis for glioma patients, yet effective models tailored to 4D pMRI are still lacking. This study presents the first attempt to model 4D pMRI using a GNN-based spatiotemporal model PerfGAT, integrating spatial information and temporal kinetics to predict Isocitrate DeHydrogenase (IDH) mutation status in glioma patients. Specifically, we propose a graph structure learning approach based on edge attention and negative graphs to optimize temporal correlations modeling. Moreover, we design a dual-attention feature fusion module to integrate spatiotemporal features while addressing tumor-related brain regions. Further, we develop a class-balanced augmentation methods tailored to spatiotemporal data, which could mitigate the common label imbalance issue in clinical datasets. Our experimental results demonstrate that the proposed method outperforms other state-of-the-art approaches, promising to model pMRI effectively for patient characterization.",
    "authors": [
      "Ruodan Yan",
      "Carola-Bibiane Sch\u00f6nlieb",
      "Chao Li"
    ],
    "url": "http://arxiv.org/abs/2406.06434v1",
    "timestamp": 1718036686,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "810e76a7-1e08-4f85-ba43-e48ac55274d3": {
    "pk": "810e76a7-1e08-4f85-ba43-e48ac55274d3",
    "title": "DISCO: An End-to-End Bandit Framework for Personalised Discount Allocation",
    "abstract": "Personalised discount codes provide a powerful mechanism for managing customer relationships and operational spend in e-commerce. Bandits are well suited for this product area, given the partial information nature of the problem, as well as the need for adaptation to the changing business environment. Here, we introduce DISCO, an end-to-end contextual bandit framework for personalised discount code allocation at ASOS.com. DISCO adapts the traditional Thompson Sampling algorithm by integrating it within an integer program, thereby allowing for operational cost control. Because bandit learning is often worse with high dimensional actions, we focused on building low dimensional action and context representations that were nonetheless capable of good accuracy. Additionally, we sought to build a model that preserved the relationship between price and sales, in which customers increasing their purchasing in response to lower prices (\"negative price elasticity\"). These aims were achieved by using radial basis functions to represent the continuous (i.e. infinite armed) action space, in combination with context embeddings extracted from a neural network. These feature representations were used within a Thompson Sampling framework to facilitate exploration, and further integrated with an integer program to allocate discount codes across ASOS's customer base. These modelling decisions result in a reward model that (a) enables pooled learning across similar actions, (b) is highly accurate, including in extrapolation, and (c) preserves the expected negative price elasticity. Through offline analysis, we show that DISCO is able to effectively enact exploration and improves its performance over time, despite the global constraint. Finally, we subjected DISCO to a rigorous online A/B test, and find that it achieves a significant improvement of >1% in average basket value, relative to the legacy systems.",
    "authors": [
      "Jason Shuo Zhang",
      "Benjamin Howson",
      "Panayiota Savva",
      "Eleanor Loh"
    ],
    "url": "http://arxiv.org/abs/2406.06433v1",
    "timestamp": 1718036675,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d0fe937a-7b7d-42f0-a0c8-8fd28ce02a96": {
    "pk": "d0fe937a-7b7d-42f0-a0c8-8fd28ce02a96",
    "title": "SYM3D: Learning Symmetric Triplanes for Better 3D-Awareness of GANs",
    "abstract": "Despite the growing success of 3D-aware GANs, which can be trained on 2D images to generate high-quality 3D assets, they still rely on multi-view images with camera annotations to synthesize sufficient details from all viewing directions. However, the scarce availability of calibrated multi-view image datasets, especially in comparison to single-view images, has limited the potential of 3D GANs. Moreover, while bypassing camera pose annotations with a camera distribution constraint reduces dependence on exact camera parameters, it still struggles to generate a consistent orientation of 3D assets. To this end, we propose SYM3D, a novel 3D-aware GAN designed to leverage the prevalent reflectional symmetry structure found in natural and man-made objects, alongside a proposed view-aware spatial attention mechanism in learning the 3D representation. We evaluate SYM3D on both synthetic (ShapeNet Chairs, Cars, and Airplanes) and real-world datasets (ABO-Chair), demonstrating its superior performance in capturing detailed geometry and texture, even when trained on only single-view images. Finally, we demonstrate the effectiveness of incorporating symmetry regularization in helping reduce artifacts in the modeling of 3D assets in the text-to-3D task.",
    "authors": [
      "Jing Yang",
      "Kyle Fogarty",
      "Fangcheng Zhong",
      "Cengiz Oztireli"
    ],
    "url": "http://arxiv.org/abs/2406.06432v1",
    "timestamp": 1718036647,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "01566f45-d9ab-4b0d-bfab-1f12eeba012c": {
    "pk": "01566f45-d9ab-4b0d-bfab-1f12eeba012c",
    "title": "Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking",
    "abstract": "Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes. While many works have been dedicated to the univariate case, little has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes. By exploiting a characterization of multivariate first stochastic dominance in terms of couplings, we introduce a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport with a smooth cost. Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic. Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm. We showcase our method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics. Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant decision on the relative performance of the models.",
    "authors": [
      "Gabriel Rioux",
      "Apoorva Nitsure",
      "Mattia Rigotti",
      "Kristjan Greenewald",
      "Youssef Mroueh"
    ],
    "url": "http://arxiv.org/abs/2406.06425v1",
    "timestamp": 1718036090,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ML",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0c7cbdc0-f08a-4235-add8-d633e21679ee": {
    "pk": "0c7cbdc0-f08a-4235-add8-d633e21679ee",
    "title": "Margin-aware Preference Optimization for Aligning Diffusion Models without Reference",
    "abstract": "Modern alignment techniques based on human preferences, such as RLHF and DPO, typically employ divergence regularization relative to the reference model to ensure training stability. However, this often limits the flexibility of models during alignment, especially when there is a clear distributional discrepancy between the preference data and the reference model. In this paper, we focus on the alignment of recent text-to-image diffusion models, such as Stable Diffusion XL (SDXL), and find that this \"reference mismatch\" is indeed a significant problem in aligning these models due to the unstructured nature of visual modalities: e.g., a preference for a particular stylistic aspect can easily induce such a discrepancy. Motivated by this observation, we propose a novel and memory-friendly preference alignment method for diffusion models that does not depend on any reference model, coined margin-aware preference optimization (MaPO). MaPO jointly maximizes the likelihood margin between the preferred and dispreferred image sets and the likelihood of the preferred sets, simultaneously learning general stylistic features and preferences. For evaluation, we introduce two new pairwise preference datasets, which comprise self-generated image pairs from SDXL, Pick-Style and Pick-Safety, simulating diverse scenarios of reference mismatch. Our experiments validate that MaPO can significantly improve alignment on Pick-Style and Pick-Safety and general preference alignment when used with Pick-a-Pic v2, surpassing the base SDXL and other existing methods. Our code, models, and datasets are publicly available via https://mapo-t2i.github.io",
    "authors": [
      "Jiwoo Hong",
      "Sayak Paul",
      "Noah Lee",
      "Kashif Rasul",
      "James Thorne",
      "Jongheon Jeong"
    ],
    "url": "http://arxiv.org/abs/2406.06424v1",
    "timestamp": 1718036085,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7c18b39e-5627-4abe-be67-8179eb0b2122": {
    "pk": "7c18b39e-5627-4abe-be67-8179eb0b2122",
    "title": "Hybrid Video Anomaly Detection for Anomalous Scenarios in Autonomous Driving",
    "abstract": "In autonomous driving, the most challenging scenarios are the ones that can only be detected within their temporal context. Most video anomaly detection approaches focus either on surveillance or traffic accidents, which are only a subfield of autonomous driving. In this work, we present HF$^2$-VAD$_{AD}$, a variation of the HF$^2$-VAD surveillance video anomaly detection method for autonomous driving. We learn a representation of normality from a vehicle's ego perspective and evaluate pixel-wise anomaly detections in rare and critical scenarios.",
    "authors": [
      "Daniel Bogdoll",
      "Jan Imhof",
      "Tim Joseph",
      "J. Marius Z\u00f6llner"
    ],
    "url": "http://arxiv.org/abs/2406.06423v1",
    "timestamp": 1718036073,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bbacfd6b-779b-496d-9dd2-c1464e25a6fd": {
    "pk": "bbacfd6b-779b-496d-9dd2-c1464e25a6fd",
    "title": "An Improved Empirical Fisher Approximation for Natural Gradient Descent",
    "abstract": "Approximate Natural Gradient Descent (NGD) methods are an important family of optimisers for deep learning models, which use approximate Fisher information matrices to pre-condition gradients during training. The empirical Fisher (EF) method approximates the Fisher information matrix empirically by reusing the per-sample gradients collected during back-propagation. Despite its ease of implementation, the EF approximation has its theoretical and practical limitations. This paper first investigates the inversely-scaled projection issue of EF, which is shown to be a major cause of the poor empirical approximation quality. An improved empirical Fisher (iEF) method, motivated as a generalised NGD method from a loss reduction perspective, is proposed to address this issue, meanwhile retaining the practical convenience of EF. The exact iEF and EF methods are experimentally evaluated using practical deep learning setups, including widely-used setups for parameter-efficient fine-tuning of pre-trained models (T5-base with LoRA and Prompt-Tuning on GLUE tasks, and ViT with LoRA for CIFAR100). Optimisation experiments show that applying exact iEF as an optimiser provides strong convergence and generalisation. It achieves the best test performance and the lowest training loss for majority of the tasks, even when compared with well-tuned AdamW/Adafactor baselines. Additionally, under a novel empirical evaluation framework, the proposed iEF method shows consistently better approximation quality to the exact Natural Gradient updates than both EF and the more expensive sampled Fisher (SF). Further investigation also shows that the superior approximation quality of iEF is robust to damping across tasks and training stages. Improving existing approximate NGD optimisers with iEF is expected to lead to better convergence ability and stronger robustness to choice of damping.",
    "authors": [
      "Xiaodong Wu",
      "Wenyi Yu",
      "Chao Zhang",
      "Philip Woodland"
    ],
    "url": "http://arxiv.org/abs/2406.06420v1",
    "timestamp": 1718035952,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7cb0ba98-6568-4129-8c13-02547e0d06a1": {
    "pk": "7cb0ba98-6568-4129-8c13-02547e0d06a1",
    "title": "Foundation Inference Models for Markov Jump Processes",
    "abstract": "Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces. These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial. In this work we introduce a methodology for zero-shot inference of Markov jump processes (MJPs), on bounded state spaces, from noisy and sparse observations, which consists of two components. First, a broad probability distribution over families of MJPs, as well as over possible observation times and noise mechanisms, with which we simulate a synthetic dataset of hidden MJPs and their noisy observation process. Second, a neural network model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way. We empirically demonstrate that one and the same (pretrained) model can infer, in a zero-shot fashion, hidden MJPs evolving in state spaces of different dimensionalities. Specifically, we infer MJPs which describe (i) discrete flashing ratchet systems, which are a type of Brownian motors, and the conformational dynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv) simple protein folding models. What is more, we show that our model performs on par with state-of-the-art models which are finetuned to the target datasets.",
    "authors": [
      "David Berghaus",
      "Kostadin Cvejoski",
      "Patrick Seifner",
      "Cesar Ojeda",
      "Ramses J. Sanchez"
    ],
    "url": "http://arxiv.org/abs/2406.06419v1",
    "timestamp": 1718035920,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7fbb4222-f4c5-4f67-aeca-46b4e1b4ccfc": {
    "pk": "7fbb4222-f4c5-4f67-aeca-46b4e1b4ccfc",
    "title": "Explainable Graph Neural Networks Under Fire",
    "abstract": "Predictions made by graph neural networks (GNNs) usually lack interpretability due to their complex computational behavior and the abstract nature of graphs. In an attempt to tackle this, many GNN explanation methods have emerged. Their goal is to explain a model's predictions and thereby obtain trust when GNN models are deployed in decision critical applications. Most GNN explanation methods work in a post-hoc manner and provide explanations in the form of a small subset of important edges and/or nodes. In this paper we demonstrate that these explanations can unfortunately not be trusted, as common GNN explanation methods turn out to be highly susceptible to adversarial perturbations. That is, even small perturbations of the original graph structure that preserve the model's predictions may yield drastically different explanations. This calls into question the trustworthiness and practical utility of post-hoc explanation methods for GNNs. To be able to attack GNN explanation models, we devise a novel attack method dubbed \\textit{GXAttack}, the first \\textit{optimization-based} adversarial attack method for post-hoc GNN explanations under such settings. Due to the devastating effectiveness of our attack, we call for an adversarial evaluation of future GNN explainers to demonstrate their robustness.",
    "authors": [
      "Zhong Li",
      "Simon Geisler",
      "Yuhang Wang",
      "Stephan G\u00fcnnemann",
      "Matthijs van Leeuwen"
    ],
    "url": "http://arxiv.org/abs/2406.06417v1",
    "timestamp": 1718035756,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5f68c023-bbbb-49b4-89bf-f22a778af8eb": {
    "pk": "5f68c023-bbbb-49b4-89bf-f22a778af8eb",
    "title": "Differentially Private Best-Arm Identification",
    "abstract": "Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence in both the local and central models, i.e. $\\epsilon$-local and $\\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive lower bounds on the sample complexity of any $\\delta$-correct BAI algorithm satisfying $\\epsilon$-global DP or $\\epsilon$-local DP. Our lower bounds suggest the existence of two privacy regimes. In the high-privacy regime, the hardness depends on a coupled effect of privacy and novel information-theoretic quantities involving the Total Variation. In the low-privacy regime, the lower bounds reduce to the non-private lower bounds. We propose $\\epsilon$-local DP and $\\epsilon$-global DP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively. For $\\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a private estimator of the means based on Randomised Response. For $\\epsilon$-global DP, our private estimator of the mean runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. By adapting the transportation costs, the expected sample complexity of AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants.",
    "authors": [
      "Achraf Azize",
      "Marc Jourdan",
      "Aymen Al Marjani",
      "Debabrota Basu"
    ],
    "url": "http://arxiv.org/abs/2406.06408v1",
    "timestamp": 1718035368,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ML",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "99d59f3d-839d-4d9f-b177-72b9b06edd84": {
    "pk": "99d59f3d-839d-4d9f-b177-72b9b06edd84",
    "title": "A Taxonomy of Challenges to Curating Fair Datasets",
    "abstract": "Despite extensive efforts to create fairer machine learning (ML) datasets, there remains a limited understanding of the practical aspects of dataset curation. Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle. Our findings underscore overarching issues within the broader fairness landscape that impact data curation. We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices.",
    "authors": [
      "Dora Zhao",
      "Morgan Klaus Scheuerman",
      "Pooja Chitre",
      "Jerone T. A. Andrews",
      "Georgia Panagiotidou",
      "Shawn Walker",
      "Kathleen H. Pine",
      "Alice Xiang"
    ],
    "url": "http://arxiv.org/abs/2406.06407v1",
    "timestamp": 1718035148,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c983b2ba-2055-4335-ab3d-16b2467e4cf3": {
    "pk": "c983b2ba-2055-4335-ab3d-16b2467e4cf3",
    "title": "Controlling Emotion in Text-to-Speech with Natural Language Prompts",
    "abstract": "In recent years, prompting has quickly become one of the standard ways of steering the outputs of generative machine learning models, due to its intuitive use of natural language. In this work, we propose a system conditioned on embeddings derived from an emotionally rich text that serves as prompt. Thereby, a joint representation of speaker and prompt embeddings is integrated at several points within a transformer-based architecture. Our approach is trained on merged emotional speech and text datasets and varies prompts in each training iteration to increase the generalization capabilities of the model. Objective and subjective evaluation results demonstrate the ability of the conditioned synthesis system to accurately transfer the emotions present in a prompt to speech. At the same time, precise tractability of speaker identities as well as overall high speech quality and intelligibility are maintained.",
    "authors": [
      "Thomas Bott",
      "Florian Lux",
      "Ngoc Thang Vu"
    ],
    "url": "http://arxiv.org/abs/2406.06406v1",
    "timestamp": 1718035122,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c7bcc59a-fa9e-4997-81a1-6c44141ad0fa": {
    "pk": "c7bcc59a-fa9e-4997-81a1-6c44141ad0fa",
    "title": "Meta Learning Text-to-Speech Synthesis in over 7000 Languages",
    "abstract": "In this work, we take on the challenging task of building a single text-to-speech synthesis system that is capable of generating speech in over 7000 languages, many of which lack sufficient data for traditional TTS development. By leveraging a novel integration of massively multilingual pretraining and meta learning to approximate language representations, our approach enables zero-shot speech synthesis in languages without any available data. We validate our system's performance through objective measures and human evaluation across a diverse linguistic landscape. By releasing our code and models publicly, we aim to empower communities with limited linguistic resources and foster further innovation in the field of speech technology.",
    "authors": [
      "Florian Lux",
      "Sarina Meyer",
      "Lyonel Behringer",
      "Frank Zalkow",
      "Phat Do",
      "Matt Coler",
      "Emanu\u00ebl A. P. Habets",
      "Ngoc Thang Vu"
    ],
    "url": "http://arxiv.org/abs/2406.06403v1",
    "timestamp": 1718035012,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "86e839b1-5f39-4f6f-ac0b-58e5b9ee8af5": {
    "pk": "86e839b1-5f39-4f6f-ac0b-58e5b9ee8af5",
    "title": "INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition",
    "abstract": "We revisit the INTERSPEECH 2009 Emotion Challenge -- the first ever speech emotion recognition (SER) challenge -- and evaluate a series of deep learning models that are representative of the major advances in SER research in the time since then. We start by training each model using a fixed set of hyperparameters, and further fine-tune the best-performing models of that initial setup with a grid search. Results are always reported on the official test set with a separate validation set only used for early stopping. Most models score below or close to the official baseline, while they marginally outperform the original challenge winners after hyperparameter tuning. Our work illustrates that, despite recent progress, FAU-AIBO remains a very challenging benchmark. An interesting corollary is that newer methods do not consistently outperform older ones, showing that progress towards `solving' SER is not necessarily monotonic.",
    "authors": [
      "Andreas Triantafyllopoulos",
      "Anton Batliner",
      "Simon Rampp",
      "Manuel Milling",
      "Bj\u00f6rn Schuller"
    ],
    "url": "http://arxiv.org/abs/2406.06401v1",
    "timestamp": 1718034906,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e4218f38-824b-443f-9b36-3edc660af71b": {
    "pk": "e4218f38-824b-443f-9b36-3edc660af71b",
    "title": "Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue",
    "abstract": "We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain). However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics. In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering. We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols. Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue. Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics.",
    "authors": [
      "Simone Alghisi",
      "Massimo Rizzoli",
      "Gabriel Roccabruna",
      "Seyed Mahed Mousavi",
      "Giuseppe Riccardi"
    ],
    "url": "http://arxiv.org/abs/2406.06399v1",
    "timestamp": 1718034769,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f8ad9166-bd77-4bb2-bf15-0a4ec6388dd0": {
    "pk": "f8ad9166-bd77-4bb2-bf15-0a4ec6388dd0",
    "title": "Contrastive learning of T cell receptor representations",
    "abstract": "Computational prediction of the interaction of T cell receptors (TCRs) and their ligands is a grand challenge in immunology. Despite advances in high-throughput assays, specificity-labelled TCR data remains sparse. In other domains, the pre-training of language models on unlabelled data has been successfully used to address data bottlenecks. However, it is unclear how to best pre-train protein language models for TCR specificity prediction. Here we introduce a TCR language model called SCEPTR (Simple Contrastive Embedding of the Primary sequence of T cell Receptors), capable of data-efficient transfer learning. Through our model, we introduce a novel pre-training strategy combining autocontrastive learning and masked-language modelling, which enables SCEPTR to achieve its state-of-the-art performance. In contrast, existing protein language models and a variant of SCEPTR pre-trained without autocontrastive learning are outperformed by sequence alignment-based methods. We anticipate that contrastive learning will be a useful paradigm to decode the rules of TCR specificity.",
    "authors": [
      "Yuta Nagano",
      "Andrew Pyo",
      "Martina Milighetti",
      "James Henderson",
      "John Shawe-Taylor",
      "Benny Chain",
      "Andreas Tiffeau-Mayer"
    ],
    "url": "http://arxiv.org/abs/2406.06397v1",
    "timestamp": 1718034645,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "q-bio.BM",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a36ad8df-7869-4528-9af1-71109936945e": {
    "pk": "a36ad8df-7869-4528-9af1-71109936945e",
    "title": "Towards Lifelong Learning of Large Language Models: A Survey",
    "abstract": "As the applications of large language models (LLMs) expand across diverse fields, the ability of these models to adapt to ongoing changes in data, tasks, and user preferences becomes crucial. Traditional training methods, relying on static datasets, are increasingly inadequate for coping with the dynamic nature of real-world information. Lifelong learning, also known as continual or incremental learning, addresses this challenge by enabling LLMs to learn continuously and adaptively over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting. This survey delves into the sophisticated landscape of lifelong learning, categorizing strategies into two primary groups: Internal Knowledge and External Knowledge. Internal Knowledge includes continual pretraining and continual finetuning, each enhancing the adaptability of LLMs in various scenarios. External Knowledge encompasses retrieval-based and tool-based lifelong learning, leveraging external data sources and computational tools to extend the model's capabilities without modifying core parameters. The key contributions of our survey are: (1) Introducing a novel taxonomy categorizing the extensive literature of lifelong learning into 12 scenarios; (2) Identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups within each scenario; (3) Highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era. Through a detailed examination of these groups and their respective categories, this survey aims to enhance the adaptability, reliability, and overall performance of LLMs in real-world applications.",
    "authors": [
      "Junhao Zheng",
      "Shengjie Qiu",
      "Chengming Shi",
      "Qianli Ma"
    ],
    "url": "http://arxiv.org/abs/2406.06391v1",
    "timestamp": 1718034385,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "aa4ed3a3-750d-4d78-a99e-04390012bfe0": {
    "pk": "aa4ed3a3-750d-4d78-a99e-04390012bfe0",
    "title": "FPN-IAIA-BL: A Multi-Scale Interpretable Deep Learning Model for Classification of Mass Margins in Digital Mammography",
    "abstract": "Digital mammography is essential to breast cancer detection, and deep learning offers promising tools for faster and more accurate mammogram analysis. In radiology and other high-stakes environments, uninterpretable (\"black box\") deep learning models are unsuitable and there is a call in these fields to make interpretable models. Recent work in interpretable computer vision provides transparency to these formerly black boxes by utilizing prototypes for case-based explanations, achieving high accuracy in applications including mammography. However, these models struggle with precise feature localization, reasoning on large portions of an image when only a small part is relevant. This paper addresses this gap by proposing a novel multi-scale interpretable deep learning model for mammographic mass margin classification. Our contribution not only offers an interpretable model with reasoning aligned with radiologist practices, but also provides a general architecture for computer vision with user-configurable prototypes from coarse- to fine-grained prototypes.",
    "authors": [
      "Julia Yang",
      "Alina Jade Barnett",
      "Jon Donnelly",
      "Satvik Kishore",
      "Jerry Fang",
      "Fides Regina Schwartz",
      "Chaofan Chen",
      "Joseph Y. Lo",
      "Cynthia Rudin"
    ],
    "url": "http://arxiv.org/abs/2406.06386v1",
    "timestamp": 1718034281,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3466b78f-3a9d-4ba8-9d88-4ccfc2d0e6c4": {
    "pk": "3466b78f-3a9d-4ba8-9d88-4ccfc2d0e6c4",
    "title": "Low-Rank Quantization-Aware Training for LLMs",
    "abstract": "Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands. Quantization is one of the most effective ways to make them more compute and memory efficient. Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs. Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing. Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques. We apply LR-QAT to the LLaMA-2/3 and Mistral model families and validate its effectiveness on several downstream tasks. Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage. Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory.",
    "authors": [
      "Yelysei Bondarenko",
      "Riccardo Del Chiaro",
      "Markus Nagel"
    ],
    "url": "http://arxiv.org/abs/2406.06385v1",
    "timestamp": 1718034262,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2e66f294-e8dc-4398-ba5a-6d86a63ba8ef": {
    "pk": "2e66f294-e8dc-4398-ba5a-6d86a63ba8ef",
    "title": "Generalizing to Unseen Domains in Diabetic Retinopathy with Disentangled Representations",
    "abstract": "Diabetic Retinopathy (DR), induced by diabetes, poses a significant risk of visual impairment. Accurate and effective grading of DR aids in the treatment of this condition. Yet existing models experience notable performance degradation on unseen domains due to domain shifts. Previous methods address this issue by simulating domain style through simple visual transformation and mitigating domain noise via learning robust representations. However, domain shifts encompass more than image styles. They overlook biases caused by implicit factors such as ethnicity, age, and diagnostic criteria. In our work, we propose a novel framework where representations of paired data from different domains are decoupled into semantic features and domain noise. The resulting augmented representation comprises original retinal semantics and domain noise from other domains, aiming to generate enhanced representations aligned with real-world clinical needs, incorporating rich information from diverse domains. Subsequently, to improve the robustness of the decoupled representations, class and domain prototypes are employed to interpolate the disentangled representations while data-aware weights are designed to focus on rare classes and domains. Finally, we devise a robust pixel-level semantic alignment loss to align retinal semantics decoupled from features, maintaining a balance between intra-class diversity and dense class features. Experimental results on multiple benchmarks demonstrate the effectiveness of our method on unseen domains. The code implementations are accessible on https://github.com/richard-peng-xia/DECO.",
    "authors": [
      "Peng Xia",
      "Ming Hu",
      "Feilong Tang",
      "Wenxue Li",
      "Wenhao Zheng",
      "Lie Ju",
      "Peibo Duan",
      "Huaxiu Yao",
      "Zongyuan Ge"
    ],
    "url": "http://arxiv.org/abs/2406.06384v1",
    "timestamp": 1718034236,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "52a682d6-8ace-434a-9e85-907e29f64e9a": {
    "pk": "52a682d6-8ace-434a-9e85-907e29f64e9a",
    "title": "Diffusion-RPO: Aligning Diffusion Models through Relative Preference Optimization",
    "abstract": "Aligning large language models with human preferences has emerged as a critical focus in language modeling research. Yet, integrating preference learning into Text-to-Image (T2I) generative models is still relatively uncharted territory. The Diffusion-DPO technique made initial strides by employing pairwise preference learning in diffusion models tailored for specific text prompts. We introduce Diffusion-RPO, a new method designed to align diffusion-based T2I models with human preferences more effectively. This approach leverages both prompt-image pairs with identical prompts and those with semantically related content across various modalities. Furthermore, we have developed a new evaluation metric, style alignment, aimed at overcoming the challenges of high costs, low reproducibility, and limited interpretability prevalent in current evaluations of human preference alignment. Our findings demonstrate that Diffusion-RPO outperforms established methods such as Supervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions 1.5 and XL-1.0, achieving superior results in both automated evaluations of human preferences and style alignment. Our code is available at https://github.com/yigu1008/Diffusion-RPO",
    "authors": [
      "Yi Gu",
      "Zhendong Wang",
      "Yueqin Yin",
      "Yujia Xie",
      "Mingyuan Zhou"
    ],
    "url": "http://arxiv.org/abs/2406.06382v1",
    "timestamp": 1718034123,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4c7a971e-61e8-4490-a16e-71a3e22c8cfe": {
    "pk": "4c7a971e-61e8-4490-a16e-71a3e22c8cfe",
    "title": "Improving Deep Learning-based Automatic Cranial Defect Reconstruction by Heavy Data Augmentation: From Image Registration to Latent Diffusion Models",
    "abstract": "Modeling and manufacturing of personalized cranial implants are important research areas that may decrease the waiting time for patients suffering from cranial damage. The modeling of personalized implants may be partially automated by the use of deep learning-based methods. However, this task suffers from difficulties with generalizability into data from previously unseen distributions that make it difficult to use the research outcomes in real clinical settings. Due to difficulties with acquiring ground-truth annotations, different techniques to improve the heterogeneity of datasets used for training the deep networks have to be considered and introduced. In this work, we present a large-scale study of several augmentation techniques, varying from classical geometric transformations, image registration, variational autoencoders, and generative adversarial networks, to the most recent advances in latent diffusion models. We show that the use of heavy data augmentation significantly increases both the quantitative and qualitative outcomes, resulting in an average Dice Score above 0.94 for the SkullBreak and above 0.96 for the SkullFix datasets. Moreover, we show that the synthetically augmented network successfully reconstructs real clinical defects. The work is a considerable contribution to the field of artificial intelligence in the automatic modeling of personalized cranial implants.",
    "authors": [
      "Marek Wodzinski",
      "Kamil Kwarciak",
      "Mateusz Daniol",
      "Daria Hemmerling"
    ],
    "url": "http://arxiv.org/abs/2406.06372v1",
    "timestamp": 1718033663,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "07bfd428-5284-4b24-aefa-0b721f740c6b": {
    "pk": "07bfd428-5284-4b24-aefa-0b721f740c6b",
    "title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models",
    "abstract": "Initially introduced as a machine translation model, the Transformer architecture has now become the foundation for modern deep learning architecture, with applications in a wide range of fields, from computer vision to natural language processing. Nowadays, to tackle increasingly more complex tasks, Transformer-based models are stretched to enormous sizes, requiring increasingly larger training datasets, and unsustainable amount of compute resources. The ubiquitous nature of the Transformer and its core component, the attention mechanism, are thus prime targets for efficiency research. In this work, we propose an alternative compatibility function for the self-attention mechanism introduced by the Transformer architecture. This compatibility function exploits an overlap in the learned representation of the traditional scaled dot-product attention, leading to a symmetric with pairwise coefficient dot-product attention. When applied to the pre-training of BERT-like models, this new symmetric attention mechanism reaches a score of 79.36 on the GLUE benchmark against 78.74 for the traditional implementation, leads to a reduction of 6% in the number of trainable parameters, and reduces the number of training steps required before convergence by half.",
    "authors": [
      "Martin Courtois",
      "Malte Ostendorff",
      "Leonhard Hennig",
      "Georg Rehm"
    ],
    "url": "http://arxiv.org/abs/2406.06366v1",
    "timestamp": 1718033055,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7d2c4008-40b1-450b-81eb-8e311f80018c": {
    "pk": "7d2c4008-40b1-450b-81eb-8e311f80018c",
    "title": "MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows",
    "abstract": "Scientific innovation relies on detailed workflows, which include critical steps such as analyzing literature, generating ideas, validating these ideas, interpreting results, and inspiring follow-up research. However, scientific publications that document these workflows are extensive and unstructured. This makes it difficult for both human researchers and AI systems to effectively navigate and explore the space of scientific innovation. To address this issue, we introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization of Scientific Workflows. MASSW includes more than 152,000 peer-reviewed publications from 17 leading computer science conferences spanning the past 50 years. Using Large Language Models (LLMs), we automatically extract five core aspects from these publications -- context, key idea, method, outcome, and projected impact -- which correspond to five key steps in the research workflow. These structured summaries facilitate a variety of downstream tasks and analyses. The quality of the LLM-extracted summaries is validated by comparing them with human annotations. We demonstrate the utility of MASSW through multiple novel machine-learning tasks that can be benchmarked using this new dataset, which make various types of predictions and recommendations along the scientific workflow. MASSW holds significant potential for researchers to create and benchmark new AI methods for optimizing scientific workflows and fostering scientific innovation in the field. Our dataset is openly available at \\url{https://github.com/xingjian-zhang/massw}.",
    "authors": [
      "Xingjian Zhang",
      "Yutong Xie",
      "Jin Huang",
      "Jinge Ma",
      "Zhaoying Pan",
      "Qijia Liu",
      "Ziyang Xiong",
      "Tolga Ergen",
      "Dongsub Shim",
      "Honglak Lee",
      "Qiaozhu Mei"
    ],
    "url": "http://arxiv.org/abs/2406.06357v1",
    "timestamp": 1718032749,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c7ee96da-4234-4cde-bfde-f1fd224f911d": {
    "pk": "c7ee96da-4234-4cde-bfde-f1fd224f911d",
    "title": "On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean Functions",
    "abstract": "We investigate the out-of-domain generalization of random feature (RF) models and Transformers. We first prove that in the `generalization on the unseen (GOTU)' setting, where training data is fully seen in some part of the domain but testing is made on another part, and for RF models in the small feature regime, the convergence takes place to interpolators of minimal degree as in the Boolean case (Abbe et al., 2023). We then consider the sparse target regime and explain how this regime relates to the small feature regime, but with a different regularization term that can alter the picture in the non-Boolean case. We show two different outcomes for the sparse regime with q-ary data tokens: (1) if the data is embedded with roots of unities, then a min-degree interpolator is learned like in the Boolean case for RF models, (2) if the data is not embedded as such, e.g., simply as integers, then RF models and Transformers may not learn minimal degree interpolators. This shows that the Boolean setting and its roots of unities generalization are special cases where the minimal degree interpolator offers a rare characterization of how learning takes place. For more general integer and real-valued settings, a more nuanced picture remains to be fully characterized.",
    "authors": [
      "Denys Pushkin",
      "Rapha\u00ebl Berthier",
      "Emmanuel Abbe"
    ],
    "url": "http://arxiv.org/abs/2406.06354v1",
    "timestamp": 1718032473,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7b324455-1784-43b0-be8e-e969c93531ed": {
    "pk": "7b324455-1784-43b0-be8e-e969c93531ed",
    "title": "Latent Directions: A Simple Pathway to Bias Mitigation in Generative AI",
    "abstract": "Mitigating biases in generative AI and, particularly in text-to-image models, is of high importance given their growing implications in society. The biased datasets used for training pose challenges in ensuring the responsible development of these models, and mitigation through hard prompting or embedding alteration, are the most common present solutions. Our work introduces a novel approach to achieve diverse and inclusive synthetic images by learning a direction in the latent space and solely modifying the initial Gaussian noise provided for the diffusion process. Maintaining a neutral prompt and untouched embeddings, this approach successfully adapts to diverse debiasing scenarios, such as geographical biases. Moreover, our work proves it is possible to linearly combine these learned latent directions to introduce new mitigations, and if desired, integrate it with text embedding adjustments. Furthermore, text-to-image models lack transparency for assessing bias in outputs, unless visually inspected. Thus, we provide a tool to empower developers to select their desired concepts to mitigate. The project page with code is available online.",
    "authors": [
      "Carolina Lopez Olmos",
      "Alexandros Neophytou",
      "Sunando Sengupta",
      "Dim P. Papadopoulos"
    ],
    "url": "http://arxiv.org/abs/2406.06352v1",
    "timestamp": 1718032431,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "92ce7999-00b5-463d-899b-b07725cf2230": {
    "pk": "92ce7999-00b5-463d-899b-b07725cf2230",
    "title": "Cascading Unknown Detection with Known Classification for Open Set Recognition",
    "abstract": "Deep learners tend to perform well when trained under the closed set assumption but struggle when deployed under open set conditions. This motivates the field of Open Set Recognition in which we seek to give deep learners the ability to recognize whether a data sample belongs to the known classes trained on or comes from the surrounding infinite world. Existing open set recognition methods typically rely upon a single function for the dual task of distinguishing between knowns and unknowns as well as making known class distinction. This dual process leaves performance on the table as the function is not specialized for either task. In this work, we introduce Cascading Unknown Detection with Known Classification (Cas-DC), where we instead learn specialized functions in a cascading fashion for both known/unknown detection and fine class classification amongst the world of knowns. Our experiments and analysis demonstrate that Cas-DC handily outperforms modern methods in open set recognition when compared using AUROC scores and correct classification rate at various true positive rates.",
    "authors": [
      "Daniel Brignac",
      "Abhijit Mahalanobis"
    ],
    "url": "http://arxiv.org/abs/2406.06351v1",
    "timestamp": 1718032387,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8243f088-91c7-4d8f-9523-db140fb2318b": {
    "pk": "8243f088-91c7-4d8f-9523-db140fb2318b",
    "title": "Error Analysis and Numerical Algorithm for PDE Approximation with Hidden-Layer Concatenated Physics Informed Neural Networks",
    "abstract": "We present the hidden-layer concatenated physics informed neural network (HLConcPINN) method, which combines hidden-layer concatenated feed-forward neural networks, a modified block time marching strategy, and a physics informed approach for approximating partial differential equations (PDEs). We analyze the convergence properties and establish the error bounds of this method for two types of PDEs: parabolic (exemplified by the heat and Burgers' equations) and hyperbolic (exemplified by the wave and nonlinear Klein-Gordon equations). We show that its approximation error of the solution can be effectively controlled by the training loss for dynamic simulations with long time horizons. The HLConcPINN method in principle allows an arbitrary number of hidden layers not smaller than two and any of the commonly-used smooth activation functions for the hidden layers beyond the first two, with theoretical guarantees. This generalizes several recent neural-network techniques, which have theoretical guarantees but are confined to two hidden layers in the network architecture and the $\\tanh$ activation function. Our theoretical analyses subsequently inform the formulation of appropriate training loss functions for these PDEs, leading to physics informed neural network (PINN) type computational algorithms that differ from the standard PINN formulation. Ample numerical experiments are presented based on the proposed algorithm to validate the effectiveness of this method and confirm aspects of the theoretical analyses.",
    "authors": [
      "Yianxia Qian",
      "Yongchao Zhang",
      "Suchuan Dong"
    ],
    "url": "http://arxiv.org/abs/2406.06350v1",
    "timestamp": 1718032373,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.NA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6da9c00e-c9e4-42d0-9e91-ad8f8237f9a5": {
    "pk": "6da9c00e-c9e4-42d0-9e91-ad8f8237f9a5",
    "title": "Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning",
    "abstract": "The aim in many sciences is to understand the mechanisms that underlie the observed distribution of variables, starting from a set of initial hypotheses. Causal discovery allows us to infer mechanisms as sets of cause and effect relationships in a generalized way -- without necessarily tailoring to a specific domain. Causal discovery algorithms search over a structured hypothesis space, defined by the set of directed acyclic graphs, to find the graph that best explains the data. For high-dimensional problems, however, this search becomes intractable and scalable algorithms for causal discovery are needed to bridge the gap. In this paper, we define a novel causal graph partition that allows for divide-and-conquer causal discovery with theoretical guarantees. We leverage the idea of a superstructure -- a set of learned or existing candidate hypotheses -- to partition the search space. We prove under certain assumptions that learning with a causal graph partition always yields the Markov Equivalence Class of the true causal graph. We show our algorithm achieves comparable accuracy and a faster time to solution for biologically-tuned synthetic networks and networks up to ${10^4}$ variables. This makes our method applicable to gene regulatory network inference and other domains with high-dimensional structured hypothesis spaces.",
    "authors": [
      "Ashka Shah",
      "Adela DePavia",
      "Nathaniel Hudson",
      "Ian Foster",
      "Rick Stevens"
    ],
    "url": "http://arxiv.org/abs/2406.06348v1",
    "timestamp": 1718032094,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "633c1439-e055-4265-b240-57d1fedbc41f": {
    "pk": "633c1439-e055-4265-b240-57d1fedbc41f",
    "title": "A Guide to Stochastic Optimisation for Large-Scale Inverse Problems",
    "abstract": "Stochastic optimisation algorithms are the de facto standard for machine learning with large amounts of data. Handling only a subset of available data in each optimisation step dramatically reduces the per-iteration computational costs, while still ensuring significant progress towards the solution. Driven by the need to solve large-scale optimisation problems as efficiently as possible, the last decade has witnessed an explosion of research in this area. Leveraging the parallels between machine learning and inverse problems has allowed harnessing the power of this research wave for solving inverse problems. In this survey, we provide a comprehensive account of the state-of-the-art in stochastic optimisation from the viewpoint of inverse problems. We present algorithms with diverse modalities of problem randomisation and discuss the roles of variance reduction, acceleration, higher-order methods, and other algorithmic modifications, and compare theoretical results with practical behaviour. We focus on the potential and the challenges for stochastic optimisation that are unique to inverse imaging problems and are not commonly encountered in machine learning. We conclude the survey with illustrative examples from imaging problems to examine the advantages and disadvantages that this new generation of algorithms bring to the field of inverse problems.",
    "authors": [
      "Matthias J. Ehrhardt",
      "Zeljko Kereta",
      "Jingwei Liang",
      "Junqi Tang"
    ],
    "url": "http://arxiv.org/abs/2406.06342v1",
    "timestamp": 1718031750,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.NA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bb38d2f4-aeb4-4536-98f9-292e3b3ce59b": {
    "pk": "bb38d2f4-aeb4-4536-98f9-292e3b3ce59b",
    "title": "Optimisation of federated learning settings under statistical heterogeneity variations",
    "abstract": "Federated Learning (FL) enables local devices to collaboratively learn a shared predictive model by only periodically sharing model parameters with a central aggregator. However, FL can be disadvantaged by statistical heterogeneity produced by the diversity in each local devices data distribution, which creates different levels of Independent and Identically Distributed (IID) data. Furthermore, this can be more complex when optimising different combinations of FL parameters and choosing optimal aggregation. In this paper, we present an empirical analysis of different FL training parameters and aggregators over various levels of statistical heterogeneity on three datasets. We propose a systematic data partition strategy to simulate different levels of statistical heterogeneity and a metric to measure the level of IID. Additionally, we empirically identify the best FL model and key parameters for datasets of different characteristics. On the basis of these, we present recommended guidelines for FL parameters and aggregators to optimise model performance under different levels of IID and with different datasets",
    "authors": [
      "Basem Suleiman",
      "Muhammad Johan Alibasa",
      "Rizka Widyarini Purwanto",
      "Lewis Jeffries",
      "Ali Anaissi",
      "Jacky Song"
    ],
    "url": "http://arxiv.org/abs/2406.06340v1",
    "timestamp": 1718031663,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "879c3676-d4e9-4df1-8460-b58a12c98433": {
    "pk": "879c3676-d4e9-4df1-8460-b58a12c98433",
    "title": "A Parameter-efficient Language Extension Framework for Multilingual ASR",
    "abstract": "Covering all languages with a multilingual speech recognition model (MASR) is very difficult. Performing language extension on top of an existing MASR is a desirable choice. In this study, the MASR continual learning problem is probabilistically decomposed into language identity prediction (LP) and cross-lingual adaptation (XLA) sub-problems. Based on this, we propose an architecture-based framework for language extension that can fundamentally solve catastrophic forgetting, debudded as PELE. PELE is designed to be parameter-efficient, incrementally incorporating an add-on module to adapt to a new language. Specifically, different parameter-efficient fine-tuning (PEFT) modules and their variants are explored as potential candidates to perform XLA. Experiments are carried out on 5 new languages with a wide range of low-resourced data sizes. The best-performing PEFT candidate can achieve satisfactory performance across all languages and demonstrates superiority in three of five languages over the continual joint learning setting. Notably, PEFT methods focusing on weight parameters or input features are revealed to be limited in performance, showing significantly inferior extension capabilities compared to inserting a lightweight module in between layers such as an Adapter.",
    "authors": [
      "Wei Liu",
      "Jingyong Hou",
      "Dong Yang",
      "Muyong Cao",
      "Tan Lee"
    ],
    "url": "http://arxiv.org/abs/2406.06329v1",
    "timestamp": 1718030767,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "254099c7-929c-4374-9784-a13e0eb6a4e1": {
    "pk": "254099c7-929c-4374-9784-a13e0eb6a4e1",
    "title": "Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching",
    "abstract": "Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on Llama2 family models reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.",
    "authors": [
      "Xiaoying Zhang",
      "Baolin Peng",
      "Ye Tian",
      "Jingyan Zhou",
      "Yipeng Zhang",
      "Haitao Mi",
      "Helen Meng"
    ],
    "url": "http://arxiv.org/abs/2406.06326v1",
    "timestamp": 1718030540,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "880f1702-74e0-4f42-80e9-12ebd54ce7ec": {
    "pk": "880f1702-74e0-4f42-80e9-12ebd54ce7ec",
    "title": "Navigating Unknowns: Deep Learning Robustness for Gravitational Wave Signal Reconstruction",
    "abstract": "We present a rapid and reliable deep learning-based method for gravitational wave signal reconstruction from elusive, generic binary black hole mergers in LIGO data. We demonstrate that our model, \\texttt{AWaRe}, effectively recovers gravitational waves with parameters it has not encountered during training. This includes features like higher black hole masses, additional harmonics, eccentricity, and varied waveform systematics, which introduce complex modulations in the waveform's amplitudes and phases. The accurate reconstructions of these unseen signal characteristics demonstrates \\texttt{AWaRe}'s ability to handle complex features in the waveforms. By directly incorporating waveform reconstruction uncertainty estimation into the \\texttt{AWaRe} framework, we show that for real gravitational wave events, the uncertainties in \\texttt{AWaRe}'s reconstructions align closely with those achieved by benchmark algorithms like BayesWave and coherent WaveBurst. The robustness of our model to real gravitational wave events and its ability to extrapolate to unseen data open new avenues for investigations in various aspects of gravitational wave astrophysics and data analysis, including tests of General Relativity and the enhancement of current gravitational wave search methodologies.",
    "authors": [
      "Chayan Chatterjee",
      "Karan Jani"
    ],
    "url": "http://arxiv.org/abs/2406.06324v1",
    "timestamp": 1718030474,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "gr-qc",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9f22420d-2190-4ea2-a00d-51c2ef885260": {
    "pk": "9f22420d-2190-4ea2-a00d-51c2ef885260",
    "title": "Measurement of the Depth of Maximum of Air-Shower Profiles with energies between $\\mathbf{10^{18.5}}$ and $\\mathbf{10^{20}}$ eV using the Surface Detector of the Pierre Auger Observatory and Deep Learning",
    "abstract": "We report an investigation of the mass composition of cosmic rays with energies from 3 to 100 EeV (1 EeV=$10^{18}$ eV) using the distributions of the depth of shower maximum $X_\\mathrm{max}$. The analysis relies on ${\\sim}50,000$ events recorded by the Surface Detector of the Pierre Auger Observatory and a deep-learning-based reconstruction algorithm. Above energies of 5 EeV, the data set offers a 10-fold increase in statistics with respect to fluorescence measurements at the Observatory. After cross-calibration using the Fluorescence Detector, this enables the first measurement of the evolution of the mean and the standard deviation of the $X_\\mathrm{max}$ distributions up to 100 EeV. Our findings are threefold:   (1.) The evolution of the mean logarithmic mass towards a heavier composition with increasing energy can be confirmed and is extended to 100 EeV.   (2.) The evolution of the fluctuations of $X_\\mathrm{max}$ towards a heavier and purer composition with increasing energy can be confirmed with high statistics. We report a rather heavy composition and small fluctuations in $X_\\mathrm{max}$ at the highest energies.   (3.) We find indications for a characteristic structure beyond a constant change in the mean logarithmic mass, featuring three breaks that are observed in proximity to the ankle, instep, and suppression features in the energy spectrum.",
    "authors": [
      "The Pierre Auger Collaboration",
      "A. Abdul Halim",
      "P. Abreu",
      "M. Aglietta",
      "I. Allekotte",
      "K. Almeida Cheminant",
      "A. Almela",
      "R. Aloisio",
      "J. Alvarez-Mu\u00f1iz",
      "J. Ammerman Yebra",
      "G. A. Anastasi",
      "L. Anchordoqui",
      "B. Andrada",
      "L. Andrade Dourado",
      "S. Andringa",
      "L. Apollonio",
      "C. Aramo",
      "P. R. Ara\u00fajo Ferreira",
      "E. Arnone",
      "J. C. Arteaga Vel\u00e1zquez",
      "P. Assis",
      "G. Avila",
      "E. Avocone",
      "A. Bakalova",
      "F. Barbato",
      "A. Bartz Mocellin",
      "C. Berat",
      "M. E. Bertaina",
      "G. Bhatta",
      "M. Bianciotto",
      "P. L. Biermann",
      "V. Binet",
      "K. Bismark",
      "T. Bister",
      "J. Biteau",
      "J. Blazek",
      "C. Bleve",
      "J. Bl\u00fcmer",
      "M. Boh\u00e1\u010dov\u00e1",
      "D. Boncioli",
      "C. Bonifazi",
      "L. Bonneau Arbeletche",
      "N. Borodai",
      "J. Brack",
      "P. G. Brichetto Orchera",
      "F. L. Briechle",
      "A. Bueno",
      "S. Buitink",
      "M. Buscemi",
      "M. B\u00fcsken",
      "A. Bwembya",
      "K. S. Caballero-Mora",
      "S. Cabana-Freire",
      "L. Caccianiga",
      "F. Campuzano",
      "R. Caruso",
      "A. Castellina",
      "F. Catalani",
      "G. Cataldi",
      "L. Cazon",
      "M. Cerda",
      "B. \u010cerm\u00e1kov\u00e1",
      "A. Cermenati",
      "J. A. Chinellato",
      "J. Chudoba",
      "L. Chytka",
      "R. W. Clay",
      "A. C. Cobos Cerutti",
      "R. Colalillo",
      "M. R. Coluccia",
      "R. Concei\u00e7\u00e3o",
      "A. Condorelli",
      "G. Consolati",
      "M. Conte",
      "F. Convenga",
      "D. Correia dos Santos",
      "P. J. Costa",
      "C. E. Covault",
      "M. Cristinziani",
      "C. S. Cruz Sanchez",
      "S. Dasso",
      "K. Daumiller",
      "B. R. Dawson",
      "R. M. de Almeida",
      "B. de Errico",
      "J. de Jes\u00fas",
      "S. J. de Jong",
      "J. R. T. de Mello Neto",
      "I. De Mitri",
      "J. de Oliveira",
      "D. de Oliveira Franco",
      "F. de Palma",
      "V. de Souza",
      "E. De Vito",
      "A. Del Popolo",
      "O. Deligny",
      "N. Denner",
      "L. Deval",
      "A. di Matteo",
      "J. A. do",
      "M. Dobre",
      "C. Dobrigkeit",
      "J. C. D'Olivo",
      "L. M. Domingues Mendes",
      "Q. Dorosti",
      "J. C. dos Anjos",
      "R. C. dos Anjos",
      "J. Ebr",
      "F. Ellwanger",
      "M. Emam",
      "R. Engel",
      "I. Epicoco",
      "M. Erdmann",
      "A. Etchegoyen",
      "C. Evoli",
      "H. Falcke",
      "G. Farrar",
      "A. C. Fauth",
      "T. Fehler",
      "F. Feldbusch",
      "F. Fenu",
      "A. Fernandes",
      "B. Fick",
      "J. M. Figueira",
      "P. Filip",
      "A. Filip\u010di\u010d",
      "T. Fitoussi",
      "B. Flaggs",
      "T. Fodran",
      "T. Fujii",
      "A. Fuster",
      "C. Galea",
      "B. Garc\u00eda",
      "C. Gaudu",
      "A. Gherghel-Lascu",
      "P. L. Ghia",
      "U. Giaccari",
      "J. Glombitza",
      "F. Gobbi",
      "F. Gollan",
      "G. Golup",
      "M. G\u00f3mez Berisso",
      "P. F. G\u00f3mez Vitale",
      "J. P. Gongora",
      "J. M. Gonz\u00e1lez",
      "N. Gonz\u00e1lez",
      "D. G\u00f3ra",
      "A. Gorgi",
      "M. Gottowik",
      "F. Guarino",
      "G. P. Guedes",
      "E. Guido",
      "L. G\u00fclzow",
      "S. Hahn",
      "P. Hamal",
      "M. R. Hampel",
      "P. Hansen",
      "D. Harari",
      "V. M. Harvey",
      "A. Haungs",
      "T. Hebbeker",
      "C. Hojvat",
      "J. R. H\u00f6randel",
      "P. Horvath",
      "M. Hrabovsk\u00fd",
      "T. Huege",
      "A. Insolia",
      "P. G. Isar",
      "P. Janecek",
      "V. Jilek",
      "J. A. Johnsen",
      "J. Jurysek",
      "K. -H. Kampert",
      "B. Keilhauer",
      "A. Khakurdikar",
      "V. V. Kizakke Covilakam",
      "H. O. Klages",
      "M. Kleifges",
      "F. Knapp",
      "J. K\u00f6hler",
      "F. Krieger",
      "N. Kunka",
      "B. L. Lago",
      "N. Langner",
      "M. A. Leigui de Oliveira",
      "Y. Lema-Capeans",
      "A. Letessier-Selvon",
      "I. Lhenry-Yvon",
      "L. Lopes",
      "L. Lu",
      "Q. Luce",
      "J. P. Lundquist",
      "A. Machado Payeras",
      "M. Majercakova",
      "D. Mandat",
      "B. C. Manning",
      "P. Mantsch",
      "F. M. Mariani",
      "A. G. Mariazzi",
      "I. C. Mari\u015f",
      "G. Marsella",
      "D. Martello",
      "S. Martinelli",
      "O. Mart\u00ednez Bravo",
      "M. A. Martins",
      "H. -J. Mathes",
      "J. Matthews",
      "G. Matthiae",
      "E. Mayotte",
      "S. Mayotte",
      "P. O. Mazur",
      "G. Medina-Tanco",
      "J. Meinert",
      "D. Melo",
      "A. Menshikov",
      "C. Merx",
      "S. Michal",
      "M. I. Micheletti",
      "L. Miramonti",
      "S. Mollerach",
      "F. Montanet",
      "L. Morejon",
      "K. Mulrey",
      "R. Mussa",
      "W. M. Namasaka",
      "S. Negi",
      "L. Nellen",
      "K. Nguyen",
      "G. Nicora",
      "M. Niechciol",
      "D. Nitz",
      "D. Nosek",
      "V. Novotny",
      "L. No\u017eka",
      "A. Nucita",
      "L. A. N\u00fa\u00f1ez",
      "C. Oliveira",
      "M. Palatka",
      "J. Pallotta",
      "S. Panja",
      "G. Parente",
      "T. Paulsen",
      "J. Pawlowsky",
      "M. Pech",
      "J. P\u0119kala",
      "R. Pelayo",
      "V. Pelgrims",
      "L. A. S. Pereira",
      "E. E. Pereira Martins",
      "C. P\u00e9rez Bertolli",
      "L. Perrone",
      "S. Petrera",
      "C. Petrucci",
      "T. Pierog",
      "M. Pimenta",
      "M. Platino",
      "B. Pont",
      "M. Pothast",
      "M. Pourmohammad Shahvar",
      "P. Privitera",
      "M. Prouza",
      "S. Querchfeld",
      "J. Rautenberg",
      "D. Ravignani",
      "J. V. Reginatto Akim",
      "M. Reininghaus",
      "A. Reuzki",
      "J. Ridky",
      "F. Riehn",
      "M. Risse",
      "V. Rizi",
      "W. Rodrigues de Carvalho",
      "E. Rodriguez",
      "J. Rodriguez Rojo",
      "M. J. Roncoroni",
      "S. Rossoni",
      "M. Roth",
      "E. Roulet",
      "A. C. Rovero",
      "A. Saftoiu",
      "M. Saharan",
      "F. Salamida",
      "H. Salazar",
      "G. Salina",
      "J. D. Sanabria Gomez",
      "F. S\u00e1nchez",
      "E. M. Santos",
      "E. Santos",
      "F. Sarazin",
      "R. Sarmento",
      "R. Sato",
      "P. Savina",
      "C. M. Sch\u00e4fer",
      "V. Scherini",
      "H. Schieler",
      "M. Schimassek",
      "M. Schimp",
      "D. Schmidt",
      "O. Scholten",
      "H. Schoorlemmer",
      "P. Schov\u00e1nek",
      "F. G. Schr\u00f6der",
      "J. Schulte",
      "T. Schulz",
      "S. J. Sciutto",
      "M. Scornavacche",
      "A. Sedoski",
      "A. Segreto",
      "S. Sehgal",
      "S. U. Shivashankara",
      "G. Sigl",
      "K. Simkova",
      "F. Simon",
      "R. Smau",
      "R. \u0160m\u00edda",
      "P. Sommers",
      "R. Squartini",
      "M. Stadelmaier",
      "S. Stani\u010d",
      "J. Stasielak",
      "P. Stassi",
      "S. Str\u00e4hnz",
      "M. Straub",
      "T. Suomij\u00e4rvi",
      "A. D. Supanitsky",
      "Z. Svozilikova",
      "Z. Szadkowski",
      "F. Tairli",
      "A. Tapia",
      "C. Taricco",
      "C. Timmermans",
      "O. Tkachenko",
      "P. Tobiska",
      "C. J. Todero Peixoto",
      "B. Tom\u00e9",
      "Z. Torr\u00e8s",
      "A. Travaini",
      "P. Travnicek",
      "M. Tueros",
      "M. Unger",
      "R. Uzeiroska",
      "L. Vaclavek",
      "M. Vacula",
      "J. F. Vald\u00e9s Galicia",
      "L. Valore",
      "E. Varela",
      "V. Va\u0161\u00ed\u010dkov\u00e1",
      "A. V\u00e1squez-Ram\u00edrez",
      "D. Veberi\u010d",
      "I. D. Vergara Quispe",
      "V. Verzi",
      "J. Vicha",
      "J. Vink",
      "S. Vorobiov",
      "C. Watanabe",
      "A. A. Watson",
      "A. Weindl",
      "L. Wiencke",
      "H. Wilczy\u0144ski",
      "D. Wittkowski",
      "B. Wundheiler",
      "B. Yue",
      "A. Yushkov",
      "O. Zapparrata",
      "E. Zas",
      "D. Zavrtanik",
      "M. Zavrtanik"
    ],
    "url": "http://arxiv.org/abs/2406.06319v1",
    "timestamp": 1718030078,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.HE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "89aa81e2-1837-4d75-bbe4-7d12a022fbc7": {
    "pk": "89aa81e2-1837-4d75-bbe4-7d12a022fbc7",
    "title": "Should my Blockchain Learn to Drive? A Study of Hyperledger Fabric",
    "abstract": "Similar to other transaction processing frameworks, blockchain systems need to be dynamically reconfigured to adapt to varying workloads and changes in network conditions. However, achieving optimal reconfiguration is particularly challenging due to the complexity of the blockchain stack, which has diverse configurable parameters. This paper explores the concept of self-driving blockchains, which have the potential to predict workload changes and reconfigure themselves for optimal performance without human intervention. We compare and contrast our discussions with existing research on databases and highlight aspects unique to blockchains. We identify specific parameters and components in Hyperledger Fabric, a popular permissioned blockchain system, that are suitable for autonomous adaptation and offer potential solutions for the challenges involved. Further, we implement three demonstrative locally autonomous systems, each targeting a different layer of the blockchain stack, and conduct experiments to understand the feasibility of our findings. Our experiments indicate up to 11% improvement in success throughput and a 30% decrease in latency, making this a significant step towards implementing a fully autonomous blockchain system in the future.",
    "authors": [
      "Jeeta Ann Chacko",
      "Ruben Mayer",
      "Hans-Arno Jacobsen"
    ],
    "url": "http://arxiv.org/abs/2406.06318v1",
    "timestamp": 1718030039,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1aa7d5de-1454-46ce-b3b7-5199f6d03b38": {
    "pk": "1aa7d5de-1454-46ce-b3b7-5199f6d03b38",
    "title": "Tx-LLM: A Large Language Model for Therapeutics",
    "abstract": "Developing therapeutics is a lengthy and expensive process that requires the satisfaction of many different criteria, and AI models capable of expediting the process would be invaluable. However, the majority of current AI approaches address only a narrowly defined set of tasks, often circumscribed within a particular domain. To bridge this gap, we introduce Tx-LLM, a generalist large language model (LLM) fine-tuned from PaLM-2 which encodes knowledge about diverse therapeutic modalities. Tx-LLM is trained using a collection of 709 datasets that target 66 tasks spanning various stages of the drug discovery pipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide variety of chemical or biological entities(small molecules, proteins, nucleic acids, cell lines, diseases) interleaved with free-text, allowing it to predict a broad range of associated properties, achieving competitive with state-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on 22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class performance on average for tasks combining molecular SMILES representations with text such as cell line names or disease names, likely due to context learned during pretraining. We observe evidence of positive transfer between tasks with diverse drug types (e.g.,tasks involving small molecules and tasks involving proteins), and we study the impact of model size, domain finetuning, and prompting strategies on performance. We believe Tx-LLM represents an important step towards LLMs encoding biochemical knowledge and could have a future role as an end-to-end tool across the drug discovery development pipeline.",
    "authors": [
      "Juan Manuel Zambrano Chaves",
      "Eric Wang",
      "Tao Tu",
      "Eeshit Dhaval Vaishnav",
      "Byron Lee",
      "S. Sara Mahdavi",
      "Christopher Semturs",
      "David Fleet",
      "Vivek Natarajan",
      "Shekoofeh Azizi"
    ],
    "url": "http://arxiv.org/abs/2406.06316v1",
    "timestamp": 1718029982,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e5f2ae07-edb3-4224-b104-1d8ea4638a60": {
    "pk": "e5f2ae07-edb3-4224-b104-1d8ea4638a60",
    "title": "Inference of the Mass Composition of Cosmic Rays with energies from $\\mathbf{10^{18.5}}$ to $\\mathbf{10^{20}}$ eV using the Pierre Auger Observatory and Deep Learning",
    "abstract": "We present measurements of the atmospheric depth of the shower maximum $X_\\mathrm{max}$, inferred for the first time on an event-by-event level using the Surface Detector of the Pierre Auger Observatory. Using deep learning, we were able to extend measurements of the $X_\\mathrm{max}$ distributions up to energies of 100 EeV ($10^{20}$ eV), not yet revealed by current measurements, providing new insights into the mass composition of cosmic rays at extreme energies. Gaining a 10-fold increase in statistics compared to the Fluorescence Detector data, we find evidence that the rate of change of the average $X_\\mathrm{max}$ with the logarithm of energy features three breaks at $6.5\\pm0.6~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, $11\\pm 2~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, and $31\\pm5~(\\mathrm{stat})\\pm3~(\\mathrm{sys})$ EeV, in the vicinity to the three prominent features (ankle, instep, suppression) of the cosmic-ray flux. The energy evolution of the mean and standard deviation of the measured $X_\\mathrm{max}$ distributions indicates that the mass composition becomes increasingly heavier and purer, thus being incompatible with a large fraction of light nuclei between 50 EeV and 100 EeV.",
    "authors": [
      "The Pierre Auger Collaboration",
      "A. Abdul Halim",
      "P. Abreu",
      "M. Aglietta",
      "I. Allekotte",
      "K. Almeida Cheminant",
      "A. Almela",
      "R. Aloisio",
      "J. Alvarez-Mu\u00f1iz",
      "J. Ammerman Yebra",
      "G. A. Anastasi",
      "L. Anchordoqui",
      "B. Andrada",
      "L. Andrade Dourado",
      "S. Andringa",
      "L. Apollonio",
      "C. Aramo",
      "P. R. Ara\u00fajo Ferreira",
      "E. Arnone",
      "J. C. Arteaga Vel\u00e1zquez",
      "P. Assis",
      "G. Avila",
      "E. Avocone",
      "A. Bakalova",
      "F. Barbato",
      "A. Bartz Mocellin",
      "C. Berat",
      "M. E. Bertaina",
      "G. Bhatta",
      "M. Bianciotto",
      "P. L. Biermann",
      "V. Binet",
      "K. Bismark",
      "T. Bister",
      "J. Biteau",
      "J. Blazek",
      "C. Bleve",
      "J. Bl\u00fcmer",
      "M. Boh\u00e1\u010dov\u00e1",
      "D. Boncioli",
      "C. Bonifazi",
      "L. Bonneau Arbeletche",
      "N. Borodai",
      "J. Brack",
      "P. G. Brichetto Orchera",
      "F. L. Briechle",
      "A. Bueno",
      "S. Buitink",
      "M. Buscemi",
      "M. B\u00fcsken",
      "A. Bwembya",
      "K. S. Caballero-Mora",
      "S. Cabana-Freire",
      "L. Caccianiga",
      "F. Campuzano",
      "R. Caruso",
      "A. Castellina",
      "F. Catalani",
      "G. Cataldi",
      "L. Cazon",
      "M. Cerda",
      "B. \u010cerm\u00e1kov\u00e1",
      "A. Cermenati",
      "J. A. Chinellato",
      "J. Chudoba",
      "L. Chytka",
      "R. W. Clay",
      "A. C. Cobos Cerutti",
      "R. Colalillo",
      "M. R. Coluccia",
      "R. Concei\u00e7\u00e3o",
      "A. Condorelli",
      "G. Consolati",
      "M. Conte",
      "F. Convenga",
      "D. Correia dos Santos",
      "P. J. Costa",
      "C. E. Covault",
      "M. Cristinziani",
      "C. S. Cruz Sanchez",
      "S. Dasso",
      "K. Daumiller",
      "B. R. Dawson",
      "R. M. de Almeida",
      "B. de Errico",
      "J. de Jes\u00fas",
      "S. J. de Jong",
      "J. R. T. de Mello Neto",
      "I. De Mitri",
      "J. de Oliveira",
      "D. de Oliveira Franco",
      "F. de Palma",
      "V. de Souza",
      "E. De Vito",
      "A. Del Popolo",
      "O. Deligny",
      "N. Denner",
      "L. Deval",
      "A. di Matteo",
      "J. A. do",
      "M. Dobre",
      "C. Dobrigkeit",
      "J. C. D'Olivo",
      "L. M. Domingues Mendes",
      "Q. Dorosti",
      "J. C. dos Anjos",
      "R. C. dos Anjos",
      "J. Ebr",
      "F. Ellwanger",
      "M. Emam",
      "R. Engel",
      "I. Epicoco",
      "M. Erdmann",
      "A. Etchegoyen",
      "C. Evoli",
      "H. Falcke",
      "G. Farrar",
      "A. C. Fauth",
      "T. Fehler",
      "F. Feldbusch",
      "F. Fenu",
      "A. Fernandes",
      "B. Fick",
      "J. M. Figueira",
      "P. Filip",
      "A. Filip\u010di\u010d",
      "T. Fitoussi",
      "B. Flaggs",
      "T. Fodran",
      "T. Fujii",
      "A. Fuster",
      "C. Galea",
      "B. Garc\u00eda",
      "C. Gaudu",
      "A. Gherghel-Lascu",
      "P. L. Ghia",
      "U. Giaccari",
      "J. Glombitza",
      "F. Gobbi",
      "F. Gollan",
      "G. Golup",
      "M. G\u00f3mez Berisso",
      "P. F. G\u00f3mez Vitale",
      "J. P. Gongora",
      "J. M. Gonz\u00e1lez",
      "N. Gonz\u00e1lez",
      "D. G\u00f3ra",
      "A. Gorgi",
      "M. Gottowik",
      "F. Guarino",
      "G. P. Guedes",
      "E. Guido",
      "L. G\u00fclzow",
      "S. Hahn",
      "P. Hamal",
      "M. R. Hampel",
      "P. Hansen",
      "D. Harari",
      "V. M. Harvey",
      "A. Haungs",
      "T. Hebbeker",
      "C. Hojvat",
      "J. R. H\u00f6randel",
      "P. Horvath",
      "M. Hrabovsk\u00fd",
      "T. Huege",
      "A. Insolia",
      "P. G. Isar",
      "P. Janecek",
      "V. Jilek",
      "J. A. Johnsen",
      "J. Jurysek",
      "K. -H. Kampert",
      "B. Keilhauer",
      "A. Khakurdikar",
      "V. V. Kizakke Covilakam",
      "H. O. Klages",
      "M. Kleifges",
      "F. Knapp",
      "J. K\u00f6hler",
      "F. Krieger",
      "N. Kunka",
      "B. L. Lago",
      "N. Langner",
      "M. A. Leigui de Oliveira",
      "Y. Lema-Capeans",
      "A. Letessier-Selvon",
      "I. Lhenry-Yvon",
      "L. Lopes",
      "L. Lu",
      "Q. Luce",
      "J. P. Lundquist",
      "A. Machado Payeras",
      "M. Majercakova",
      "D. Mandat",
      "B. C. Manning",
      "P. Mantsch",
      "F. M. Mariani",
      "A. G. Mariazzi",
      "I. C. Mari\u015f",
      "G. Marsella",
      "D. Martello",
      "S. Martinelli",
      "O. Mart\u00ednez Bravo",
      "M. A. Martins",
      "H. -J. Mathes",
      "J. Matthews",
      "G. Matthiae",
      "E. Mayotte",
      "S. Mayotte",
      "P. O. Mazur",
      "G. Medina-Tanco",
      "J. Meinert",
      "D. Melo",
      "A. Menshikov",
      "C. Merx",
      "S. Michal",
      "M. I. Micheletti",
      "L. Miramonti",
      "S. Mollerach",
      "F. Montanet",
      "L. Morejon",
      "K. Mulrey",
      "R. Mussa",
      "W. M. Namasaka",
      "S. Negi",
      "L. Nellen",
      "K. Nguyen",
      "G. Nicora",
      "M. Niechciol",
      "D. Nitz",
      "D. Nosek",
      "V. Novotny",
      "L. No\u017eka",
      "A. Nucita",
      "L. A. N\u00fa\u00f1ez",
      "C. Oliveira",
      "M. Palatka",
      "J. Pallotta",
      "S. Panja",
      "G. Parente",
      "T. Paulsen",
      "J. Pawlowsky",
      "M. Pech",
      "J. P\u0119kala",
      "R. Pelayo",
      "V. Pelgrims",
      "L. A. S. Pereira",
      "E. E. Pereira Martins",
      "C. P\u00e9rez Bertolli",
      "L. Perrone",
      "S. Petrera",
      "C. Petrucci",
      "T. Pierog",
      "M. Pimenta",
      "M. Platino",
      "B. Pont",
      "M. Pothast",
      "M. Pourmohammad Shahvar",
      "P. Privitera",
      "M. Prouza",
      "S. Querchfeld",
      "J. Rautenberg",
      "D. Ravignani",
      "J. V. Reginatto Akim",
      "M. Reininghaus",
      "A. Reuzki",
      "J. Ridky",
      "F. Riehn",
      "M. Risse",
      "V. Rizi",
      "W. Rodrigues de Carvalho",
      "E. Rodriguez",
      "J. Rodriguez Rojo",
      "M. J. Roncoroni",
      "S. Rossoni",
      "M. Roth",
      "E. Roulet",
      "A. C. Rovero",
      "A. Saftoiu",
      "M. Saharan",
      "F. Salamida",
      "H. Salazar",
      "G. Salina",
      "J. D. Sanabria Gomez",
      "F. S\u00e1nchez",
      "E. M. Santos",
      "E. Santos",
      "F. Sarazin",
      "R. Sarmento",
      "R. Sato",
      "P. Savina",
      "C. M. Sch\u00e4fer",
      "V. Scherini",
      "H. Schieler",
      "M. Schimassek",
      "M. Schimp",
      "D. Schmidt",
      "O. Scholten",
      "H. Schoorlemmer",
      "P. Schov\u00e1nek",
      "F. G. Schr\u00f6der",
      "J. Schulte",
      "T. Schulz",
      "S. J. Sciutto",
      "M. Scornavacche",
      "A. Sedoski",
      "A. Segreto",
      "S. Sehgal",
      "S. U. Shivashankara",
      "G. Sigl",
      "K. Simkova",
      "F. Simon",
      "R. Smau",
      "R. \u0160m\u00edda",
      "P. Sommers",
      "R. Squartini",
      "M. Stadelmaier",
      "S. Stani\u010d",
      "J. Stasielak",
      "P. Stassi",
      "S. Str\u00e4hnz",
      "M. Straub",
      "T. Suomij\u00e4rvi",
      "A. D. Supanitsky",
      "Z. Svozilikova",
      "Z. Szadkowski",
      "F. Tairli",
      "A. Tapia",
      "C. Taricco",
      "C. Timmermans",
      "O. Tkachenko",
      "P. Tobiska",
      "C. J. Todero Peixoto",
      "B. Tom\u00e9",
      "Z. Torr\u00e8s",
      "A. Travaini",
      "P. Travnicek",
      "M. Tueros",
      "M. Unger",
      "R. Uzeiroska",
      "L. Vaclavek",
      "M. Vacula",
      "J. F. Vald\u00e9s Galicia",
      "L. Valore",
      "E. Varela",
      "V. Va\u0161\u00ed\u010dkov\u00e1",
      "A. V\u00e1squez-Ram\u00edrez",
      "D. Veberi\u010d",
      "I. D. Vergara Quispe",
      "V. Verzi",
      "J. Vicha",
      "J. Vink",
      "S. Vorobiov",
      "C. Watanabe",
      "A. A. Watson",
      "A. Weindl",
      "L. Wiencke",
      "H. Wilczy\u0144ski",
      "D. Wittkowski",
      "B. Wundheiler",
      "B. Yue",
      "A. Yushkov",
      "O. Zapparrata",
      "E. Zas",
      "D. Zavrtanik",
      "M. Zavrtanik"
    ],
    "url": "http://arxiv.org/abs/2406.06315v1",
    "timestamp": 1718029930,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.HE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a69b0a08-f650-4716-83cc-4157e00db295": {
    "pk": "a69b0a08-f650-4716-83cc-4157e00db295",
    "title": "ProAct: Progressive Training for Hybrid Clipped Activation Function to Enhance Resilience of DNNs",
    "abstract": "Deep Neural Networks (DNNs) are extensively employed in safety-critical applications where ensuring hardware reliability is a primary concern. To enhance the reliability of DNNs against hardware faults, activation restriction techniques significantly mitigate the fault effects at the DNN structure level, irrespective of accelerator architectures. State-of-the-art methods offer either neuron-wise or layer-wise clipping activation functions. They attempt to determine optimal clipping thresholds using heuristic and learning-based approaches. Layer-wise clipped activation functions cannot preserve DNNs resilience at high bit error rates. On the other hand, neuron-wise clipping activation functions introduce considerable memory overhead due to the addition of parameters, which increases their vulnerability to faults. Moreover, the heuristic-based optimization approach demands numerous fault injections during the search process, resulting in time-consuming threshold identification. On the other hand, learning-based techniques that train thresholds for entire layers concurrently often yield sub-optimal results. In this work, first, we demonstrate that it is not essential to incorporate neuron-wise activation functions throughout all layers in DNNs. Then, we propose a hybrid clipped activation function that integrates neuron-wise and layer-wise methods that apply neuron-wise clipping only in the last layer of DNNs. Additionally, to attain optimal thresholds in the clipping activation function, we introduce ProAct, a progressive training methodology. This approach iteratively trains the thresholds on a layer-by-layer basis, aiming to obtain optimal threshold values in each layer separately.",
    "authors": [
      "Seyedhamidreza Mousavi",
      "Mohammad Hasan Ahmadilivani",
      "Jaan Raik",
      "Maksim Jenihhin",
      "Masoud Daneshtalab"
    ],
    "url": "http://arxiv.org/abs/2406.06313v1",
    "timestamp": 1718029898,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0ec900a4-9dfe-443f-99ba-3e483a635931": {
    "pk": "0ec900a4-9dfe-443f-99ba-3e483a635931",
    "title": "Is Value Functions Estimation with Classification Plug-and-play for Offline Reinforcement Learning?",
    "abstract": "In deep Reinforcement Learning (RL), value functions are typically approximated using deep neural networks and trained via mean squared error regression objectives to fit the true value functions. Recent research has proposed an alternative approach, utilizing the cross-entropy classification objective, which has demonstrated improved performance and scalability of RL algorithms. However, existing study have not extensively benchmarked the effects of this replacement across various domains, as the primary objective was to demonstrate the efficacy of the concept across a broad spectrum of tasks, without delving into in-depth analysis. Our work seeks to empirically investigate the impact of such a replacement in an offline RL setup and analyze the effects of different aspects on performance. Through large-scale experiments conducted across a diverse range of tasks using different algorithms, we aim to gain deeper insights into the implications of this approach. Our results reveal that incorporating this change can lead to superior performance over state-of-the-art solutions for some algorithms in certain tasks, while maintaining comparable performance levels in other tasks, however for other algorithms this modification might lead to the dramatic performance drop. This findings are crucial for further application of classification approach in research and practical tasks.",
    "authors": [
      "Denis Tarasov",
      "Kirill Brilliantov",
      "Dmitrii Kharlapenko"
    ],
    "url": "http://arxiv.org/abs/2406.06309v1",
    "timestamp": 1718029511,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "69e55c0e-e8e4-4d23-b71a-c952006fce3c": {
    "pk": "69e55c0e-e8e4-4d23-b71a-c952006fce3c",
    "title": "Building Continuous Quantum-Classical Bayesian Neural Networks for a Classical Clinical Dataset",
    "abstract": "In this work, we are introducing a Quantum-Classical Bayesian Neural Network (QCBNN) that is capable to perform uncertainty-aware classification of classical medical dataset. This model is a symbiosis of a classical Convolutional NN that performs ultra-sound image processing and a quantum circuit that generates its stochastic weights, within a Bayesian learning framework. To test the utility of this idea for the possible future deployment in the medical sector we track multiple behavioral metrics that capture both predictive performance as well as model's uncertainty. It is our ambition to create a hybrid model that is capable to classify samples in a more uncertainty aware fashion, which will advance the trustworthiness of these models and thus bring us step closer to utilizing them in the industry. We test multiple setups for quantum circuit for this task, and our best architectures display bigger uncertainty gap between correctly and incorrectly identified samples than its classical benchmark at an expense of a slight drop in predictive performance. The innovation of this paper is two-fold: (1) combining of different approaches that allow the stochastic weights from the quantum circuit to be continues thus allowing the model to classify application-driven dataset; (2) studying architectural features of quantum circuit that make-or-break these models, which pave the way into further investigation of more informed architectural designs.",
    "authors": [
      "Alona Sakhnenko",
      "Julian Sikora",
      "Jeanette Miriam Lorenz"
    ],
    "url": "http://arxiv.org/abs/2406.06307v1",
    "timestamp": 1718029405,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6de436b5-0982-461d-babb-13939dfa7c87": {
    "pk": "6de436b5-0982-461d-babb-13939dfa7c87",
    "title": "NeuroMoCo: A Neuromorphic Momentum Contrast Learning Method for Spiking Neural Networks",
    "abstract": "Recently, brain-inspired spiking neural networks (SNNs) have attracted great research attention owing to their inherent bio-interpretability, event-triggered properties and powerful perception of spatiotemporal information, which is beneficial to handling event-based neuromorphic datasets. In contrast to conventional static image datasets, event-based neuromorphic datasets present heightened complexity in feature extraction due to their distinctive time series and sparsity characteristics, which influences their classification accuracy. To overcome this challenge, a novel approach termed Neuromorphic Momentum Contrast Learning (NeuroMoCo) for SNNs is introduced in this paper by extending the benefits of self-supervised pre-training to SNNs to effectively stimulate their potential. This is the first time that self-supervised learning (SSL) based on momentum contrastive learning is realized in SNNs. In addition, we devise a novel loss function named MixInfoNCE tailored to their temporal characteristics to further increase the classification accuracy of neuromorphic datasets, which is verified through rigorous ablation experiments. Finally, experiments on DVS-CIFAR10, DVS128Gesture and N-Caltech101 have shown that NeuroMoCo of this paper establishes new state-of-the-art (SOTA) benchmarks: 83.6% (Spikformer-2-256), 98.62% (Spikformer-2-256), and 84.4% (SEW-ResNet-18), respectively.",
    "authors": [
      "Yuqi Ma",
      "Huamin Wang",
      "Hangchi Shen",
      "Xuemei Chen",
      "Shukai Duan",
      "Shiping Wen"
    ],
    "url": "http://arxiv.org/abs/2406.06305v1",
    "timestamp": 1718029248,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "59e432f8-b92a-41a6-a9bf-4d05e60d4df1": {
    "pk": "59e432f8-b92a-41a6-a9bf-4d05e60d4df1",
    "title": "Learning-based cognitive architecture for enhancing coordination in human groups",
    "abstract": "As interactions with autonomous agents-ranging from robots in physical settings to avatars in virtual and augmented realities-become more prevalent, developing advanced cognitive architectures is critical for enhancing the dynamics of human-avatar groups. This paper presents a reinforcement-learning-based cognitive architecture, trained via a sim-to-real approach, designed to improve synchronization in periodic motor tasks, crucial for applications in group rehabilitation and sports training. Extensive numerical validation consistently demonstrates improvements in synchronization. Theoretical derivations and numerical investigations are complemented by preliminary experiments with real participants, showing that our avatars can integrate seamlessly into human groups, often being indistinguishable from humans.",
    "authors": [
      "Antonio Grotta",
      "Marco Coraggio",
      "Antonio Spallone",
      "Francesco De Lellis",
      "Mario di Bernardo"
    ],
    "url": "http://arxiv.org/abs/2406.06297v1",
    "timestamp": 1718029046,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1077585f-a3a7-460d-afed-58526bd6f88a": {
    "pk": "1077585f-a3a7-460d-afed-58526bd6f88a",
    "title": "Sample Rate Independent Recurrent Neural Networks for Audio Effects Processing",
    "abstract": "In recent years, machine learning approaches to modelling guitar amplifiers and effects pedals have been widely investigated and have become standard practice in some consumer products. In particular, recurrent neural networks (RNNs) are a popular choice for modelling non-linear devices such as vacuum tube amplifiers and distortion circuitry. One limitation of such models is that they are trained on audio at a specific sample rate and therefore give unreliable results when operating at another rate. Here, we investigate several methods of modifying RNN structures to make them approximately sample rate independent, with a focus on oversampling. In the case of integer oversampling, we demonstrate that a previously proposed delay-based approach provides high fidelity sample rate conversion whilst additionally reducing aliasing. For non-integer sample rate adjustment, we propose two novel methods and show that one of these, based on cubic Lagrange interpolation of a delay-line, provides a significant improvement over existing methods. To our knowledge, this work provides the first in-depth study into this problem.",
    "authors": [
      "Alistair Carson",
      "Alec Wright",
      "Jatin Chowdhury",
      "Vesa V\u00e4lim\u00e4ki",
      "Stefan Bilbao"
    ],
    "url": "http://arxiv.org/abs/2406.06293v1",
    "timestamp": 1718028863,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.AS",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1b901314-e9fb-44e8-9542-dd5f933e01a4": {
    "pk": "1b901314-e9fb-44e8-9542-dd5f933e01a4",
    "title": "Geometric sparsification in recurrent neural networks",
    "abstract": "A common technique for ameliorating the computational costs of running large neural models is sparsification, or the removal of neural connections during training. Sparse models are capable of maintaining the high accuracy of state of the art models, while functioning at the cost of more parsimonious models. The structures which underlie sparse architectures are, however, poorly understood and not consistent between differently trained models and sparsification schemes. In this paper, we propose a new technique for sparsification of recurrent neural nets (RNNs), called moduli regularization, in combination with magnitude pruning. Moduli regularization leverages the dynamical system induced by the recurrent structure to induce a geometric relationship between neurons in the hidden state of the RNN. By making our regularizing term explicitly geometric, we provide the first, to our knowledge, a priori description of the desired sparse architecture of our neural net. We verify the effectiveness of our scheme for navigation and natural language processing RNNs. Navigation is a structurally geometric task, for which there are known moduli spaces, and we show that regularization can be used to reach 90% sparsity while maintaining model performance only when coefficients are chosen in accordance with a suitable moduli space. Natural language processing, however, has no known moduli space in which computations are performed. Nevertheless, we show that moduli regularization induces more stable recurrent neural nets with a variety of moduli regularizers, and achieves high fidelity models at 98% sparsity.",
    "authors": [
      "Wyatt Mackey",
      "Ioannis Schizas",
      "Jared Deighton",
      "David L. Boothe, Jr.",
      "Vasileios Maroulas"
    ],
    "url": "http://arxiv.org/abs/2406.06290v1",
    "timestamp": 1718028753,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2885eb8d-7138-4370-b9a8-c49154d9ede0": {
    "pk": "2885eb8d-7138-4370-b9a8-c49154d9ede0",
    "title": "VS-PINN: A Fast and efficient training of physics-informed neural networks using variable-scaling methods for solving PDEs with stiff behavior",
    "abstract": "Physics-informed neural networks (PINNs) have recently emerged as a promising way to compute the solutions of partial differential equations (PDEs) using deep neural networks. However, despite their significant success in various fields, it remains unclear in many aspects how to effectively train PINNs if the solutions of PDEs exhibit stiff behaviors or high frequencies. In this paper, we propose a new method for training PINNs using variable-scaling techniques. This method is simple and it can be applied to a wide range of problems including PDEs with rapidly-varying solutions. Throughout various numerical experiments, we will demonstrate the effectiveness of the proposed method for these problems and confirm that it can significantly improve the training efficiency and performance of PINNs. Furthermore, based on the analysis of the neural tangent kernel (NTK), we will provide theoretical evidence for this phenomenon and show that our methods can indeed improve the performance of PINNs.",
    "authors": [
      "Seungchan Ko",
      "Sang Hyeon Park"
    ],
    "url": "http://arxiv.org/abs/2406.06287v1",
    "timestamp": 1718028675,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.NA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "77347eac-48fe-4979-8b49-5a7172c68451": {
    "pk": "77347eac-48fe-4979-8b49-5a7172c68451",
    "title": "Holographic complex potential of a quarkonium from deep learning",
    "abstract": "Utilizing an emergent metric developed from deep learning techniques, we determine the complex potential associated with static quarkonium. This study explores the disintegration process of quarkonium by analyzing the real component of this potential, which is crucial for understanding its stability in various conditions. We show that the dissociation length, the critical distance at which a quark and antiquark pair disintegrate, decreases as the temperature increases. Furthermore, our assessment of the imaginary component of the potential indicates an increase in the magnitude of the imaginary potential for quarkonium as temperatures rise. This enhancement contributes to the quarkonium's suppression within the quark-gluon plasma, mirroring the anticipated outcomes from QCD. Our findings not only confirm the theoretical predictions but also demonstrate the efficacy of deep learning methods in advancing our understanding of high-energy particle physics.",
    "authors": [
      "Mahdi Mansouri",
      "Kazem Bitaghsir Fadafan",
      "Xun Chen"
    ],
    "url": "http://arxiv.org/abs/2406.06285v1",
    "timestamp": 1718028561,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "hep-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a85c5164-1d60-4a98-9686-a5a9f86a22cf": {
    "pk": "a85c5164-1d60-4a98-9686-a5a9f86a22cf",
    "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
    "abstract": "This paper introduces PowerInfer-2, a framework designed for high-speed inference of Large Language Models (LLMs) on smartphones, particularly effective for models whose sizes exceed the device's memory capacity. The key insight of PowerInfer-2 is to utilize the heterogeneous computation, memory, and I/O resources in smartphones by decomposing traditional matrix computations into fine-grained neuron cluster computations. Specifically, PowerInfer-2 features a polymorphic neuron engine that adapts computational strategies for various stages of LLM inference. Additionally, it introduces segmented neuron caching and fine-grained neuron-cluster-level pipelining, which effectively minimize and conceal the overhead caused by I/O operations. The implementation and evaluation of PowerInfer-2 demonstrate its capability to support a wide array of LLM models on two smartphones, achieving up to a 29.2x speed increase compared with state-of-the-art frameworks. Notably, PowerInfer-2 is the first system to serve the TurboSparse-Mixtral-47B model with a generation rate of 11.68 tokens per second on a smartphone. For models that fit entirely within the memory, PowerInfer-2 can achieve approximately a 40% reduction in memory usage while maintaining inference speeds comparable to llama.cpp and MLC-LLM. For more details, including a demonstration video, please visit the project site at www.powerinfer.ai/v2.",
    "authors": [
      "Zhenliang Xue",
      "Yixin Song",
      "Zeyu Mi",
      "Le Chen",
      "Yubin Xia",
      "Haibo Chen"
    ],
    "url": "http://arxiv.org/abs/2406.06282v1",
    "timestamp": 1718028081,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8c12df2b-170b-43fa-aec3-ac802fce951c": {
    "pk": "8c12df2b-170b-43fa-aec3-ac802fce951c",
    "title": "Modular Growth of Hierarchical Networks: Efficient, General, and Robust Curriculum Learning",
    "abstract": "Structural modularity is a pervasive feature of biological neural networks, which have been linked to several functional and computational advantages. Yet, the use of modular architectures in artificial neural networks has been relatively limited despite early successes. Here, we explore the performance and functional dynamics of a modular network trained on a memory task via an iterative growth curriculum. We find that for a given classical, non-modular recurrent neural network (RNN), an equivalent modular network will perform better across multiple metrics, including training time, generalizability, and robustness to some perturbations. We further examine how different aspects of a modular network's connectivity contribute to its computational capability. We then demonstrate that the inductive bias introduced by the modular topology is strong enough for the network to perform well even when the connectivity within modules is fixed and only the connections between modules are trained. Our findings suggest that gradual modular growth of RNNs could provide advantages for learning increasingly complex tasks on evolutionary timescales, and help build more scalable and compressible artificial networks.",
    "authors": [
      "Mani Hamidi",
      "Sina Khajehabdollahi",
      "Emmanouil Giannakakis",
      "Tim Sch\u00e4fer",
      "Anna Levina",
      "Charley M. Wu"
    ],
    "url": "http://arxiv.org/abs/2406.06262v1",
    "timestamp": 1718027047,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.NE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "266ec240-cb0e-4509-b442-9f7753874f65": {
    "pk": "266ec240-cb0e-4509-b442-9f7753874f65",
    "title": "Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning",
    "abstract": "As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.",
    "authors": [
      "Chung-Ming Chien",
      "Andros Tjandra",
      "Apoorv Vyas",
      "Matt Le",
      "Bowen Shi",
      "Wei-Ning Hsu"
    ],
    "url": "http://arxiv.org/abs/2406.06251v1",
    "timestamp": 1718026278,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.AS",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "47c9cd11-3a6f-4e78-aa76-812849b8524b": {
    "pk": "47c9cd11-3a6f-4e78-aa76-812849b8524b",
    "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
    "abstract": "Dense linear layers are the dominant computational bottleneck in foundation models. Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain. In this work, we systematically explore structured matrices as replacements for dense matrices. We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale. Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers. Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute. We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models.",
    "authors": [
      "Shikai Qiu",
      "Andres Potapczynski",
      "Marc Finzi",
      "Micah Goldblum",
      "Andrew Gordon Wilson"
    ],
    "url": "http://arxiv.org/abs/2406.06248v1",
    "timestamp": 1718025943,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a3839c2b-92ff-415e-ab92-0ab3f97d4bd4": {
    "pk": "a3839c2b-92ff-415e-ab92-0ab3f97d4bd4",
    "title": "Data-Efficient Learning with Neural Programs",
    "abstract": "Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites \"neural programs\" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite. When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner.",
    "authors": [
      "Alaia Solko-Breslin",
      "Seewon Choi",
      "Ziyang Li",
      "Neelay Velingker",
      "Rajeev Alur",
      "Mayur Naik",
      "Eric Wong"
    ],
    "url": "http://arxiv.org/abs/2406.06246v1",
    "timestamp": 1718025780,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "85bb60e2-469b-4a19-9c47-edbf12e2f218": {
    "pk": "85bb60e2-469b-4a19-9c47-edbf12e2f218",
    "title": "I-MPN: Inductive Message Passing Network for Effective and Efficient Human-in-the-Loop Annotation of Mobile Eye Tracking Data",
    "abstract": "Understanding human visual processing in dynamic environments is essential for psychology and human-centered interaction design. Mobile eye-tracking systems, combining egocentric video and gaze signals, offer valuable insights. However, manual analysis of these recordings is time-intensive. In this work, we present a novel human-centered learning algorithm designed for automated object recognition within mobile eye-tracking settings. Our approach seamlessly integrates an object detector with an inductive message-passing network technique (I-MPN), harnessing node features such as node profile information and positions. This integration enables our algorithm to learn embedding functions capable of generalizing to new object angle views, thereby facilitating rapid adaptation and efficient reasoning in dynamic contexts as users navigate through their environment. Through experiments conducted on three distinct video sequences, our \\textit{interactive-based method} showcases significant performance improvements over fixed training/testing algorithms, even when trained on considerably smaller annotated samples collected through user feedback. Furthermore, we showcase exceptional efficiency in data annotation processes, surpassing approaches that use complete object detectors, combine detectors with convolutional networks, or employ interactive video segmentation.",
    "authors": [
      "Hoang H. Le",
      "Duy M. H. Nguyen",
      "Omair Shahzad Bhatti",
      "Laszlo Kopacsi",
      "Thinh P. Ngo",
      "Binh T. Nguyen",
      "Michael Barz",
      "Daniel Sonntag"
    ],
    "url": "http://arxiv.org/abs/2406.06239v1",
    "timestamp": 1718024911,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "212e3b47-8138-4877-bd36-84cea27b4e99": {
    "pk": "212e3b47-8138-4877-bd36-84cea27b4e99",
    "title": "Efficient Neural Compression with Inference-time Decoding",
    "abstract": "This paper explores the combination of neural network quantization and entropy coding for memory footprint minimization. Edge deployment of quantized models is hampered by the harsh Pareto frontier of the accuracy-to-bitwidth tradeoff, causing dramatic accuracy loss below a certain bitwidth. This accuracy loss can be alleviated thanks to mixed precision quantization, allowing for more flexible bitwidth allocation. However, standard mixed precision benefits remain limited due to the 1-bit frontier, that forces each parameter to be encoded on at least 1 bit of data. This paper introduces an approach that combines mixed precision, zero-point quantization and entropy coding to push the compression boundary of Resnets beyond the 1-bit frontier with an accuracy drop below 1% on the ImageNet benchmark. From an implementation standpoint, a compact decoder architecture features reduced latency, thus allowing for inference-compatible decoding.",
    "authors": [
      "C. Metz",
      "O. Bichler",
      "A. Dupret"
    ],
    "url": "http://arxiv.org/abs/2406.06237v1",
    "timestamp": 1718024833,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e92bf8a8-b862-4692-a738-69a71813eb71": {
    "pk": "e92bf8a8-b862-4692-a738-69a71813eb71",
    "title": "PAC-Bayes Analysis for Recalibration in Classification",
    "abstract": "Nonparametric estimation with binning is widely employed in the calibration error evaluation and the recalibration of machine learning models. Recently, theoretical analyses of the bias induced by this estimation approach have been actively pursued; however, the understanding of the generalization of the calibration error to unknown data remains limited. In addition, although many recalibration algorithms have been proposed, their generalization performance lacks theoretical guarantees. To address this problem, we conduct a generalization analysis of the calibration error under the probably approximately correct (PAC) Bayes framework. This approach enables us to derive a first optimizable upper bound for the generalization error in the calibration context. We then propose a generalization-aware recalibration algorithm based on our generalization theory. Numerical experiments show that our algorithm improves the Gaussian-process-based recalibration performance on various benchmark datasets and models.",
    "authors": [
      "Masahiro Fujisawa",
      "Futoshi Futami"
    ],
    "url": "http://arxiv.org/abs/2406.06227v1",
    "timestamp": 1718023993,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2b9c684d-5d31-47b9-a42e-4fc516fb527e": {
    "pk": "2b9c684d-5d31-47b9-a42e-4fc516fb527e",
    "title": "Siren -- Advancing Cybersecurity through Deception and Adaptive Analysis",
    "abstract": "Siren represents a pioneering research effort aimed at fortifying cybersecurity through strategic integration of deception, machine learning, and proactive threat analysis. Drawing inspiration from mythical sirens, this project employs sophisticated methods to lure potential threats into controlled environments. The system features a dynamic machine learning model for real-time analysis and classification, ensuring continuous adaptability to emerging cyber threats. The architectural framework includes a link monitoring proxy, a purpose-built machine learning model for dynamic link analysis, and a honeypot enriched with simulated user interactions to intensify threat engagement. Data protection within the honeypot is fortified with probabilistic encryption. Additionally, the incorporation of simulated user activity extends the system's capacity to capture and learn from potential attackers even after user disengagement. Siren introduces a paradigm shift in cybersecurity, transforming traditional defense mechanisms into proactive systems that actively engage and learn from potential adversaries. The research strives to enhance user protection while yielding valuable insights for ongoing refinement in response to the evolving landscape of cybersecurity threats.",
    "authors": [
      "Girish Kulathumani",
      "Samruth Ananthanarayanan",
      "Ganesh Narayanan"
    ],
    "url": "http://arxiv.org/abs/2406.06225v1",
    "timestamp": 1718023669,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "dbaa0921-79fa-4e7c-bc46-e061831d3c9f": {
    "pk": "dbaa0921-79fa-4e7c-bc46-e061831d3c9f",
    "title": "Label-Looping: Highly Efficient Decoding for Transducers",
    "abstract": "This paper introduces a highly efficient greedy decoding algorithm for Transducer inference. We propose a novel data structure using CUDA tensors to represent partial hypotheses in a batch that supports parallelized hypothesis manipulations. During decoding, our algorithm maximizes GPU parallelism by adopting a nested-loop design, where the inner loop consumes all blank predictions, while non-blank predictions are handled in the outer loop. Our algorithm is general-purpose and can work with both conventional Transducers and Token-and-Duration Transducers. Experiments show that the label-looping algorithm can bring a speedup up to 2.0X compared to conventional batched decoding algorithms when using batch size 32, and can be combined with other compiler or GPU call-related techniques to bring more speedup. We will open-source our implementation to benefit the research community.",
    "authors": [
      "Vladimir Bataev",
      "Hainan Xu",
      "Daniel Galvez",
      "Vitaly Lavrukhin",
      "Boris Ginsburg"
    ],
    "url": "http://arxiv.org/abs/2406.06220v1",
    "timestamp": 1718022878,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.AS",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5634efcb-b6dc-4d6f-a388-45874c8f5c2d": {
    "pk": "5634efcb-b6dc-4d6f-a388-45874c8f5c2d",
    "title": "Completeness classes in algebraic complexity theory",
    "abstract": "The purpose of this overview is to explain the enormous impact of Les Valiant's eponymous short conference contribution from 1979 on the development of algebraic complexity.",
    "authors": [
      "Peter B\u00fcrgisser"
    ],
    "url": "http://arxiv.org/abs/2406.06217v1",
    "timestamp": 1718022802,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9096d51d-c53f-4b1b-b0fc-9356fb012c06": {
    "pk": "9096d51d-c53f-4b1b-b0fc-9356fb012c06",
    "title": "A Statistical Theory of Regularization-Based Continual Learning",
    "abstract": "We provide a statistical analysis of regularization-based continual learning on a sequence of linear regression tasks, with emphasis on how different regularization terms affect the model performance. We first derive the convergence rate for the oracle estimator obtained as if all data were available simultaneously. Next, we consider a family of generalized $\\ell_2$-regularization algorithms indexed by matrix-valued hyperparameters, which includes the minimum norm estimator and continual ridge regression as special cases. As more tasks are introduced, we derive an iterative update formula for the estimation error of generalized $\\ell_2$-regularized estimators, from which we determine the hyperparameters resulting in the optimal algorithm. Interestingly, the choice of hyperparameters can effectively balance the trade-off between forward and backward knowledge transfer and adjust for data heterogeneity. Moreover, the estimation error of the optimal algorithm is derived explicitly, which is of the same order as that of the oracle estimator. In contrast, our lower bounds for the minimum norm estimator and continual ridge regression show their suboptimality. A byproduct of our theoretical analysis is the equivalence between early stopping and generalized $\\ell_2$-regularization in continual learning, which may be of independent interest. Finally, we conduct experiments to complement our theory.",
    "authors": [
      "Xuyang Zhao",
      "Huiyuan Wang",
      "Weiran Huang",
      "Wei Lin"
    ],
    "url": "http://arxiv.org/abs/2406.06213v1",
    "timestamp": 1718022313,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "63a59ca6-a4b1-40b2-966b-8bd388123bad": {
    "pk": "63a59ca6-a4b1-40b2-966b-8bd388123bad",
    "title": "Quantum Architecture Search: A Survey",
    "abstract": "Quantum computing has made significant progress in recent years, attracting immense interest not only in research laboratories but also in various industries. However, the application of quantum computing to solve real-world problems is still hampered by a number of challenges, including hardware limitations and a relatively under-explored landscape of quantum algorithms, especially when compared to the extensive development of classical computing. The design of quantum circuits, in particular parameterized quantum circuits (PQCs), which contain learnable parameters optimized by classical methods, is a non-trivial and time-consuming task requiring expert knowledge. As a result, research on the automated generation of PQCs, known as quantum architecture search (QAS), has gained considerable interest. QAS focuses on the use of machine learning and optimization-driven techniques to generate PQCs tailored to specific problems and characteristics of quantum hardware. In this paper, we provide an overview of QAS methods by examining relevant research studies in the field. We discuss main challenges in designing and performing an automated search for an optimal PQC, and survey ways to address them to ease future research.",
    "authors": [
      "Darya Martyniuk",
      "Johannes Jung",
      "Adrian Paschke"
    ],
    "url": "http://arxiv.org/abs/2406.06210v1",
    "timestamp": 1718021866,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8ed75398-1ea0-4dca-9cbd-3c01a0a9d407": {
    "pk": "8ed75398-1ea0-4dca-9cbd-3c01a0a9d407",
    "title": "Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning",
    "abstract": "Federated Learning (FL) is a collaborative machine learning technique where multiple clients work together with a central server to train a global model without sharing their private data. However, the distribution shift across non-IID datasets of clients poses a challenge to this one-model-fits-all method hindering the ability of the global model to effectively adapt to each client's unique local data. To echo this challenge, personalized FL (PFL) is designed to allow each client to create personalized local models tailored to their private data. While extensive research has scrutinized backdoor risks in FL, it has remained underexplored in PFL applications. In this study, we delve deep into the vulnerabilities of PFL to backdoor attacks. Our analysis showcases a tale of two cities. On the one hand, the personalization process in PFL can dilute the backdoor poisoning effects injected into the personalized local models. Furthermore, PFL systems can also deploy both server-end and client-end defense mechanisms to strengthen the barrier against backdoor attacks. On the other hand, our study shows that PFL fortified with these defense methods may offer a false sense of security. We propose \\textit{PFedBA}, a stealthy and effective backdoor attack strategy applicable to PFL systems. \\textit{PFedBA} ingeniously aligns the backdoor learning task with the main learning task of PFL by optimizing the trigger generation process. Our comprehensive experiments demonstrate the effectiveness of \\textit{PFedBA} in seamlessly embedding triggers into personalized local models. \\textit{PFedBA} yields outstanding attack performance across 10 state-of-the-art PFL algorithms, defeating the existing 6 defense mechanisms. Our study sheds light on the subtle yet potent backdoor threats to PFL systems, urging the community to bolster defenses against emerging backdoor challenges.",
    "authors": [
      "Xiaoting Lyu",
      "Yufei Han",
      "Wei Wang",
      "Jingkai Liu",
      "Yongsheng Zhu",
      "Guangquan Xu",
      "Jiqiang Liu",
      "Xiangliang Zhang"
    ],
    "url": "http://arxiv.org/abs/2406.06207v1",
    "timestamp": 1718021645,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "84cb531c-a786-489f-b62d-04c37f3c4b50": {
    "pk": "84cb531c-a786-489f-b62d-04c37f3c4b50",
    "title": "Federated learning in food research",
    "abstract": "Research in the food domain is at times limited due to data sharing obstacles, such as data ownership, privacy requirements, and regulations. While important, these obstacles can restrict data-driven methods such as machine learning. Federated learning, the approach of training models on locally kept data and only sharing the learned parameters, is a potential technique to alleviate data sharing obstacles. This systematic review investigates the use of federated learning within the food domain, structures included papers in a federated learning framework, highlights knowledge gaps, and discusses potential applications. A total of 41 papers were included in the review. The current applications include solutions to water and milk quality assessment, cybersecurity of water processing, pesticide residue risk analysis, weed detection, and fraud detection, focusing on centralized horizontal federated learning. One of the gaps found was the lack of vertical or transfer federated learning and decentralized architectures.",
    "authors": [
      "Zuzanna Fendor",
      "Bas H. M. van der Velden",
      "Xinxin Wang",
      "Andrea Jr. Carnoli",
      "Osman Mutlu",
      "Ali H\u00fcrriyeto\u011flu"
    ],
    "url": "http://arxiv.org/abs/2406.06202v1",
    "timestamp": 1718020691,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b2d64684-0d6a-4e15-89ec-b218a3304d26": {
    "pk": "b2d64684-0d6a-4e15-89ec-b218a3304d26",
    "title": "Learning effective Hamiltonians for adaptive time-evolution quantum algorithms",
    "abstract": "Digital quantum simulation of many-body dynamics relies on Trotterization to decompose the target time evolution into elementary quantum gates operating at a fixed equidistant time discretization. Recent advances have outlined protocols enabling more efficient adaptive Trotter protocols, which have been shown to exhibit a controlled error in the dynamics of local observables and correlation functions. However, it has remained open to which extent the errors on the actual generator of the dynamics, i.e., the target many-body Hamiltonian, remain controlled. Here, we propose to use quantum Hamiltonian learning to numerically obtain the effective Hamiltonian and apply it on the recently introduced ADA-Trotter algorithm as a concrete demonstration. Our key observation is that deviations from the target generator remain bounded on all simulation times. This result suggests that the ADA-Trotter not only generates reliable digital quantum simulation of local dynamics, but also controllably approximates the global quantum state of the target system. Our proposal is sufficiently general and readily applicable to other adaptive time-evolution algorithms.",
    "authors": [
      "Hongzheng Zhao",
      "Ao Chen",
      "Shu-Wei Liu",
      "Marin Bukov",
      "Markus Heyl",
      "Roderich Moessner"
    ],
    "url": "http://arxiv.org/abs/2406.06198v1",
    "timestamp": 1718020238,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "795242e6-0496-449f-bfb9-d80bf0d38d99": {
    "pk": "795242e6-0496-449f-bfb9-d80bf0d38d99",
    "title": "An Effective-Efficient Approach for Dense Multi-Label Action Detection",
    "abstract": "Unlike the sparse label action detection task, where a single action occurs in each timestamp of a video, in a dense multi-label scenario, actions can overlap. To address this challenging task, it is necessary to simultaneously learn (i) temporal dependencies and (ii) co-occurrence action relationships. Recent approaches model temporal information by extracting multi-scale features through hierarchical transformer-based networks. However, the self-attention mechanism in transformers inherently loses temporal positional information. We argue that combining this with multiple sub-sampling processes in hierarchical designs can lead to further loss of positional information. Preserving this information is essential for accurate action detection. In this paper, we address this issue by proposing a novel transformer-based network that (a) employs a non-hierarchical structure when modelling different ranges of temporal dependencies and (b) embeds relative positional encoding in its transformer layers. Furthermore, to model co-occurrence action relationships, current methods explicitly embed class relations into the transformer network. However, these approaches are not computationally efficient, as the network needs to compute all possible pair action class relations. We also overcome this challenge by introducing a novel learning paradigm that allows the network to benefit from explicitly modelling temporal co-occurrence action dependencies without imposing their additional computational costs during inference. We evaluate the performance of our proposed approach on two challenging dense multi-label benchmark datasets and show that our method improves the current state-of-the-art results.",
    "authors": [
      "Faegheh Sardari",
      "Armin Mustafa",
      "Philip J. B. Jackson",
      "Adrian Hilton"
    ],
    "url": "http://arxiv.org/abs/2406.06187v1",
    "timestamp": 1718019214,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  }
}